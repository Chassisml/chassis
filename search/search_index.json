{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"None","text":"<p>Welcome to Chassis.</p>"},{"location":"chassisml_sdk-reference/","title":"Chassisml Python SDK","text":""},{"location":"chassisml_sdk-reference/#introduction","title":"Introduction","text":"<p>The Chassisml Python SDK offers convenience functions that interact with the Chassisml service to automate the containerization and deployment of models to your preferred model serving platform. It is organized into two classes: </p> <ul> <li><code>ChassisClient</code>: interacts with the Chassis routes that build a user's container from their model artifacts</li> <li><code>ChasissModel</code>: creates a chassis compliant model out of a few lines of Python code from user supplied model artifacts </li> </ul> <p>First, install the Chassisml SDK to get started:</p> <p><code>pip install chassisml</code></p> <p>To import the library into your editor:</p> <p><code>import chassisml</code></p>"},{"location":"chassisml_sdk-reference/#usage","title":"Usage","text":""},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient","title":"<code> ChassisClient        </code>","text":"<p>The Chassis Client object.</p> <p>This class is used to interact with the Kaniko service.</p> <p>Attributes:</p> Name Type Description <code>base_url</code> <code>str</code> <p>The base url for the API.</p> <code>auth_header</code> <code>str</code> <p>Optional authorization header to be included with all requests.</p> <code>ssl_verification</code> <code>Union[str, bool]</code> <p>Can be path to certificate to use during requests to service, True (use verification), or False (don't use verification).</p> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>class ChassisClient:\n\"\"\"The Chassis Client object.\n    This class is used to interact with the Kaniko service.\n    Attributes:\n        base_url (str): The base url for the API.\n        auth_header (str): Optional authorization header to be included with all requests.\n        ssl_verification (Union[str, bool]): Can be path to certificate to use during requests to service, True (use verification), or False (don't use verification).\n    \"\"\"\ndef __init__(self,base_url='http://localhost:5000',auth_header=None,ssl_verification=True):\nself.base_url = base_url\nself.auth_header = auth_header\nself.ssl_verification = ssl_verification\nif self.auth_header:\nres = requests.get(base_url,headers={'Authorization': self.auth_header},verify=self.ssl_verification)\nelse:\nres = requests.get(base_url,verify=self.ssl_verification)\nversion_route = os.path.join(base_url,'version')\nif self.auth_header:\nres = requests.get(version_route,headers={'Authorization': self.auth_header},verify=self.ssl_verification)\nelse:\nres = requests.get(version_route,verify=self.ssl_verification)\nparsed_version = version.parse(res.text)\nif parsed_version &lt; version.Version('1.0.0'):\nwarnings.warn(\"Chassis service version should be &gt;=1.0.0 for compatibility with this SDK version, things may not work as expected. Please update the service.\")\ndef get_job_status(self, job_id):\n'''\n        Checks the status of a chassis job\n        Args:\n            job_id (str): Chassis job identifier generated from `ChassisModel.publish` method\n        Returns:\n            Dict: JSON Chassis job status\n        Examples:\n        ```python\n        # Create Chassisml model\n        chassis_model = chassis_client.create_model(process_fn=process)\n        # Define Dockerhub credentials\n        dockerhub_user = \"user\"\n        dockerhub_pass = \"password\"\n        # Publish model to Docker registry\n        response = chassis_model.publish(\n            model_name=\"Chassisml Regression Model\",\n            model_version=\"0.0.1\",\n            registry_user=dockerhub_user,\n            registry_pass=dockerhub_pass,\n        ) \n        job_id = response.get('job_id')\n        job_status = chassis_client.get_job_status(job_id)\n        ```\n        '''\nroute = f'{urllib.parse.urljoin(self.base_url, routes[\"job\"])}/{job_id}'\nif self.auth_header:\nres = requests.get(route,headers={'Authorization': self.auth_header},verify=self.ssl_verification)\nelse:\nres = requests.get(route,verify=self.ssl_verification)\ndata = res.json()\nreturn data\ndef get_job_logs(self, job_id):\n'''\n        Checks the status of a chassis job\n        Args:\n            job_id (str): Chassis job identifier generated from `ChassisModel.publish` method\n        Returns:\n            Dict: JSON Chassis job status\n        Examples:\n        ```python\n        # Create Chassisml model\n        chassis_model = chassis_client.create_model(process_fn=process)\n        # Define Dockerhub credentials\n        dockerhub_user = \"user\"\n        dockerhub_pass = \"password\"\n        # Publish model to Docker registry\n        response = chassis_model.publish(\n            model_name=\"Chassisml Regression Model\",\n            model_version=\"0.0.1\",\n            registry_user=dockerhub_user,\n            registry_pass=dockerhub_pass,\n        ) \n        job_id = response.get('job_id')\n        job_status = chassis_client.get_job_logs(job_id)\n        ```\n        '''\nroute = f'{urllib.parse.urljoin(self.base_url, routes[\"job\"])}/{job_id}/logs'\nif self.auth_header:\nres = requests.get(route,headers={'Authorization': self.auth_header},verify=self.ssl_verification)\nelse:\nres = requests.get(route,verify=self.ssl_verification)\nres.raise_for_status()\nreturn res.text\ndef block_until_complete(self,job_id,timeout=None,poll_interval=5):\n'''\n        Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished.\n        Args:\n            job_id (str): Chassis job identifier generated from `ChassisModel.publish` method\n            timeout (int): Timeout threshold in seconds\n            poll_intervall (int): Amount of time to wait in between API polls to check status of job\n        Returns:\n            Dict: final job status returned by `ChassisClient.block_until_complete` method\n        Examples:\n        ```python\n        # Create Chassisml model\n        chassis_model = chassis_client.create_model(process_fn=process)\n        # Define Dockerhub credentials\n        dockerhub_user = \"user\"\n        dockerhub_pass = \"password\"\n        # Publish model to Docker registry\n        response = chassis_model.publish(\n            model_name=\"Chassisml Regression Model\",\n            model_version=\"0.0.1\",\n            registry_user=dockerhub_user,\n            registry_pass=dockerhub_pass,\n        ) \n        job_id = response.get('job_id')\n        final_status = chassis_client.block_until_complete(job_id)\n        ```        \n        '''\nendby = time.time() + timeout if (timeout is not None) else None\nwhile True:\nstatus = self.get_job_status(job_id)\nif status['status']['succeeded'] or status['status']['failed']:\nreturn status\nif (endby is not None) and (time.time() &gt; endby - poll_interval):\nprint('Timed out before completion.')\nreturn False\ntime.sleep(poll_interval)\ndef download_tar(self, job_id, output_filename):\n'''\n        Downloads container image as tar archive\n        **NOTE**: This method is not available in the publicly-hosted service.\n        Args:\n            job_id (str): Chassis job identifier generated from `ChassisModel.publish` method\n            output_filename (str): Local output filepath to save container image\n        Returns:\n            None: This method does not return an object\n        Examples:\n        ```python\n        # Publish model to Docker registry\n        response = chassis_model.publish(\n            model_name=\"Chassisml Regression Model\",\n            model_version=\"0.0.1\",\n            registry_user=dockerhub_user,\n            registry_pass=dockerhub_pass,\n        ) \n        job_id = response.get('job_id)\n        chassis_client.download_tar(job_id, \"./chassis-model.tar\")\n        ```\n        '''\nurl = f'{urllib.parse.urljoin(self.base_url, routes[\"job\"])}/{job_id}/download-tar'\nif self.auth_header:\nr = requests.get(url,headers={'Authorization': self.auth_header},verify=self.ssl_verification)\nelse:\nr = requests.get(url,verify=self.ssl_verification)\nif r.status_code == 200:\nwith open(output_filename, 'wb') as f:\nf.write(r.content)\nelse:\nprint(f'Error download tar: {r.text}')\ndef test_OMI_compliance(self, image_id=None):\n'''\n        Tests a local image for compliance with the [Open Model Interface Specification](https://openmodel.ml/spec/)\n        Args:\n            image_id (str): image id of a local docker container. e.g. `dockerusername/repositoryname:tag`\n        Returns:\n            tuple(bool, str): Tuple containing compliance boolean (`True` if compliant, `False` if not) and corresponding string containing concatenation of any logs.\n        Examples:\n        ```python\n        # test a local docker image\n        OMI_test, logs = chassis_client.test_OMI_compliance(image_id)\n        if OMI_test:\n            print(\"OMI compliance test passed\")\n        else:\n            print(\"OMI compliance test failed\",logs)\n        ```\n        '''\nrValue = (False, \"Nothing Initialized\")\ntry:\ncheckObject = OMI_check(image_id=image_id)\nif checkObject.client is None:\nraise TypeError(\"The Docker Client couldn't be initialized. Is Docker installed?\")\nimage_check = checkObject.validate_image()\nif \"Failure\" in image_check:\nraise ValueError(image_check)\ncontainer_start = checkObject.start_container()\nif \"Failure\" in container_start:\nraise  ValueError(container_start)\ngRPC_check = checkObject.validate_gRPC()\nif \"Failure\"in gRPC_check:\nraise ValueError(gRPC_check)\nclean_up = checkObject.clean_up()\nif \"Failure\" in clean_up:\nraise ValueError(clean_up)\nrValue = (True, \"\\n\" + image_check + \"\\n\" + container_start + \"\\n\" + gRPC_check + \"\\n\" +clean_up)\nexcept Exception as e:\nrValue = (False, e)\nreturn rValue\ndef create_model(self,process_fn=None,batch_process_fn=None,batch_size=None):\n'''\n        Builds chassis model locally\n        Args:\n            process_fn (function): Python function that must accept a single piece of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method\n            batch_process_fn (function): Python function that must accept a batch of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method\n            batch_size (int): Maximum batch size if `batch_process_fn` is defined\n        Returns:\n            ChassisModel: Chassis Model object that can be tested locally and published to a Docker Registry\n        Examples:\n        The following snippet was taken from this [example](https://docs.modzy.com/docs/chassis-ml).\n        ```python\n        # Import and normalize data\n        X_digits, y_digits = datasets.load_digits(return_X_y=True)\n        X_digits = X_digits / X_digits.max()\n        n_samples = len(X_digits)\n        # Split data into training and test sets\n        X_train = X_digits[: int(0.9 * n_samples)]\n        y_train = y_digits[: int(0.9 * n_samples)]\n        X_test = X_digits[int(0.9 * n_samples) :]\n        y_test = y_digits[int(0.9 * n_samples) :]\n        # Train Model\n        logistic = LogisticRegression(max_iter=1000)\n        print(\n            \"LogisticRegression mean accuracy score: %f\"\n            % logistic.fit(X_train, y_train).score(X_test, y_test)\n        )\n        # Save small sample input to use for testing later\n        sample = X_test[:5].tolist()\n        with open(\"digits_sample.json\", 'w') as out:\n            json.dump(sample, out)        \n        # Define Process function\n        def process(input_bytes):\n            inputs = np.array(json.loads(input_bytes))\n            inference_results = logistic.predict(inputs)\n            structured_results = []\n            for inference_result in inference_results:\n                structured_output = {\n                    \"data\": {\n                        \"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]}\n                    }\n                }\n                structured_results.append(structured_output)\n            return structured_results      \n        # create Chassis model\n        chassis_model = chassis_client.create_model(process_fn=process)              \n        ```\n        '''\nif not (process_fn or batch_process_fn):\nraise ValueError(\"At least one of process_fn or batch_process_fn must be provided.\")\nif (batch_process_fn and not batch_size) or (batch_size and not batch_process_fn):\nraise ValueError(\"Both batch_process_fn and batch_size must be provided for batch support.\")\nreturn ChassisModel(process_fn,batch_process_fn,batch_size,self.base_url,self.auth_header,self.ssl_verification)\ndef run_inference(self,input_data,container_url=\"localhost\",host_port=45000):\n'''\n        This is the method you use to submit data to a container chassis has built for inference. It assumes the container has been downloaded from dockerhub and is running somewhere you have access to.\n        Args:\n            input_data (json): dictionary of the form {\"input\": &lt;binary respresentaion of your data&gt;}\n            container_url (str): URL where container is running\n            host_port (int): host port that forwards to container's grpc server port\n        Returns:\n            return_value (str): Success -&gt; results from model processing as specified in the process function.\n                                Failure -&gt; Error codes from processing errors. All errors should container the word \"Error.\"\n        Examples:\n        ```python\n        # assume that the container is running locally, and that it was started with this docker command\n        #  docker run -it -p 5001:45000 &lt;docker_uname&gt;/&lt;container_name&gt;:&lt;tag_id&gt;\n        from chassisml_sdk.chassisml import chassisml\n        client = chassisml.ChassisClient()\n        input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"}\n        input_list = [input_data for _ in range(30)]\n        print(\"single input\")\n        print(client.run_inference(input_data, container_url=\"localhost\", host_port=5001))\n        print(\"multi inputs\")\n        results = client.run_inference(input_list, container_url=\"localhost\", host_port=5001)\n        for x in results:\n            print(x)\n        ```\n        '''\nmodel_client.override_server_URL(container_url, host_port)\nreturn model_client.run(input_data)\ndef docker_infer(self,image_id,input_data,container_url=\"localhost\",host_port=5001,container_port=None,timeout=20,clean_up=True,pull_container=False):\n'''\n        Runs inference on an OMI compliant container. This method checks to see if a container is running and if not starts it. The method then runs inference against the input_data with the model in the container, and optionally shuts down the container.\n        Args:\n            image_id (string): the name of an OMI container image usually of the form &lt;docker_uname&gt;/&lt;container_name&gt;:&lt;tag_id&gt;\n            input_data (json): dictionary of the form {\"input\": &lt;binary respresentaion of your data&gt;}\n            container_url (str): URL where container is running\n            host_port (int): host port that forwards to container's grpc server port\n            container_port (str): container port the grpc server listens to\n            timeout (int): number of seconds to wait for gRPC server to spin up\n            clean_up (bool): whether or not to stop and remove the container after inference\n            pull_container (bool): if True pulls missing container from repo\n        Returns:\n            return_value (str):    Success -&gt; model output as defined in the process function\n                                    Failure -&gt; Error message if any success criteria is missing.\n        Example:\n        ```python\n        host_port = 5002\n        client = chassisml.ChassisClient()\n        input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"}\n        input_list = [input_data for _ in range(30)]\n        print(\"single input\")\n        print(client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_data, container_url=\"localhost\", host_port=host_port, clean_up=False, pull_container=True))\n        print(\"multi inputs\")\n        results = client.run_inference(input_list, container_url=\"localhost\", host_port=host_port)\n        results = client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_list, container_url=\"localhost\", host_port=host_port)\n        for x in results:\n            print(x)\n        ```\n        '''\ntry:\ncontainer_id = docker_start(image_id, host_port=host_port, container_port=container_port, timeout=timeout, pull_container=pull_container)\nif \"Error\" in container_id:\nraise ValueError(\"container_id wrong\")\nreturn_value = self.run_inference(input_data, container_url=container_url,  host_port=host_port)\nif clean_up:\ndocker_clean_up(container_id)\nexcept Exception as e:\nreturn_value = {\"results\": [\"Error \" + str(e)]}\nreturn return_value\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.block_until_complete","title":"<code>block_until_complete(self, job_id, timeout=None, poll_interval=5)</code>","text":"<p>Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Chassis job identifier generated from <code>ChassisModel.publish</code> method</p> required <code>timeout</code> <code>int</code> <p>Timeout threshold in seconds</p> <code>None</code> <code>poll_intervall</code> <code>int</code> <p>Amount of time to wait in between API polls to check status of job</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>final job status returned by <code>ChassisClient.block_until_complete</code> method</p> <p>Examples:</p> <pre><code># Create Chassisml model\nchassis_model = chassis_client.create_model(process_fn=process)\n# Define Dockerhub credentials\ndockerhub_user = \"user\"\ndockerhub_pass = \"password\"\n# Publish model to Docker registry\nresponse = chassis_model.publish(\nmodel_name=\"Chassisml Regression Model\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n) \njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def block_until_complete(self,job_id,timeout=None,poll_interval=5):\n'''\n    Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished.\n    Args:\n        job_id (str): Chassis job identifier generated from `ChassisModel.publish` method\n        timeout (int): Timeout threshold in seconds\n        poll_intervall (int): Amount of time to wait in between API polls to check status of job\n    Returns:\n        Dict: final job status returned by `ChassisClient.block_until_complete` method\n    Examples:\n    ```python\n    # Create Chassisml model\n    chassis_model = chassis_client.create_model(process_fn=process)\n    # Define Dockerhub credentials\n    dockerhub_user = \"user\"\n    dockerhub_pass = \"password\"\n    # Publish model to Docker registry\n    response = chassis_model.publish(\n        model_name=\"Chassisml Regression Model\",\n        model_version=\"0.0.1\",\n        registry_user=dockerhub_user,\n        registry_pass=dockerhub_pass,\n    ) \n    job_id = response.get('job_id')\n    final_status = chassis_client.block_until_complete(job_id)\n    ```        \n    '''\nendby = time.time() + timeout if (timeout is not None) else None\nwhile True:\nstatus = self.get_job_status(job_id)\nif status['status']['succeeded'] or status['status']['failed']:\nreturn status\nif (endby is not None) and (time.time() &gt; endby - poll_interval):\nprint('Timed out before completion.')\nreturn False\ntime.sleep(poll_interval)\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.create_model","title":"<code>create_model(self, process_fn=None, batch_process_fn=None, batch_size=None)</code>","text":"<p>Builds chassis model locally</p> <p>Parameters:</p> Name Type Description Default <code>process_fn</code> <code>function</code> <p>Python function that must accept a single piece of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the <code>process</code> method</p> <code>None</code> <code>batch_process_fn</code> <code>function</code> <p>Python function that must accept a batch of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the <code>process</code> method</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Maximum batch size if <code>batch_process_fn</code> is defined</p> <code>None</code> <p>Returns:</p> Type Description <code>ChassisModel</code> <p>Chassis Model object that can be tested locally and published to a Docker Registry</p> <p>Examples:</p> <p>The following snippet was taken from this example. <pre><code># Import and normalize data\nX_digits, y_digits = datasets.load_digits(return_X_y=True)\nX_digits = X_digits / X_digits.max()\nn_samples = len(X_digits)\n# Split data into training and test sets\nX_train = X_digits[: int(0.9 * n_samples)]\ny_train = y_digits[: int(0.9 * n_samples)]\nX_test = X_digits[int(0.9 * n_samples) :]\ny_test = y_digits[int(0.9 * n_samples) :]\n# Train Model\nlogistic = LogisticRegression(max_iter=1000)\nprint(\n\"LogisticRegression mean accuracy score: %f\"\n% logistic.fit(X_train, y_train).score(X_test, y_test)\n)\n# Save small sample input to use for testing later\nsample = X_test[:5].tolist()\nwith open(\"digits_sample.json\", 'w') as out:\njson.dump(sample, out)        \n# Define Process function\ndef process(input_bytes):\ninputs = np.array(json.loads(input_bytes))\ninference_results = logistic.predict(inputs)\nstructured_results = []\nfor inference_result in inference_results:\nstructured_output = {\n\"data\": {\n\"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]}\n}\n}\nstructured_results.append(structured_output)\nreturn structured_results      \n# create Chassis model\nchassis_model = chassis_client.create_model(process_fn=process)              \n</code></pre></p> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def create_model(self,process_fn=None,batch_process_fn=None,batch_size=None):\n'''\n    Builds chassis model locally\n    Args:\n        process_fn (function): Python function that must accept a single piece of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method\n        batch_process_fn (function): Python function that must accept a batch of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method\n        batch_size (int): Maximum batch size if `batch_process_fn` is defined\n    Returns:\n        ChassisModel: Chassis Model object that can be tested locally and published to a Docker Registry\n    Examples:\n    The following snippet was taken from this [example](https://docs.modzy.com/docs/chassis-ml).\n    ```python\n    # Import and normalize data\n    X_digits, y_digits = datasets.load_digits(return_X_y=True)\n    X_digits = X_digits / X_digits.max()\n    n_samples = len(X_digits)\n    # Split data into training and test sets\n    X_train = X_digits[: int(0.9 * n_samples)]\n    y_train = y_digits[: int(0.9 * n_samples)]\n    X_test = X_digits[int(0.9 * n_samples) :]\n    y_test = y_digits[int(0.9 * n_samples) :]\n    # Train Model\n    logistic = LogisticRegression(max_iter=1000)\n    print(\n        \"LogisticRegression mean accuracy score: %f\"\n        % logistic.fit(X_train, y_train).score(X_test, y_test)\n    )\n    # Save small sample input to use for testing later\n    sample = X_test[:5].tolist()\n    with open(\"digits_sample.json\", 'w') as out:\n        json.dump(sample, out)        \n    # Define Process function\n    def process(input_bytes):\n        inputs = np.array(json.loads(input_bytes))\n        inference_results = logistic.predict(inputs)\n        structured_results = []\n        for inference_result in inference_results:\n            structured_output = {\n                \"data\": {\n                    \"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]}\n                }\n            }\n            structured_results.append(structured_output)\n        return structured_results      \n    # create Chassis model\n    chassis_model = chassis_client.create_model(process_fn=process)              \n    ```\n    '''\nif not (process_fn or batch_process_fn):\nraise ValueError(\"At least one of process_fn or batch_process_fn must be provided.\")\nif (batch_process_fn and not batch_size) or (batch_size and not batch_process_fn):\nraise ValueError(\"Both batch_process_fn and batch_size must be provided for batch support.\")\nreturn ChassisModel(process_fn,batch_process_fn,batch_size,self.base_url,self.auth_header,self.ssl_verification)\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.docker_infer","title":"<code>docker_infer(self, image_id, input_data, container_url='localhost', host_port=5001, container_port=None, timeout=20, clean_up=True, pull_container=False)</code>","text":"<p>Runs inference on an OMI compliant container. This method checks to see if a container is running and if not starts it. The method then runs inference against the input_data with the model in the container, and optionally shuts down the container.</p> <p>Parameters:</p> Name Type Description Default <code>image_id</code> <code>string</code> <p>the name of an OMI container image usually of the form /: required <code>input_data</code> <code>json</code> <p>dictionary of the form {\"input\": } required <code>container_url</code> <code>str</code> <p>URL where container is running</p> <code>'localhost'</code> <code>host_port</code> <code>int</code> <p>host port that forwards to container's grpc server port</p> <code>5001</code> <code>container_port</code> <code>str</code> <p>container port the grpc server listens to</p> <code>None</code> <code>timeout</code> <code>int</code> <p>number of seconds to wait for gRPC server to spin up</p> <code>20</code> <code>clean_up</code> <code>bool</code> <p>whether or not to stop and remove the container after inference</p> <code>True</code> <code>pull_container</code> <code>bool</code> <p>if True pulls missing container from repo</p> <code>False</code> <p>Returns:</p> Type Description <code>return_value (str)</code> <p>Success -&gt; model output as defined in the process function                         Failure -&gt; Error message if any success criteria is missing.</p> <p>Examples:</p> <pre><code>host_port = 5002\nclient = chassisml.ChassisClient()\ninput_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"}\ninput_list = [input_data for _ in range(30)]\nprint(\"single input\")\nprint(client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_data, container_url=\"localhost\", host_port=host_port, clean_up=False, pull_container=True))\nprint(\"multi inputs\")\nresults = client.run_inference(input_list, container_url=\"localhost\", host_port=host_port)\nresults = client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_list, container_url=\"localhost\", host_port=host_port)\nfor x in results:\nprint(x)\n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def docker_infer(self,image_id,input_data,container_url=\"localhost\",host_port=5001,container_port=None,timeout=20,clean_up=True,pull_container=False):\n'''\n    Runs inference on an OMI compliant container. This method checks to see if a container is running and if not starts it. The method then runs inference against the input_data with the model in the container, and optionally shuts down the container.\n    Args:\n        image_id (string): the name of an OMI container image usually of the form &lt;docker_uname&gt;/&lt;container_name&gt;:&lt;tag_id&gt;\n        input_data (json): dictionary of the form {\"input\": &lt;binary respresentaion of your data&gt;}\n        container_url (str): URL where container is running\n        host_port (int): host port that forwards to container's grpc server port\n        container_port (str): container port the grpc server listens to\n        timeout (int): number of seconds to wait for gRPC server to spin up\n        clean_up (bool): whether or not to stop and remove the container after inference\n        pull_container (bool): if True pulls missing container from repo\n    Returns:\n        return_value (str):    Success -&gt; model output as defined in the process function\n                                Failure -&gt; Error message if any success criteria is missing.\n    Example:\n    ```python\n    host_port = 5002\n    client = chassisml.ChassisClient()\n    input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"}\n    input_list = [input_data for _ in range(30)]\n    print(\"single input\")\n    print(client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_data, container_url=\"localhost\", host_port=host_port, clean_up=False, pull_container=True))\n    print(\"multi inputs\")\n    results = client.run_inference(input_list, container_url=\"localhost\", host_port=host_port)\n    results = client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_list, container_url=\"localhost\", host_port=host_port)\n    for x in results:\n        print(x)\n    ```\n    '''\ntry:\ncontainer_id = docker_start(image_id, host_port=host_port, container_port=container_port, timeout=timeout, pull_container=pull_container)\nif \"Error\" in container_id:\nraise ValueError(\"container_id wrong\")\nreturn_value = self.run_inference(input_data, container_url=container_url,  host_port=host_port)\nif clean_up:\ndocker_clean_up(container_id)\nexcept Exception as e:\nreturn_value = {\"results\": [\"Error \" + str(e)]}\nreturn return_value\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.download_tar","title":"<code>download_tar(self, job_id, output_filename)</code>","text":"<p>Downloads container image as tar archive</p> <p>NOTE: This method is not available in the publicly-hosted service.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Chassis job identifier generated from <code>ChassisModel.publish</code> method</p> required <code>output_filename</code> <code>str</code> <p>Local output filepath to save container image</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method does not return an object</p> <p>Examples:</p> <pre><code># Publish model to Docker registry\nresponse = chassis_model.publish(\nmodel_name=\"Chassisml Regression Model\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n) \njob_id = response.get('job_id)\nchassis_client.download_tar(job_id, \"./chassis-model.tar\")\n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def download_tar(self, job_id, output_filename):\n'''\n    Downloads container image as tar archive\n    **NOTE**: This method is not available in the publicly-hosted service.\n    Args:\n        job_id (str): Chassis job identifier generated from `ChassisModel.publish` method\n        output_filename (str): Local output filepath to save container image\n    Returns:\n        None: This method does not return an object\n    Examples:\n    ```python\n    # Publish model to Docker registry\n    response = chassis_model.publish(\n        model_name=\"Chassisml Regression Model\",\n        model_version=\"0.0.1\",\n        registry_user=dockerhub_user,\n        registry_pass=dockerhub_pass,\n    ) \n    job_id = response.get('job_id)\n    chassis_client.download_tar(job_id, \"./chassis-model.tar\")\n    ```\n    '''\nurl = f'{urllib.parse.urljoin(self.base_url, routes[\"job\"])}/{job_id}/download-tar'\nif self.auth_header:\nr = requests.get(url,headers={'Authorization': self.auth_header},verify=self.ssl_verification)\nelse:\nr = requests.get(url,verify=self.ssl_verification)\nif r.status_code == 200:\nwith open(output_filename, 'wb') as f:\nf.write(r.content)\nelse:\nprint(f'Error download tar: {r.text}')\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.get_job_logs","title":"<code>get_job_logs(self, job_id)</code>","text":"<p>Checks the status of a chassis job</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Chassis job identifier generated from <code>ChassisModel.publish</code> method</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>JSON Chassis job status</p> <p>Examples:</p> <pre><code># Create Chassisml model\nchassis_model = chassis_client.create_model(process_fn=process)\n# Define Dockerhub credentials\ndockerhub_user = \"user\"\ndockerhub_pass = \"password\"\n# Publish model to Docker registry\nresponse = chassis_model.publish(\nmodel_name=\"Chassisml Regression Model\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n) \njob_id = response.get('job_id')\njob_status = chassis_client.get_job_logs(job_id)\n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def get_job_logs(self, job_id):\n'''\n    Checks the status of a chassis job\n    Args:\n        job_id (str): Chassis job identifier generated from `ChassisModel.publish` method\n    Returns:\n        Dict: JSON Chassis job status\n    Examples:\n    ```python\n    # Create Chassisml model\n    chassis_model = chassis_client.create_model(process_fn=process)\n    # Define Dockerhub credentials\n    dockerhub_user = \"user\"\n    dockerhub_pass = \"password\"\n    # Publish model to Docker registry\n    response = chassis_model.publish(\n        model_name=\"Chassisml Regression Model\",\n        model_version=\"0.0.1\",\n        registry_user=dockerhub_user,\n        registry_pass=dockerhub_pass,\n    ) \n    job_id = response.get('job_id')\n    job_status = chassis_client.get_job_logs(job_id)\n    ```\n    '''\nroute = f'{urllib.parse.urljoin(self.base_url, routes[\"job\"])}/{job_id}/logs'\nif self.auth_header:\nres = requests.get(route,headers={'Authorization': self.auth_header},verify=self.ssl_verification)\nelse:\nres = requests.get(route,verify=self.ssl_verification)\nres.raise_for_status()\nreturn res.text\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.get_job_status","title":"<code>get_job_status(self, job_id)</code>","text":"<p>Checks the status of a chassis job</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Chassis job identifier generated from <code>ChassisModel.publish</code> method</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>JSON Chassis job status</p> <p>Examples:</p> <pre><code># Create Chassisml model\nchassis_model = chassis_client.create_model(process_fn=process)\n# Define Dockerhub credentials\ndockerhub_user = \"user\"\ndockerhub_pass = \"password\"\n# Publish model to Docker registry\nresponse = chassis_model.publish(\nmodel_name=\"Chassisml Regression Model\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n) \njob_id = response.get('job_id')\njob_status = chassis_client.get_job_status(job_id)\n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def get_job_status(self, job_id):\n'''\n    Checks the status of a chassis job\n    Args:\n        job_id (str): Chassis job identifier generated from `ChassisModel.publish` method\n    Returns:\n        Dict: JSON Chassis job status\n    Examples:\n    ```python\n    # Create Chassisml model\n    chassis_model = chassis_client.create_model(process_fn=process)\n    # Define Dockerhub credentials\n    dockerhub_user = \"user\"\n    dockerhub_pass = \"password\"\n    # Publish model to Docker registry\n    response = chassis_model.publish(\n        model_name=\"Chassisml Regression Model\",\n        model_version=\"0.0.1\",\n        registry_user=dockerhub_user,\n        registry_pass=dockerhub_pass,\n    ) \n    job_id = response.get('job_id')\n    job_status = chassis_client.get_job_status(job_id)\n    ```\n    '''\nroute = f'{urllib.parse.urljoin(self.base_url, routes[\"job\"])}/{job_id}'\nif self.auth_header:\nres = requests.get(route,headers={'Authorization': self.auth_header},verify=self.ssl_verification)\nelse:\nres = requests.get(route,verify=self.ssl_verification)\ndata = res.json()\nreturn data\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.run_inference","title":"<code>run_inference(self, input_data, container_url='localhost', host_port=45000)</code>","text":"<p>This is the method you use to submit data to a container chassis has built for inference. It assumes the container has been downloaded from dockerhub and is running somewhere you have access to.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>json</code> <p>dictionary of the form {\"input\": } required <code>container_url</code> <code>str</code> <p>URL where container is running</p> <code>'localhost'</code> <code>host_port</code> <code>int</code> <p>host port that forwards to container's grpc server port</p> <code>45000</code> <p>Returns:</p> Type Description <code>return_value (str)</code> <p>Success -&gt; results from model processing as specified in the process function.                     Failure -&gt; Error codes from processing errors. All errors should container the word \"Error.\"</p> <p>Examples:</p> <pre><code># assume that the container is running locally, and that it was started with this docker command\n#  docker run -it -p 5001:45000 &lt;docker_uname&gt;/&lt;container_name&gt;:&lt;tag_id&gt;\nfrom chassisml_sdk.chassisml import chassisml\nclient = chassisml.ChassisClient()\ninput_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"}\ninput_list = [input_data for _ in range(30)]\nprint(\"single input\")\nprint(client.run_inference(input_data, container_url=\"localhost\", host_port=5001))\nprint(\"multi inputs\")\nresults = client.run_inference(input_list, container_url=\"localhost\", host_port=5001)\nfor x in results:\nprint(x)\n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def run_inference(self,input_data,container_url=\"localhost\",host_port=45000):\n'''\n    This is the method you use to submit data to a container chassis has built for inference. It assumes the container has been downloaded from dockerhub and is running somewhere you have access to.\n    Args:\n        input_data (json): dictionary of the form {\"input\": &lt;binary respresentaion of your data&gt;}\n        container_url (str): URL where container is running\n        host_port (int): host port that forwards to container's grpc server port\n    Returns:\n        return_value (str): Success -&gt; results from model processing as specified in the process function.\n                            Failure -&gt; Error codes from processing errors. All errors should container the word \"Error.\"\n    Examples:\n    ```python\n    # assume that the container is running locally, and that it was started with this docker command\n    #  docker run -it -p 5001:45000 &lt;docker_uname&gt;/&lt;container_name&gt;:&lt;tag_id&gt;\n    from chassisml_sdk.chassisml import chassisml\n    client = chassisml.ChassisClient()\n    input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"}\n    input_list = [input_data for _ in range(30)]\n    print(\"single input\")\n    print(client.run_inference(input_data, container_url=\"localhost\", host_port=5001))\n    print(\"multi inputs\")\n    results = client.run_inference(input_list, container_url=\"localhost\", host_port=5001)\n    for x in results:\n        print(x)\n    ```\n    '''\nmodel_client.override_server_URL(container_url, host_port)\nreturn model_client.run(input_data)\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.test_OMI_compliance","title":"<code>test_OMI_compliance(self, image_id=None)</code>","text":"<p>Tests a local image for compliance with the Open Model Interface Specification</p> <p>Parameters:</p> Name Type Description Default <code>image_id</code> <code>str</code> <p>image id of a local docker container. e.g. <code>dockerusername/repositoryname:tag</code></p> <code>None</code> <p>Returns:</p> Type Description <code>tuple(bool, str)</code> <p>Tuple containing compliance boolean (<code>True</code> if compliant, <code>False</code> if not) and corresponding string containing concatenation of any logs.</p> <p>Examples:</p> <pre><code># test a local docker image\nOMI_test, logs = chassis_client.test_OMI_compliance(image_id)\nif OMI_test:\nprint(\"OMI compliance test passed\")\nelse:\nprint(\"OMI compliance test failed\",logs)\n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def test_OMI_compliance(self, image_id=None):\n'''\n    Tests a local image for compliance with the [Open Model Interface Specification](https://openmodel.ml/spec/)\n    Args:\n        image_id (str): image id of a local docker container. e.g. `dockerusername/repositoryname:tag`\n    Returns:\n        tuple(bool, str): Tuple containing compliance boolean (`True` if compliant, `False` if not) and corresponding string containing concatenation of any logs.\n    Examples:\n    ```python\n    # test a local docker image\n    OMI_test, logs = chassis_client.test_OMI_compliance(image_id)\n    if OMI_test:\n        print(\"OMI compliance test passed\")\n    else:\n        print(\"OMI compliance test failed\",logs)\n    ```\n    '''\nrValue = (False, \"Nothing Initialized\")\ntry:\ncheckObject = OMI_check(image_id=image_id)\nif checkObject.client is None:\nraise TypeError(\"The Docker Client couldn't be initialized. Is Docker installed?\")\nimage_check = checkObject.validate_image()\nif \"Failure\" in image_check:\nraise ValueError(image_check)\ncontainer_start = checkObject.start_container()\nif \"Failure\" in container_start:\nraise  ValueError(container_start)\ngRPC_check = checkObject.validate_gRPC()\nif \"Failure\"in gRPC_check:\nraise ValueError(gRPC_check)\nclean_up = checkObject.clean_up()\nif \"Failure\" in clean_up:\nraise ValueError(clean_up)\nrValue = (True, \"\\n\" + image_check + \"\\n\" + container_start + \"\\n\" + gRPC_check + \"\\n\" +clean_up)\nexcept Exception as e:\nrValue = (False, e)\nreturn rValue\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel","title":"<code> ChassisModel            (PythonModel)         </code>","text":"<p>The Chassis Model object.</p> <p>This class inherits from <code>mlflow.pyfunc.PythonModel</code> and adds Chassis functionality.</p> <p>Attributes:</p> Name Type Description <code>predict</code> <code>function</code> <p>MLflow pyfunc compatible predict function.  Will wrap user-provided function which takes two arguments: model_input (bytes) and model_context (dict).</p> <code>chassis_build_url</code> <code>str</code> <p>The build url for the Chassis API.</p> <code>ssl_verification</code> <code>Union[str, bool]</code> <p>Can be path to certificate to use during requests to service, True (use verification), or False (don't use verification).</p> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>class ChassisModel(mlflow.pyfunc.PythonModel):\n\"\"\"The Chassis Model object.\n    This class inherits from `mlflow.pyfunc.PythonModel` and adds Chassis functionality.\n    Attributes:\n        predict (function): MLflow pyfunc compatible predict function. \n            Will wrap user-provided function which takes two arguments: model_input (bytes) and model_context (dict).\n        chassis_build_url (str): The build url for the Chassis API.\n        ssl_verification (Union[str, bool]): Can be path to certificate to use during requests to service, True (use verification), or False (don't use verification).\n    \"\"\"\ndef __init__(self,process_fn,batch_process_fn,batch_size,chassis_base_url,chassis_auth_header,ssl_verification):      \nif process_fn and batch_process_fn:\nif not batch_size:\nraise ValueError(\"Both batch_process_fn and batch_size must be provided for batch support.\")\nself.predict = self._gen_predict_method(process_fn)\nself.batch_predict = self._gen_predict_method(batch_process_fn,batch=True)\nself.batch_input = True\nself.batch_size = batch_size\nelif process_fn and not batch_process_fn:\nself.predict = self._gen_predict_method(process_fn)\nself.batch_input = False\nself.batch_size = None\nelif batch_process_fn and not process_fn:\nif not batch_size:\nraise ValueError(\"Both batch_process_fn and batch_size must be provided for batch support.\")\nself.predict = self._gen_predict_method(batch_process_fn,batch_to_single=True)\nself.batch_predict = self._gen_predict_method(batch_process_fn,batch=True)\nself.batch_input = True\nself.batch_size = batch_size\nelse:\nraise ValueError(\"At least one of process_fn or batch_process_fn must be provided.\")\nself.chassis_build_url = urllib.parse.urljoin(chassis_base_url, routes['build'])\nself.chassis_test_url = urllib.parse.urljoin(chassis_base_url, routes['test'])\nself.chassis_auth_header = chassis_auth_header\nself.ssl_verification = ssl_verification\ndef _gen_predict_method(self,process_fn,batch=False,batch_to_single=False):\ndef predict(_,model_input):\nif batch_to_single:\noutput = process_fn([model_input])[0]\nelse:\noutput = process_fn(model_input)\nif batch:\nreturn [json.dumps(out,separators=(\",\", \":\"),cls=NumpyEncoder).encode() for out in output]\nelse:\nreturn json.dumps(output,separators=(\",\", \":\"),cls=NumpyEncoder).encode()\nreturn predict\ndef test(self,test_input):\n'''\n        Runs a sample inference test on a single input on chassis model locally\n        Args:\n            test_input (Union[str, bytes, BufferedReader]): Single sample input data to test model\n        Returns:\n            bytes: raw model predictions returned by `process_fn` method\n        Examples:\n        ```python\n        chassis_model = chassis_client.create_model(process_fn=process)\n        sample_filepath = './sample_data.json'\n        results = chassis_model.test(sample_filepath)\n        ```\n        '''\nif isinstance(test_input,_io.BufferedReader):\nresult = self.predict(None,test_input.read())\nelif isinstance(test_input,bytes):\nresult = self.predict(None,test_input)\nelif isinstance(test_input,str):\nif os.path.exists(test_input):\nresult = self.predict(None,open(test_input,'rb').read())\nelse:\nresult = self.predict(None,bytes(test_input,encoding='utf8'))\nelse:\nprint(\"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\")\nreturn False\nreturn result\ndef test_batch(self,test_input):\n'''\n        Takes a single input file, creates a batch of size `batch_size` defined in `ChassisModel.create_model`, and runs a batch job against chassis model locally if `batch_process_fn` is defined.\n        Args:\n            test_input (Union[str, bytes, BufferedReader]): Batch of sample input data to test model\n        Returns:\n            bytes: raw model predictions returned by `batch_process_fn` method\n        Examples:\n        ```python\n        chassis_model = chassis_client.create_model(process_fn=process)\n        sample_input = sample_filepath = './sample_data.json'\n        results = chassis_model.test_batch(sample_filepath)\n        ```        \n        '''\nif not self.batch_input:\nraise NotImplementedError(\"Batch inference not implemented.\")\nif hasattr(self,'batch_predict'):\nbatch_method = self.batch_predict\nelse:\nbatch_method = self.predict\nif isinstance(test_input,_io.BufferedReader):\nresults = batch_method(None,[test_input.read() for _ in range(self.batch_size)])\nelif isinstance(test_input,bytes):\nresults = batch_method(None,[test_input for _ in range(self.batch_size)])\nelif isinstance(test_input,str):\nif os.path.exists(test_input):\nresults = batch_method(None,[open(test_input,'rb').read() for _ in range(self.batch_size)])\nelse:\nresults = batch_method(None,[bytes(test_input,encoding='utf8') for _ in range(self.batch_size)])\nelse:\nprint(\"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\")\nreturn False\nreturn results\ndef test_env(self,test_input_path,conda_env=None,fix_env=True):\n'''\n        Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service.\n        **NOTE**: This method is not available in the publicly-hosted service.\n        Args:\n            test_input_path (str): Filepath to sample input data\n            conda_env (str): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment\n            fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build\n        Returns:\n            Dict: raw model predictions returned by `process_fn` or `batch_process_fn` run from within chassis service\n        Examples:\n        ```python\n        chassis_model = chassis_client.create_model(process_fn=process)\n        sample_filepath = './sample_data.json'\n        results = chassis_model.test_env(sample_filepath)\n        ```        \n        '''\nmodel_directory = os.path.join(tempfile.mkdtemp(),CHASSIS_TMP_DIRNAME)\nmlflow.pyfunc.save_model(path=model_directory, python_model=self, conda_env=conda_env, \nextra_pip_requirements = None if conda_env else [\"chassisml=={}\".format(__version__)])\nif fix_env:\nfix_dependencies(model_directory)\n# Compress all files in model directory to send them as a zip.\ntmppath = tempfile.mkdtemp()\nzipdir(model_directory,tmppath,MODEL_ZIP_NAME)\nwith open('{}/{}'.format(tmppath,MODEL_ZIP_NAME),'rb') as model_f, \\\n                open(test_input_path,'rb') as test_input_f:\nfiles = [\n('sample_input', test_input_f),\n('model', model_f)\n]\nprint('Starting test job... ', end='', flush=True)\nif self.chassis_auth_header:\nres = requests.post(self.chassis_test_url, files=files, headers={'Authorization': self.chassis_auth_header}, verify=self.ssl_verification)\nelse:\nres = requests.post(self.chassis_test_url, files=files, verify=self.ssl_verification)\nres.raise_for_status()\nprint('Ok!')\nshutil.rmtree(tmppath)\nshutil.rmtree(model_directory)\nreturn res.json()\ndef save(self,path,conda_env=None,overwrite=False,fix_env=True,gpu=False,arm64=False):\n'''\n        Saves a copy of ChassisModel to local filepath\n        Args:\n            path (str): Filepath to save chassis model as local MLflow model\n            conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment\n            overwrite (bool): If True, overwrites existing contents of `path` parameter\n            gpu (bool): If True and `arm64` is True, modifies dependencies as needed by chassis for ARM64+GPU support\n            arm64 (bool): If True and `gpu` is True, modifies dependencies as needed by chassis for ARM64+GPU support\n        Returns:\n            None: This method does not return an object\n        Examples:\n        ```python\n        chassis_model = chassis_client.create_model(process_fn=process)\n        chassis_model.save(\"local_model_directory\")\n        ```\n        '''\nif overwrite and os.path.exists(path):\nshutil.rmtree(path)\nmlflow.pyfunc.save_model(path=path, python_model=self, conda_env=conda_env)\nif fix_env:\nfix_dependencies(path)\nif arm64 and gpu:\nfix_dependencies_arm_gpu(path)\nprint(\"Chassis model saved.\")\ndef publish(self,model_name,model_version,registry_user=None,registry_pass=None,\nconda_env=None,fix_env=True,gpu=False,arm64=False,\nsample_input_path=None,webhook=None):\n'''\n        Executes chassis job, which containerizes model and pushes container image to Docker registry.\n        Args:\n            model_name (str): Model name that serves as model's name and docker registry repository name. **Note**: this string cannot include punctuation\n            model_version (str): Version of model\n            registry_user (str): Docker registry username\n            registry_pass (str): Docker registry password\n            conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment\n            fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build\n            gpu (bool): If True, builds container image that runs on GPU hardware\n            arm64 (bool): If True, builds container image that runs on ARM64 architecture\n            sample_input_path (str): Optional filepath to sample input data\n            webhook (str): Optional webhook for Chassis service to update status\n        Returns:\n            Dict: Response to Chassis `/build` endpoint\n        Examples:\n        ```python\n        # Create Chassisml model\n        chassis_model = chassis_client.create_model(process_fn=process)\n        # Define Dockerhub credentials\n        dockerhub_user = \"user\"\n        dockerhub_pass = \"password\"\n        # Publish model to Docker registry\n        response = chassis_model.publish(\n            model_name=\"Chassisml Regression Model\",\n            model_version=\"0.0.1\",\n            registry_user=dockerhub_user,\n            registry_pass=dockerhub_pass,\n        )        \n        ```            \n        '''\nif webhook and not validators.url(webhook):\nraise ValueError(\"Provided webhook is not a valid URL\")\ntry:\nmodel_directory = os.path.join(tempfile.mkdtemp(),CHASSIS_TMP_DIRNAME)\nmlflow.pyfunc.save_model(path=model_directory, python_model=self, conda_env=conda_env, \nextra_pip_requirements = None if conda_env else [\"chassisml=={}\".format(__version__)])\nif fix_env:\nfix_dependencies(model_directory)\nif arm64:\nwarnings.warn(\"ARM64 support is experimental, KServe currently not supported and builds may take a while or fail depending on your required dependencies\")\nif gpu:\nwarnings.warn(\"ARM64+GPU support tested on Nvidia Jetson Nano\")\nfix_dependencies_arm_gpu(model_directory)\n# Compress all files in model directory to send them as a zip.\ntmppath = tempfile.mkdtemp()\nzipdir(model_directory,tmppath,MODEL_ZIP_NAME)\nimage_name = \"-\".join(model_name.translate(str.maketrans('', '', string.punctuation)).lower().split())\nimage_data = {\n'name': f\"{registry_user+'/' if (registry_user and registry_pass) else ''}{image_name}:{model_version}\",\n'model_name': model_name,\n'model_path': tmppath,\n'publish': True,\n'gpu': gpu,\n'arm64': arm64,\n'webhook': webhook\n}\nif registry_user and registry_pass:\nimage_data['registry_auth'] = base64.b64encode(\"{}:{}\".format(registry_user,registry_pass).encode(\"utf-8\")).decode(\"utf-8\")\nmetadata_path = os.path.join(tmppath,YAML_NAME)\nwrite_metadata_yaml(model_name,model_version,metadata_path,batch_size=self.batch_size,gpu=gpu)\nwith open('{}/{}'.format(tmppath,MODEL_ZIP_NAME),'rb') as f:\nfiles = [\n('image_data', json.dumps(image_data)),\n('model', f)\n]\nfile_pointers = []\nmeta_fp = open(metadata_path, 'rb')\nfiles.append(('metadata_data', meta_fp))\nfile_pointers.append(meta_fp)\nif sample_input_path:\nsample_fp = open(sample_input_path, 'rb')\nfiles.append(('sample_input', sample_fp))\nfile_pointers.append(sample_fp)\nprint('Starting build job... ', end='', flush=True)\nif self.chassis_auth_header:\nres = requests.post(self.chassis_build_url, files=files, headers={'Authorization': self.chassis_auth_header}, verify=self.ssl_verification)\nelse:\nres = requests.post(self.chassis_build_url, files=files, verify=self.ssl_verification)\nres.raise_for_status()\nprint('Ok!')\nfor fp in file_pointers:\nfp.close()\nshutil.rmtree(tmppath)\nshutil.rmtree(model_directory)\nreturn res.json()\nexcept Exception as e:\nif os.path.exists(tmppath):\nshutil.rmtree(tmppath)\nif os.path.exists(model_directory):\nshutil.rmtree(model_directory)\nraise(e)\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel.publish","title":"<code>publish(self, model_name, model_version, registry_user=None, registry_pass=None, conda_env=None, fix_env=True, gpu=False, arm64=False, sample_input_path=None, webhook=None)</code>","text":"<p>Executes chassis job, which containerizes model and pushes container image to Docker registry.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model name that serves as model's name and docker registry repository name. Note: this string cannot include punctuation</p> required <code>model_version</code> <code>str</code> <p>Version of model</p> required <code>registry_user</code> <code>str</code> <p>Docker registry username</p> <code>None</code> <code>registry_pass</code> <code>str</code> <p>Docker registry password</p> <code>None</code> <code>conda_env</code> <code>Union[str, dict]</code> <p>Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment</p> <code>None</code> <code>fix_env</code> <code>bool</code> <p>Modifies conda or pip-installable packages into list of dependencies to be installed during the container build</p> <code>True</code> <code>gpu</code> <code>bool</code> <p>If True, builds container image that runs on GPU hardware</p> <code>False</code> <code>arm64</code> <code>bool</code> <p>If True, builds container image that runs on ARM64 architecture</p> <code>False</code> <code>sample_input_path</code> <code>str</code> <p>Optional filepath to sample input data</p> <code>None</code> <code>webhook</code> <code>str</code> <p>Optional webhook for Chassis service to update status</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Response to Chassis <code>/build</code> endpoint</p> <p>Examples:</p> <pre><code># Create Chassisml model\nchassis_model = chassis_client.create_model(process_fn=process)\n# Define Dockerhub credentials\ndockerhub_user = \"user\"\ndockerhub_pass = \"password\"\n# Publish model to Docker registry\nresponse = chassis_model.publish(\nmodel_name=\"Chassisml Regression Model\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)        \n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def publish(self,model_name,model_version,registry_user=None,registry_pass=None,\nconda_env=None,fix_env=True,gpu=False,arm64=False,\nsample_input_path=None,webhook=None):\n'''\n    Executes chassis job, which containerizes model and pushes container image to Docker registry.\n    Args:\n        model_name (str): Model name that serves as model's name and docker registry repository name. **Note**: this string cannot include punctuation\n        model_version (str): Version of model\n        registry_user (str): Docker registry username\n        registry_pass (str): Docker registry password\n        conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment\n        fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build\n        gpu (bool): If True, builds container image that runs on GPU hardware\n        arm64 (bool): If True, builds container image that runs on ARM64 architecture\n        sample_input_path (str): Optional filepath to sample input data\n        webhook (str): Optional webhook for Chassis service to update status\n    Returns:\n        Dict: Response to Chassis `/build` endpoint\n    Examples:\n    ```python\n    # Create Chassisml model\n    chassis_model = chassis_client.create_model(process_fn=process)\n    # Define Dockerhub credentials\n    dockerhub_user = \"user\"\n    dockerhub_pass = \"password\"\n    # Publish model to Docker registry\n    response = chassis_model.publish(\n        model_name=\"Chassisml Regression Model\",\n        model_version=\"0.0.1\",\n        registry_user=dockerhub_user,\n        registry_pass=dockerhub_pass,\n    )        \n    ```            \n    '''\nif webhook and not validators.url(webhook):\nraise ValueError(\"Provided webhook is not a valid URL\")\ntry:\nmodel_directory = os.path.join(tempfile.mkdtemp(),CHASSIS_TMP_DIRNAME)\nmlflow.pyfunc.save_model(path=model_directory, python_model=self, conda_env=conda_env, \nextra_pip_requirements = None if conda_env else [\"chassisml=={}\".format(__version__)])\nif fix_env:\nfix_dependencies(model_directory)\nif arm64:\nwarnings.warn(\"ARM64 support is experimental, KServe currently not supported and builds may take a while or fail depending on your required dependencies\")\nif gpu:\nwarnings.warn(\"ARM64+GPU support tested on Nvidia Jetson Nano\")\nfix_dependencies_arm_gpu(model_directory)\n# Compress all files in model directory to send them as a zip.\ntmppath = tempfile.mkdtemp()\nzipdir(model_directory,tmppath,MODEL_ZIP_NAME)\nimage_name = \"-\".join(model_name.translate(str.maketrans('', '', string.punctuation)).lower().split())\nimage_data = {\n'name': f\"{registry_user+'/' if (registry_user and registry_pass) else ''}{image_name}:{model_version}\",\n'model_name': model_name,\n'model_path': tmppath,\n'publish': True,\n'gpu': gpu,\n'arm64': arm64,\n'webhook': webhook\n}\nif registry_user and registry_pass:\nimage_data['registry_auth'] = base64.b64encode(\"{}:{}\".format(registry_user,registry_pass).encode(\"utf-8\")).decode(\"utf-8\")\nmetadata_path = os.path.join(tmppath,YAML_NAME)\nwrite_metadata_yaml(model_name,model_version,metadata_path,batch_size=self.batch_size,gpu=gpu)\nwith open('{}/{}'.format(tmppath,MODEL_ZIP_NAME),'rb') as f:\nfiles = [\n('image_data', json.dumps(image_data)),\n('model', f)\n]\nfile_pointers = []\nmeta_fp = open(metadata_path, 'rb')\nfiles.append(('metadata_data', meta_fp))\nfile_pointers.append(meta_fp)\nif sample_input_path:\nsample_fp = open(sample_input_path, 'rb')\nfiles.append(('sample_input', sample_fp))\nfile_pointers.append(sample_fp)\nprint('Starting build job... ', end='', flush=True)\nif self.chassis_auth_header:\nres = requests.post(self.chassis_build_url, files=files, headers={'Authorization': self.chassis_auth_header}, verify=self.ssl_verification)\nelse:\nres = requests.post(self.chassis_build_url, files=files, verify=self.ssl_verification)\nres.raise_for_status()\nprint('Ok!')\nfor fp in file_pointers:\nfp.close()\nshutil.rmtree(tmppath)\nshutil.rmtree(model_directory)\nreturn res.json()\nexcept Exception as e:\nif os.path.exists(tmppath):\nshutil.rmtree(tmppath)\nif os.path.exists(model_directory):\nshutil.rmtree(model_directory)\nraise(e)\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel.save","title":"<code>save(self, path, conda_env=None, overwrite=False, fix_env=True, gpu=False, arm64=False)</code>","text":"<p>Saves a copy of ChassisModel to local filepath</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Filepath to save chassis model as local MLflow model</p> required <code>conda_env</code> <code>Union[str, dict]</code> <p>Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrites existing contents of <code>path</code> parameter</p> <code>False</code> <code>gpu</code> <code>bool</code> <p>If True and <code>arm64</code> is True, modifies dependencies as needed by chassis for ARM64+GPU support</p> <code>False</code> <code>arm64</code> <code>bool</code> <p>If True and <code>gpu</code> is True, modifies dependencies as needed by chassis for ARM64+GPU support</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>This method does not return an object</p> <p>Examples:</p> <pre><code>chassis_model = chassis_client.create_model(process_fn=process)\nchassis_model.save(\"local_model_directory\")\n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def save(self,path,conda_env=None,overwrite=False,fix_env=True,gpu=False,arm64=False):\n'''\n    Saves a copy of ChassisModel to local filepath\n    Args:\n        path (str): Filepath to save chassis model as local MLflow model\n        conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment\n        overwrite (bool): If True, overwrites existing contents of `path` parameter\n        gpu (bool): If True and `arm64` is True, modifies dependencies as needed by chassis for ARM64+GPU support\n        arm64 (bool): If True and `gpu` is True, modifies dependencies as needed by chassis for ARM64+GPU support\n    Returns:\n        None: This method does not return an object\n    Examples:\n    ```python\n    chassis_model = chassis_client.create_model(process_fn=process)\n    chassis_model.save(\"local_model_directory\")\n    ```\n    '''\nif overwrite and os.path.exists(path):\nshutil.rmtree(path)\nmlflow.pyfunc.save_model(path=path, python_model=self, conda_env=conda_env)\nif fix_env:\nfix_dependencies(path)\nif arm64 and gpu:\nfix_dependencies_arm_gpu(path)\nprint(\"Chassis model saved.\")\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel.test","title":"<code>test(self, test_input)</code>","text":"<p>Runs a sample inference test on a single input on chassis model locally</p> <p>Parameters:</p> Name Type Description Default <code>test_input</code> <code>Union[str, bytes, BufferedReader]</code> <p>Single sample input data to test model</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>raw model predictions returned by <code>process_fn</code> method</p> <p>Examples:</p> <pre><code>chassis_model = chassis_client.create_model(process_fn=process)\nsample_filepath = './sample_data.json'\nresults = chassis_model.test(sample_filepath)\n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def test(self,test_input):\n'''\n    Runs a sample inference test on a single input on chassis model locally\n    Args:\n        test_input (Union[str, bytes, BufferedReader]): Single sample input data to test model\n    Returns:\n        bytes: raw model predictions returned by `process_fn` method\n    Examples:\n    ```python\n    chassis_model = chassis_client.create_model(process_fn=process)\n    sample_filepath = './sample_data.json'\n    results = chassis_model.test(sample_filepath)\n    ```\n    '''\nif isinstance(test_input,_io.BufferedReader):\nresult = self.predict(None,test_input.read())\nelif isinstance(test_input,bytes):\nresult = self.predict(None,test_input)\nelif isinstance(test_input,str):\nif os.path.exists(test_input):\nresult = self.predict(None,open(test_input,'rb').read())\nelse:\nresult = self.predict(None,bytes(test_input,encoding='utf8'))\nelse:\nprint(\"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\")\nreturn False\nreturn result\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel.test_batch","title":"<code>test_batch(self, test_input)</code>","text":"<p>Takes a single input file, creates a batch of size <code>batch_size</code> defined in <code>ChassisModel.create_model</code>, and runs a batch job against chassis model locally if <code>batch_process_fn</code> is defined.</p> <p>Parameters:</p> Name Type Description Default <code>test_input</code> <code>Union[str, bytes, BufferedReader]</code> <p>Batch of sample input data to test model</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>raw model predictions returned by <code>batch_process_fn</code> method</p> <p>Examples:</p> <pre><code>chassis_model = chassis_client.create_model(process_fn=process)\nsample_input = sample_filepath = './sample_data.json'\nresults = chassis_model.test_batch(sample_filepath)\n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def test_batch(self,test_input):\n'''\n    Takes a single input file, creates a batch of size `batch_size` defined in `ChassisModel.create_model`, and runs a batch job against chassis model locally if `batch_process_fn` is defined.\n    Args:\n        test_input (Union[str, bytes, BufferedReader]): Batch of sample input data to test model\n    Returns:\n        bytes: raw model predictions returned by `batch_process_fn` method\n    Examples:\n    ```python\n    chassis_model = chassis_client.create_model(process_fn=process)\n    sample_input = sample_filepath = './sample_data.json'\n    results = chassis_model.test_batch(sample_filepath)\n    ```        \n    '''\nif not self.batch_input:\nraise NotImplementedError(\"Batch inference not implemented.\")\nif hasattr(self,'batch_predict'):\nbatch_method = self.batch_predict\nelse:\nbatch_method = self.predict\nif isinstance(test_input,_io.BufferedReader):\nresults = batch_method(None,[test_input.read() for _ in range(self.batch_size)])\nelif isinstance(test_input,bytes):\nresults = batch_method(None,[test_input for _ in range(self.batch_size)])\nelif isinstance(test_input,str):\nif os.path.exists(test_input):\nresults = batch_method(None,[open(test_input,'rb').read() for _ in range(self.batch_size)])\nelse:\nresults = batch_method(None,[bytes(test_input,encoding='utf8') for _ in range(self.batch_size)])\nelse:\nprint(\"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\")\nreturn False\nreturn results\n</code></pre>"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel.test_env","title":"<code>test_env(self, test_input_path, conda_env=None, fix_env=True)</code>","text":"<p>Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service.</p> <p>NOTE: This method is not available in the publicly-hosted service.</p> <p>Parameters:</p> Name Type Description Default <code>test_input_path</code> <code>str</code> <p>Filepath to sample input data</p> required <code>conda_env</code> <code>str</code> <p>Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment</p> <code>None</code> <code>fix_env</code> <code>bool</code> <p>Modifies conda or pip-installable packages into list of dependencies to be installed during the container build</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict</code> <p>raw model predictions returned by <code>process_fn</code> or <code>batch_process_fn</code> run from within chassis service</p> <p>Examples:</p> <pre><code>chassis_model = chassis_client.create_model(process_fn=process)\nsample_filepath = './sample_data.json'\nresults = chassis_model.test_env(sample_filepath)\n</code></pre> Source code in <code>chassisml_sdk/chassisml/chassisml.py</code> <pre><code>def test_env(self,test_input_path,conda_env=None,fix_env=True):\n'''\n    Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service.\n    **NOTE**: This method is not available in the publicly-hosted service.\n    Args:\n        test_input_path (str): Filepath to sample input data\n        conda_env (str): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment\n        fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build\n    Returns:\n        Dict: raw model predictions returned by `process_fn` or `batch_process_fn` run from within chassis service\n    Examples:\n    ```python\n    chassis_model = chassis_client.create_model(process_fn=process)\n    sample_filepath = './sample_data.json'\n    results = chassis_model.test_env(sample_filepath)\n    ```        \n    '''\nmodel_directory = os.path.join(tempfile.mkdtemp(),CHASSIS_TMP_DIRNAME)\nmlflow.pyfunc.save_model(path=model_directory, python_model=self, conda_env=conda_env, \nextra_pip_requirements = None if conda_env else [\"chassisml=={}\".format(__version__)])\nif fix_env:\nfix_dependencies(model_directory)\n# Compress all files in model directory to send them as a zip.\ntmppath = tempfile.mkdtemp()\nzipdir(model_directory,tmppath,MODEL_ZIP_NAME)\nwith open('{}/{}'.format(tmppath,MODEL_ZIP_NAME),'rb') as model_f, \\\n            open(test_input_path,'rb') as test_input_f:\nfiles = [\n('sample_input', test_input_f),\n('model', model_f)\n]\nprint('Starting test job... ', end='', flush=True)\nif self.chassis_auth_header:\nres = requests.post(self.chassis_test_url, files=files, headers={'Authorization': self.chassis_auth_header}, verify=self.ssl_verification)\nelse:\nres = requests.post(self.chassis_test_url, files=files, verify=self.ssl_verification)\nres.raise_for_status()\nprint('Ok!')\nshutil.rmtree(tmppath)\nshutil.rmtree(model_directory)\nreturn res.json()\n</code></pre>"},{"location":"common-errors/","title":"Frequently Asked Questions","text":""},{"location":"common-errors/#service-congifuration-and-setup","title":"Service Congifuration and Setup","text":"<p>Does Chassis only build and push public container images to Docker Hub?</p> <p>No, as a configuration step before you install the Chassis helm charts, you can set up a private Docker registry that Chassis will use as the destination to push all container images. However, note that this is only available if you set up Chassis manually. Follow our manual install and private registry support guides for more details.  </p> <p>Why am I getting an OOM error in Kubernetes when I run a Chassis job?</p> <p>If you are seeing an OOM error, you likely installed Chassis in a local Kubernetes cluster (e.g., minikube, Kubernetes in Docker Desktop, etc.). If the pod that was running your Chassis job has an <code>OOM</code> status, that means the pod did not have enough memory to complete the process. To further verify this, check the logs of the pod: </p> <pre><code>kubectl logs chassis-builder-job-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx-xxxx kaniko\n</code></pre> <p>If the logs do not end in an error, or in other words, the job seemingly just stops randomly, this likely means the pod was killed mid-process due to a lack of RAM, which can happen when building container images for larger models (typically ~3-5GB or higher). To fix this, stop your cluster and before restarting it, make sure you have allocated enough RAM and CPUs to complete your jobs.  </p>"},{"location":"common-errors/#sdk-and-user-workflow","title":"SDK and User Workflow","text":"<p>What is the purpose of the <code>process</code> function?</p> <p>Amongst other benefits the service provides, Chassis abstracts any user interaction with containers, Kubernetes, or web servers - all of which are essential technologies to operationalize machine learning models. In doing so, Chassis performs a lot of automatic inferring of package dependencies, objects, or utility functions your model requires to run inference. Consequently, the <code>process</code> function gives users the opportunity to include all relevent inference code in a single function. In short, you can think of the <code>process</code> function as your model inference function that can access previously defined functions, model objects, variables, and dependencies. Chassis will simply take everything included in this function, serialize it as a <code>ChassisModel</code> object, and build a Docker container out of these assets.   </p> <p>What are the required format(s) of my <code>process</code> function's input and outputs?</p> <p>Throughout the site's tutorials, examples, and how-to-guides, you have likely seen mention of a <code>process</code> function. This function acts as your model inferencing function and is required to create your <code>ChassisModel</code> object. There are two things you must consider when creating your <code>process</code> function:</p> <ol> <li>The input parameter for this method will always be of type <code>bytes</code>. As a result, you must adjust your code to process the input data as raw bytes. This data format is likely different than you are used to working with (e.g., if your model normally expects a text string, say \"Hello World!\", you will need to decode the raw bytes into the expected format, or a text string). If working with raw bytes is unfamiliar to you, no need to worry! We document and include code snippets for decoding raw bytes in many common data types. Check out Support for Common Data Types to learn more.</li> <li>The output of your <code>process</code> function must be JSON serializable. In other words, the Chassis service will take the output of your process function and feed it to a <code>json.dumps()</code> method. For example, if you wanted your model to return JSON, your <code>process</code> function should simply return a <code>Dict</code> object, which can be serialized with a <code>json.dumps()</code> method. </li> </ol>"},{"location":"common-errors/#general","title":"General","text":"<p>What is Chassis?</p> <p>Check out our Conceptual Guides page to learn more!</p> <p>How can Chassis be used?</p> <p>Chassis is a powerful tool that can be used to accelerate and automate workflows. For data scientists, Chassis provides an inuitive interface to automatically build containers out of models without any prior knowledge of Docker. Machine learning engineers and DevOps teams can leverage the Chassis API to enhance CI/CD processes with automatic model containerization and deployment to their serving platform of choice. Below are some examples of Chassis in action:</p> <ul> <li>Jupyter Notebook Examples</li> <li>GitHub Actions Example</li> </ul>"},{"location":"get-involved/","title":"Get Involved","text":""},{"location":"get-involved/#contributors","title":"Contributors","text":"<p>We are actively looking for new contributors to join us! Feel free to fork and open pull requests to contribute to modzy/chassis.</p> <p>A full list of contributors can be found here.</p>"},{"location":"get-involved/#community","title":"Community","text":"<p>Join the <code>#chassisml</code> channel on Modzy's Discord Server where our maintainers meet to plan changes and improvements.</p> <p>We also have a <code>#chassis-model-builder</code> Slack channel on the MLOps.community Slack!</p>"},{"location":"release-notes/","title":"Release Notes","text":""},{"location":"release-notes/#v145","title":"v1.4.5","text":"<p>July 2022 v1.4.5 release of Chassis inlcudes:</p> <ul> <li>Support for private docker registry configuration</li> <li>Removal of automatic model deployment to Modzy</li> </ul>"},{"location":"release-notes/#v100","title":"v1.0.0","text":"<p>March 2022</p> <p>v1.0.0 release of Chassis includes:</p> <ul> <li>GPU support</li> <li>Batch processing support </li> <li>Arm64 compatibility</li> <li>Open Model Interface compliance check</li> </ul>"},{"location":"release-notes/#v001","title":"v0.0.1","text":"<p>July 2021</p> <p>First working release of Chassis.</p>"},{"location":"service-reference/","title":"Chassisml API Reference","text":"<p>Welcome to the Chassisml API Reference documentation homepage. The API is designed using the principles of REST services using standard HTTP verbs and status codes, implemented with Flask. On this page, you will find documentation for:</p> <ul> <li>Available REST endpoints in API Service</li> <li>Methods implemented within the each endpoint</li> </ul>"},{"location":"service-reference/#endpoints","title":"Endpoints","text":"<p><code>/health</code> (GET)</p> <ul> <li>Confirms Chassis service is up and running </li> </ul> <p><code>/build</code> (POST)</p> <ul> <li>Kicks off the container image build process</li> </ul> <p><code>/job/{job_id}</code> (GET)</p> <ul> <li>Retrieves the status of a chassis <code>/build</code> job</li> </ul> <p><code>/job/{job_id}/download-tar</code> (GET)</p> <ul> <li>Retrieves docker image tar archive from a volume attached to the Kubernetes cluster hosting chassis and downloads it to a local filepath</li> </ul> <p><code>/test</code> (POST)</p> <ul> <li>Creates a conda environment as specified by the user's model artifacts and runs the <code>ChassisModel</code> to ensure the model code can run within the provided conda environment</li> </ul>"},{"location":"service-reference/#service.app-functions","title":"Functions","text":""},{"location":"service-reference/#service.app.build_image","title":"<code>build_image()</code>","text":"<p>This method is run by the <code>/build</code> endpoint.  It generates a model image based upon a POST request. The <code>request.files</code> structure can be seen in the Python SDK docs.</p> <p>Parameters:</p> Name Type Description Default <code>None</code> <code>None</code> <p>This method does not take any parameters</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>information about whether or not the image build resulted in an error</p> Source code in <code>service/app.py</code> <pre><code>def build_image():\n'''\n    This method is run by the `/build` endpoint. \n    It generates a model image based upon a POST request. The `request.files` structure can be seen in the Python SDK docs.\n    Args:\n        None (None): This method does not take any parameters\n    Returns:\n        Dict: information about whether or not the image build resulted in an error\n    '''\nif not ('image_data' in request.files and 'model' in request.files):\nreturn 'Both model and image_data are required', 500\n# retrieve image_data and populate variables accordingly\nimage_data = json.load(request.files.get('image_data'))\nmodel_name = image_data.get('model_name')\nimage_name = image_data.get('name')\ngpu = image_data.get('gpu')\narm64 = image_data.get('arm64')\npublish = image_data.get('publish', False)\npublish = True if publish else ''\nregistry_auth = image_data.get('registry_auth')\nwebhook = image_data.get('webhook')\n# retrieve binary representations for all three variables\nmodel = request.files.get('model')\nmetadata_data = request.files.get('metadata_data')\n# This is a future proofing variable in case we encounter a model that cannot be converted into mlflow.\n# It will remain hardcoded for now.\nmodule_name = 'mlflow'\n# This name is a random id used to ensure that all jobs are uniquely named and traceable.\nrandom_name = str(uuid.uuid4())\n# Unzip model archive\nif PV_MODE:\nunzip_model(model, module_name, random_name)\ncontext_uri = None\nelse:\ndockerfile = choose_dockerfile(gpu,arm64)\ncontext_uri = upload_context(model, module_name, random_name, metadata_data, dockerfile)\nif not context_uri:\nreturn Response(f\"403 Forbidden: Cloud storage credentials could not push to context bucket.\",403)\nmetadata_path = extract_metadata(metadata_data, module_name, random_name)\n# this path is the local location that kaniko will store the image it creates\npath_to_tar_file = f'{DATA_DIR if PV_MODE else \"/tar\"}/kaniko_image-{random_name}.tar'\nlogger.debug(f'Request data: {image_name}, {module_name}, {model_name}, {path_to_tar_file}')\nerror = run_kaniko(\nimage_name,\nmodule_name,\nmodel_name,\npath_to_tar_file,\nrandom_name,\npublish,\nregistry_auth,\ngpu,\narm64,\ncontext_uri,\nmetadata_path,\nwebhook\n)\nif error:\nreturn {'error': error, 'job_id': None}\nreturn {'error': False, 'job_id': f'{K_JOB_NAME}-{random_name}'}\n</code></pre>"},{"location":"service-reference/#service.app.create_job_object","title":"<code>create_job_object(image_name, module_name, model_name, path_to_tar_file, random_name, publish, registry_auth, gpu=False, arm64=False, context_uri=None, metadata_path=None)</code>","text":"<p>This utility method sets up all the required objects needed to create a model image and is run within the <code>run_kaniko</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>container image name</p> required <code>module_name</code> <code>str</code> <p>reference module to locate location within service input is saved</p> required <code>model_name</code> <code>str</code> <p>name of model to package</p> required <code>path_to_tar_file</code> <code>str</code> <p>filepath destination to save docker image tar file</p> required <code>random_name</code> <code>str</code> <p>random id generated during build process that is used to ensure that all jobs are uniquely named and traceable</p> required <code>publish</code> <code>bool</code> <p>determines if image will be published to Docker registry</p> required <code>registry_auth</code> <code>dict</code> <p>Docker registry authorization credentials  </p> required <code>gpu</code> <code>bool</code> <p>If <code>True</code>, will build container image that runs on GPU </p> <code>False</code> <code>arm64</code> <code>bool</code> <p>If <code>True</code>, will build container image that runs on ARM64 architecture</p> <code>False</code> <code>context_uri</code> <code>str</code> <p>Location of build context in S3 (S3 mode only)</p> <code>None</code> <p>Returns:</p> Type Description <code>Job</code> <p>Chassis job object</p> Source code in <code>service/app.py</code> <pre><code>def create_job_object(\nimage_name,\nmodule_name,\nmodel_name,\npath_to_tar_file,\nrandom_name,\npublish,\nregistry_auth,\ngpu=False,\narm64=False,\ncontext_uri=None,\nmetadata_path=None\n):\n'''\n    This utility method sets up all the required objects needed to create a model image and is run within the `run_kaniko` method.\n    Args:\n        image_name (str): container image name\n        module_name (str): reference module to locate location within service input is saved\n        model_name (str): name of model to package\n        path_to_tar_file (str): filepath destination to save docker image tar file\n        random_name (str): random id generated during build process that is used to ensure that all jobs are uniquely named and traceable\n        publish (bool): determines if image will be published to Docker registry\n        registry_auth (dict): Docker registry authorization credentials  \n        gpu (bool): If `True`, will build container image that runs on GPU \n        arm64 (bool): If `True`, will build container image that runs on ARM64 architecture\n        context_uri (str): Location of build context in S3 (S3 mode only)\n    Returns:\n        Job: Chassis job object\n    '''\njob_name = f'{K_JOB_NAME}-{random_name}'\nif registry_auth and not REGISTRY_URI:\n# credential setup for Docker Hub.\n# json for holding registry credentials that will access docker hub.\n# reference: https://github.com/GoogleContainerTools/kaniko#pushing-to-docker-hub\nregistry_credentials = f'{{\"auths\":{{\"https://index.docker.io/v1/\":{{\"auth\":\"{registry_auth}\"}}}}}}'\nb64_registry_credentials = base64.b64encode(registry_credentials.encode(\"utf-8\")).decode(\"utf-8\")\nelif registry_auth and REGISTRY_URI:\nregistry_credentials = f'{{\"auths\":{{\"{REGISTRY_URI}\":{{\"auth\":\"{registry_auth}\"}}}}}}'\nb64_registry_credentials = base64.b64encode(registry_credentials.encode(\"utf-8\")).decode(\"utf-8\")\nelif not registry_auth and not REGISTRY_CREDENTIALS:\nraise ValueError(\"No registry credentials provided by user or during Chassis installation.\")\nelse:\nb64_registry_credentials = REGISTRY_CREDENTIALS\n# mount path leads to /data\n# this is a mount point. NOT the volume itself.\n# name aligns with a volume defined below.\nif PV_MODE:\ndata_volume_mount = client.V1VolumeMount(\nmount_path=MOUNT_PATH_DIR,\nname=\"local-volume-code\"\n) if CHASSIS_DEV else client.V1VolumeMount(\nmount_path=MOUNT_PATH_DIR,\nname=DATA_VOLUME_NAME\n)\n# This volume will be used by kaniko container to get registry credentials.\n# mount path leads to /kaniko/.docker per kaniko reference documentation\n# this is a mount point. NOT the volume itself.\n# name aligns with a volume defined below.\nkaniko_credentials_volume_mount = client.V1VolumeMount(\nmount_path=K_KANIKO_EMPTY_DIR_PATH,\nname=K_EMPTY_DIR_NAME\n)\n# create secret for registry credentials       \nregistry_creds_secret_name = f'{random_name}-creds'\nmetadata = {'name': registry_creds_secret_name, 'namespace': ENVIRONMENT}\ndata = {'config.json': b64_registry_credentials}\napi_version = 'v1'\nkind = 'Secret'\nsecret = client.V1Secret(api_version, data , kind, metadata)\nclient.CoreV1Api().create_namespaced_secret(ENVIRONMENT, secret)\n# volume holding credentials\nkaniko_credentials_volume = client.V1Volume(\nname=K_EMPTY_DIR_NAME,\nsecret=client.V1SecretVolumeSource(secret_name=registry_creds_secret_name)\n)\n# This is the kaniko container used to build the final image.\nkaniko_args = [\n'' if publish else '--no-push',\nf'--destination={REGISTRY_URI+\"/\" if REGISTRY_URI else \"\"}{REPOSITORY_PREFIX}{image_name}{\"\" if \":\" in image_name else \":latest\"}',\n'--snapshotMode=redo',\n'--use-new-run',\nf'--build-arg=MODEL_DIR=model-{random_name}',\nf'--build-arg=MODZY_METADATA_PATH={metadata_path if metadata_path is not None else \"flavours/mlflow/interfaces/modzy/asset_bundle/0.1.0/model.yaml\"}',\nf'--build-arg=MODEL_NAME={model_name}',\nf'--build-arg=MODEL_CLASS={module_name}',\n# Modzy is the default interface.\n'--build-arg=INTERFACE=modzy',\n]\nvolumes = [kaniko_credentials_volume]\nkaniko_volume_mounts = [kaniko_credentials_volume_mount]\nbase_resources = {\"memory\": \"8Gi\", \"cpu\": \"2\"}\nslim_reqs = {\"memory\": \"2Gi\", \"cpu\": \"1\"}\nkaniko_reqs = client.V1ResourceRequirements(limits=base_resources, requests=base_resources)\nif PV_MODE:\ndockerfile = choose_dockerfile(gpu,arm64)\nkaniko_args.extend([f'--dockerfile={DATA_DIR}/flavours/{module_name}/{dockerfile}',f'--context={DATA_DIR}',\nf'--tarPath={path_to_tar_file}',])\nkaniko_volume_mounts.append(data_volume_mount)\ninit_container_kaniko = client.V1Container(\nname='kaniko',\nimage='gcr.io/kaniko-project/executor:v1.8.1',\nvolume_mounts=kaniko_volume_mounts,\nresources=kaniko_reqs,\nargs=kaniko_args\n)\n# volume claim\ndata_pv_claim = client.V1PersistentVolumeClaimVolumeSource(\nclaim_name=\"dir-claim-chassis\"\n) if CHASSIS_DEV else client.V1PersistentVolumeClaimVolumeSource(\nclaim_name=DATA_VOLUME_CLAIM_NAME\n)\n# volume holding data\ndata_volume = client.V1Volume(\nname=\"local-volume-code\",\npersistent_volume_claim=data_pv_claim\n) if CHASSIS_DEV else client.V1Volume(\nname=DATA_VOLUME_NAME,\npersistent_volume_claim=data_pv_claim\n)\nvolumes.append(data_volume)\nelse:\nkaniko_args.append(f'--context={context_uri}')\nif MODE==\"s3\":\nkaniko_s3_volume_mount = client.V1VolumeMount(\nmount_path='/root/.aws',\nname='storage-key'\n)\nkaniko_storage_key_volume = client.V1Volume(\nname='storage-key',\nsecret=client.V1SecretVolumeSource(secret_name=STORAGE_CREDENTIALS_SECRET_NAME)\n)\nkaniko_volume_mounts.append(kaniko_s3_volume_mount)\ninit_container_kaniko = client.V1Container(\nname='kaniko',\nimage='gcr.io/kaniko-project/executor:v1.8.1',\nvolume_mounts=kaniko_volume_mounts,\nenv=[client.V1EnvVar(name='AWS_REGION', value=AWS_REGION)],\nresources=kaniko_reqs,\nargs=kaniko_args\n)\nelif MODE==\"gs\":\nkaniko_gs_volume_mount = client.V1VolumeMount(\nmount_path='/secret',\nname='storage-key'\n)\nkaniko_storage_key_volume = client.V1Volume(\nname='storage-key',\nsecret=client.V1SecretVolumeSource(secret_name=STORAGE_CREDENTIALS_SECRET_NAME)\n)\nkaniko_volume_mounts.append(kaniko_gs_volume_mount)\ninit_container_kaniko = client.V1Container(\nname='kaniko',\nimage='gcr.io/kaniko-project/executor:v1.8.1',\nvolume_mounts=kaniko_volume_mounts,\nenv=[client.V1EnvVar(name='GOOGLE_APPLICATION_CREDENTIALS', value='/secret/storage-key.json')],\nresources=kaniko_reqs,\nargs=kaniko_args\n)\nelse:\nraise ValueError(\"Only allowed modes are: 'pv', 'gs', 's3'\")\nvolumes.append(kaniko_storage_key_volume)\n# Pod spec for the image build process\ninit_container_list = []\ncontainers_list = [init_container_kaniko]\npod_spec = client.V1PodSpec(\nservice_account_name=K_SERVICE_ACOUNT_NAME,\nrestart_policy='Never',\ninit_containers=init_container_list,\ncontainers=containers_list,\nvolumes=volumes\n)\n# setup and initiate model image build\ntemplate = client.V1PodTemplateSpec(\nmetadata=client.V1ObjectMeta(name=job_name),\nspec=pod_spec\n)\nspec = client.V1JobSpec(\nbackoff_limit=0,\ntemplate=template\n)\njob = client.V1Job(\napi_version='batch/v1',\nkind='Job',\nmetadata=client.V1ObjectMeta(\nname=job_name,\n),\nspec=spec\n)\nreturn job\n</code></pre>"},{"location":"service-reference/#service.app.download_tar","title":"<code>download_tar(job_id)</code>","text":"<p>This method is run by the <code>/job/{job_id}/download-tar</code> endpoint.  It downloads the container image from kaniko, built during the chassis job with the name <code>job_id</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>valid Chassis job identifier, generated by <code>create_job</code> method </p> required <p>Returns:</p> Type Description <code>Dict</code> <p>response from <code>download_tar</code> endpoint</p> Source code in <code>service/app.py</code> <pre><code>def download_tar(job_id):\n'''\n    This method is run by the `/job/{job_id}/download-tar` endpoint. \n    It downloads the container image from kaniko, built during the chassis job with the name `job_id`\n    Args:\n        job_id (str): valid Chassis job identifier, generated by `create_job` method \n    Returns:\n        Dict: response from `download_tar` endpoint\n    '''\nuid = job_id.split(f'{K_JOB_NAME}-')[1]\nif PV_MODE:\nreturn send_from_directory(DATA_DIR, path=f'kaniko_image-{uid}.tar', as_attachment=False)\nelse:\nreturn Response(f\"400 Bad Request: Tar download not available in production mode, please use 'docker pull ...'\",400)\n</code></pre>"},{"location":"service-reference/#service.app.get_job_status","title":"<code>get_job_status(job_id)</code>","text":"<p>This method is run by the <code>/job/{job_id}</code> endpoint. Based on a GET request, it retrieves the status of the Kaniko job and the results if the job has completed.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>valid Chassis job identifier, generated by <code>create_job</code> method</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary containing corresponding job data of job <code>job_id</code> </p> Source code in <code>service/app.py</code> <pre><code>def get_job_status(job_id):\n'''\n    This method is run by the `/job/{job_id}` endpoint.\n    Based on a GET request, it retrieves the status of the Kaniko job and the results if the job has completed.\n    Args:\n        job_id (str): valid Chassis job identifier, generated by `create_job` method\n    Returns:\n        Dict: Dictionary containing corresponding job data of job `job_id` \n    '''\nif CHASSIS_DEV:\n# if you are doing local dev you need to point at the local kubernetes cluster with your config file\nkubefile = os.getenv(\"CHASSIS_KUBECONFIG\")\nconfig.load_kube_config(kubefile)\nelse:\n# if the service is running inside a cluster during production then the config can be inherited\nconfig.load_incluster_config()\nbatch_v1 = client.BatchV1Api()\ntry:\njob = batch_v1.read_namespaced_job(job_id, ENVIRONMENT)\nannotations = job.metadata.annotations or {}\nresult = annotations.get('result')\nresult = json.loads(result) if result else None\nstatus = job.status.to_dict()\njob_data = {\n'result': result,\n'status': status\n}\nif status['failed']:\njob_data['logs'] = get_job_logs(job_id)\nreturn job_data\nexcept ApiException as e:\nlogger.error(f'Exception when getting job status: {e}')\nreturn e.body\n</code></pre>"},{"location":"service-reference/#service.app.test_model","title":"<code>test_model()</code>","text":"<p>This method is run by the <code>/test</code> endpoint. It creates a new conda environment from the provided <code>conda.yaml</code> file and then tests the provided model in that conda environment with provided test input file.</p> <p>Parameters:</p> Name Type Description Default <code>None</code> <code>None</code> <p>This method does not take any parameters</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>model response to <code>/test</code> endpoint. Should contain either successful predictions or error message</p> Source code in <code>service/app.py</code> <pre><code>def test_model():\n'''\n    This method is run by the `/test` endpoint. It creates a new conda environment from the provided `conda.yaml` file and then tests the provided model in that conda environment with provided test input file.\n    Args:\n        None (None): This method does not take any parameters\n    Returns:\n        Dict: model response to `/test` endpoint. Should contain either successful predictions or error message\n    '''\nif not ('sample_input' in request.files and 'model' in request.files):\nreturn 'Both sample input and model are required', 500\noutput_dict = {}\n# retrieve binary representations for both variables\nmodel = request.files.get('model')\nsample_input = request.files.get('sample_input')\n# This is a future proofing variable in case we encounter a model that cannot be converted into mlflow.\n# It will remain hardcoded for now.\nmodule_name = 'mlflow'\n# This name is a random id used to ensure that all jobs are uniquely named and traceable.\nrandom_name = str(uuid.uuid4())\n# Unzip model archive\nunzipped_path = unzip_model(model, module_name, random_name)\n# get sample input path\nsample_input_path = extract_sample_input(sample_input, module_name, random_name)\n# create conda env, return error if fails\ntry:\ntmp_env_name = str(time.time())\nrm_env_cmd = \"conda env remove --name {}\".format(tmp_env_name)\nyaml_path = os.path.join(unzipped_path,\"conda.yaml\")\ncreate_env_cmd = \"conda env create -f {} -n {}\".format(yaml_path,tmp_env_name)\nsubprocess.run(create_env_cmd, capture_output=True, shell=True, executable='/bin/bash', check=True)\nexcept subprocess.CalledProcessError as e:\nprint(e)\nsubprocess.run(rm_env_cmd, capture_output=True, shell=True, executable='/bin/bash')\noutput_dict[\"env_error\"] = e.stderr.decode()\nreturn output_dict\n# test model in env with sample input file, return error if fails\ntry:\ntest_model_cmd = \"\"\"\n        source activate {};\n        python test_chassis_model.py {} {}\n        \"\"\".format(tmp_env_name,unzipped_path,sample_input_path)\ntest_ret = subprocess.run(test_model_cmd, capture_output=True, shell=True, executable='/bin/bash', check=True)\noutput_dict[\"model_output\"] = test_ret.stdout.decode()\nexcept subprocess.CalledProcessError as e:\nsubprocess.run(rm_env_cmd, capture_output=True, shell=True, executable='/bin/bash')\noutput_dict[\"model_error\"] = e.stderr.decode()\nreturn output_dict\n# if we make it here, test was successful, remove env and return output\nsubprocess.run(rm_env_cmd, capture_output=True, shell=True, executable='/bin/bash')\nreturn output_dict\n</code></pre>"},{"location":"conceptual-guides/containers/","title":"Containers","text":""},{"location":"conceptual-guides/containers/#overview","title":"Overview","text":"<p>Chassis packages your model into a \"container\" image which can be run in different environments. Let's understand what containers are and why they're used.</p> <p>Without containers, applications which work in one environment often fail when moved to another environment. This is the problem that containers solve. A container can be defined as a package of software containing everything required to run your application. This includes things such as the application code, dependencies, runtime, system libraries, system settings, etc... </p>"},{"location":"conceptual-guides/containers/#how-to-use-containers","title":"How to Use Containers","text":"<p>The first step is to determine all of those components required for your application to run and build a container \"image\" which includes those components. This process can get complicated, luckily Chassis handles all of this for you!</p> <p>This \"image\" is a static, immutable file which can create a container when executed. It consists of layers which are added on to a \"parent\" or \"base\" image. This use of layers allows for efficient reuse of common components across images. In order to execute container images, a container \"engine\" such as Docker must be installed on the host machine to run and manage your containers. Containers share the host machine's operating system kernel allowing for multiple containers to run simultaneously on a single host. </p> <p>Containers can be stored and managed in \"registries\", which are collections of \"repositories\", which are collections of container images with the same name but different \"tags\", which are labels that convey information about specific versions of images. For machine learning models, we often use the model version number as the image tag.</p>"},{"location":"conceptual-guides/containers/#the-bottom-line","title":"The Bottom Line","text":"<p>Chassis makes it super easy to package your model into a container, which allows you to seamlessly deploy your model to a wide variety of environments.</p>"},{"location":"conceptual-guides/design/","title":"Design &amp; Architecture","text":"<p>Chassis makes it easy to create a deployable docker image from your trained ML model.</p> <p>The idea behind this project is to provide Data Scientists with a way to package their models into a Docker image. This image will manage to build the inference service compatible with several common platforms for free.</p> <ul> <li>Build models directly into DevOps-ready container images for inference (using MLflow under the hood)</li> <li>Supports parallel builds in Kubernetes jobs, using Kaniko, no Docker socket required!</li> <li>Generates Open Model Interface compatible images that are multi-purpose and portable, they work on multiple platforms: KServe and Modzy</li> <li>Try the test drive today, then deploy our Helm chart to your K8s cluster to use it for real</li> </ul> <p>At the moment, Chassis images are compatible with KServe and Modzy gRPC. This means you can deploy your built image into these platforms once it has been built.</p> <p>Deploy Chassis, send your model to it and start using the built container image to run inference on your data.</p>"},{"location":"conceptual-guides/design/#architecture","title":"Architecture","text":"<p>This diagram shows the overall architecture of the Chassis system:</p> <p></p> <p>This diagram zooms in on the generated container, showing the pluggable interface, configurable at runtime:</p> <p></p>"},{"location":"conceptual-guides/grpc/","title":"gRPC","text":""},{"location":"conceptual-guides/grpc/#overview","title":"Overview","text":"<p>Chassis builds model containers which support gRPC. Let's understand what it is and why it's used.</p> <p>RPC stands for \"Remote Procedure Call\", and it is a newer model for API design. In this model, a client script is able to call remote procedures, which are actually executing server-side, as if they were executing locally. </p> <p>gRPC is an RPC framework built by Google that uses the HTTP 2.0 protocol under the hood. In gRPC, code \"stubs\" are auto-generated (in different languages) for the client and server, so the details of the RPC to HTTP mapping are abstracted away for developers.</p>"},{"location":"conceptual-guides/grpc/#benefits-versus-traditional-rest","title":"Benefits Versus Traditional REST","text":""},{"location":"conceptual-guides/grpc/#client-server-interaction","title":"Client-Server Interaction","text":"<p>Traditional REST (REpresentational State Transfer) APIs are built using HTTP 1.1, which can support unary interactions (client sends single request, server sends back single response). Since gRPC uses HTTP 2.0, it can handle more complex client-server interactions in addition to those unary interactions, such as: client streaming (client sends stream of messages, server sends back single response), server streaming (client sends single message, server sends back stream of messages), bidirectional streaming (client and server send messages to each other in two independent streams). </p>"},{"location":"conceptual-guides/grpc/#data-transmission-format","title":"Data Transmission Format","text":"<p>Another key difference has to do with data transmission formats. REST mainly uses JSON or XML whereas gRPC uses protocol buffers (protobuf). Protobuf is a structured data serialization/deserialization mechanism developed by Google, and it is much more lightweight than JSON and XML. </p>"},{"location":"conceptual-guides/grpc/#the-bottom-line","title":"The Bottom Line","text":"<p>These key differences contribute to the significant speed increase typically observed using gRPC APIs as opposed to traditional REST APIs.</p>"},{"location":"conceptual-guides/overview/","title":"Overview","text":""},{"location":"conceptual-guides/overview/#goals","title":"Goals","text":"<p>Chassis is a Kubernetes service that can be deployed in your preferred cluster using Helm. It works by creating jobs that can be run in parallel to create Docker images that package ML models. It provides integration with most common deployment platforms so your model will be ready to be deployed in a simple way.</p> <p>It also provides a python SDK that makes it very easy to communicate with Chassis service in order to build your image.</p> <p>Simple</p> <ul> <li>Just make a request to build your image using the python SDK</li> <li>Small set of dependencies: mlflow, flask</li> <li>Supports multiple deployment platforms</li> <li>No DevOps knowledge needed</li> </ul> <p>Fast</p> <ul> <li>Start building the image as soon as you make the request</li> <li>Automatically upload the image to Docker Hub</li> <li>Image ready to be deployed</li> </ul> <p>Secure</p> <ul> <li>Using Kaniko to securely build the image</li> </ul>"},{"location":"conceptual-guides/overview/#non-goals","title":"Non-goals","text":"<p>Some non-goals of this project are:</p> <ul> <li>Deploy the built image - this is up to the platform that you deploy the container into, like KServe or Modzy</li> </ul>"},{"location":"conceptual-guides/overview/#intro-video","title":"Intro Video","text":""},{"location":"getting-started/getting-started/","title":"Getting Started","text":"<p>Welcome!</p> <p>In this getting started guide, you will learn how to use the Chassis SDK to build your first ML container by connecting to the publicly-hosted Chassis service.  Connecting to this service eliminates the need for you to deploy and stand up a private Kubernetes cluster. Each chassis job run on our hosted service has enough resources to containerize even the most memory intensive ML models (up to 8GB RAM and 2 CPUs).  </p>"},{"location":"getting-started/getting-started/#installation","title":"Installation","text":"<p>To get started, make sure you set up a Python virtual enviornment and install the Chassis SDK (Python 3.6 and above supported).</p> <pre><code>pip install chassisml\n</code></pre>"},{"location":"getting-started/getting-started/#try-it-out","title":"Try it out","text":"<p>Follow these steps to build your first model container (estimated time: 5 minutes)</p> <ol> <li>Clone the Chassis repository into your environment: <code>git clone https://github.com/modzy/chassis.git</code></li> <li>Install Jupyter in your conda or virtual environment if it is not already installed</li> <li>Execute the Python code below</li> </ol> <p>Note</p> <ul> <li>To follow along with the example code, you must create a free Docker Hub account if you do not already have one</li> <li>You have the option of either opening and running the pre-configured Example Notebook or following the below instructions</li> <li>The example code connects to the publicly-hosted Kubernetes service through this URL: <code>https://chassis.app.modzy.com</code>, which is hosted and managed in Modzy's public cloud</li> </ul> <p>In your Python environment, install the remaining dependencies required to run the example code.</p> <pre><code>pip install scikit-learn numpy\n</code></pre> <p>Finally, create a file in the <code>./getting-started/</code> directory called <code>example.py</code> and add this code:</p> <pre><code>import json\nimport pickle\nimport chassisml # (1)\nimport numpy as np\n# load model # (2)\nmodel = pickle.load(open(\"./model.pkl\", \"rb\"))\n# define process function # (3)\ndef process(input_bytes):\ninputs = np.array(json.loads(input_bytes))\ninference_results = model.predict(inputs)\nstructured_results = []\nfor inference_result in inference_results:\nstructured_output = {\n\"data\": {\n\"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]}\n}\n}\nstructured_results.append(structured_output)\nreturn structured_results\n# connect to Chassis client # (4)\nchassis_client = chassisml.ChassisClient(\"https://chassis.app.modzy.com/\")\n# create Chassis model # (5)\nchassis_model = chassis_client.create_model(process_fn=process)\n# test Chassis model # (6)\nsample_filepath = './digits_sample.json'\nresults = chassis_model.test(sample_filepath)\nprint(results)\n# publish model to Dockerhub # (7)\ndocker_user = \"&lt;insert-Docker Hub username&gt;\"\ndocker_pass = \"&lt;insert-Docker Hub password&gt;\"\nmodel_name = \"My First Chassis Model\"\nresponse = chassis_model.publish(\nmodel_name=model_name,\nmodel_version=\"0.0.1\",\nregistry_user=docker_user,\nregistry_pass=docker_pass\n) # (8)\n# wait for job to complete and print result # (9)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\nif final_status['status']['succeeded'] == 1:\nprint(\"Job Completed. View your new container image here: https://hub.docker.com/repository/docker/{}/{}\".format(docker_user, \"-\".join(model_name.lower().split(\" \"))))\nelse:\nprint(\"Job Failed. See logs below:\\n\\n{}\".format(final_status['logs']))\n</code></pre> <ol> <li>First, we will import the Chassis SDK. If you have not already done so, make sure you install it via PyPi: <code>pip install chassisml</code></li> <li>Next, we will load our model. For this example, we have a pre-trained Scikit-learn classifier saved as a pickle file (<code>./model.pkl</code>). When integrating Chassis into your own code, this can be done however you load your model. It could be loaded from a pickle file, checkpoint file, multiple configuration files, etc. The key is that you load your model into memory so it can be accessed in the below <code>process</code> function. </li> <li>Here, we will define our <code>process</code> function, which you can think of as an inference function for your model. This function can access objects loaded into memory (e.g., <code>model</code> loaded above), and the only requirement is it must convert input data in raw bytes form to the data type your model expects. See this guide for help on converting common data types. In this example, we process the raw bytes data using <code>numpy</code> and <code>json</code>, pass this processed data through to our model for predictions (<code>model.predict</code>), and perform some postprocessing to return the results in a human-readable manner. You can customize this function based on your model and preferences.    </li> <li>Here, we will connect to the publicly-hosted Chassis service.</li> <li>Now, we will simply create a <code>ChassisModel</code> object directly from our process function. See the reference docs for more details on this method.</li> <li>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> object by passing through a sample data path.</li> <li>Here is where you will need to add in your own Docker Hub credentials for the <code>docker_user</code> and <code>docker_pass</code> variables. Chassis will use these credentials when the container image is built and it is time to push it to a container registry.</li> <li>Finally, kick off the Chassis job. If you follow this example code as-is, this execution should take 4-5 minutes to complete. To see more parameter options for this method, view the reference docs</li> <li>After a successful Chassis job, these next few lines of code will check the final status of your job and print your the URL to your newly-built container!</li> </ol> <p>Next, run your script.</p> <pre><code>python example.py\n</code></pre> <p>In just a few minutes, the Chassis job will complete. Congratulations! You just built your first ML container from a Scikit learn digits classification model. Check out our Tutorials to learn about different ways to run this container.</p>"},{"location":"how-to-guides/arm-support/","title":"OS &amp; Arm Support","text":"<p>AI/ML models, Docker containers, and processors - what are they, how do they work together, and why mention them in relation to Chassisml?</p> <p>If you are familiar with the Chassisml service, you know that it is a tool Data Scientists can leverage to push their AI/ML models into a production application, without having any DevOps knowledge or experience. The way in which this happens is by auto-packaging model code into a Docker container, which allows the model to be shipped to various ModelOps/MLOps/Model serving platforms, across both the commercial and open-source landscapes. In most cases, these containers are built to run on Intel processors, which are more commonly found in larger devices such as desktop computers. This is great for running models in data centers or cloud-based infrastructure, but it does not bode well for running these models on any sort of mobile or edge device.</p> <p>ARM processors come in handy in these situations. As AI edge processing becomes more and more desirable (think AI running directly on a drone or security camera as an example), it is critical to be able to compile containers into an ARM-architecture supported format. </p> <p>This page walks through the process of automatically building a model container that can run on ARM, with the option to also make it GPU-compatible on an ARM architecture. </p>"},{"location":"how-to-guides/arm-support/#enable-arm-support","title":"Enable Arm Support","text":"<p>To get started, we will install our required dependencies.</p> <pre><code>import chassisml\nimport pickle\nimport cv2\nimport torch\nimport getpass\nimport numpy as np\nimport torchvision.models as models\nfrom torchvision import transforms\n</code></pre> <p>Next, we will load the pretrained ResNet50 model, define a data transformation object, and define a <code>device</code> variable. This is how we cast both our model and data to the CPU (<code>device=\"cpu\"</code>) or GPU (<code>device=\"cuda\"</code>).</p> <pre><code>model = models.resnet50(pretrained=True)\nmodel.eval()\nlabels = pickle.load(open('./data/imagenet_labels.pkl','rb'))\ntransform = transforms.Compose([\ntransforms.ToPILImage(),\ntransforms.Resize(224),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])        \ndevice = 'cpu'\nmodel.to(device)\n</code></pre> <p>Most deep learning frameworks have built-in support for batch processing. This support includes different dataloader functionalities that will take an entire folder of data in some cases and process it in a way that can be fed to a neural network in the proper tensor form. We will define a <code>batch_process</code> function that takes a list of inputs, formats them into the structure our model expects, and runs inference on the batch of data.</p> <pre><code>def batch_process(inputs):\n# preprocess list of inputs\nimages = []\nfor input_bytes in inputs:\ndecoded = cv2.imdecode(np.frombuffer(input_bytes, np.uint8), -1)\nresized = cv2.resize(decoded, (224, 224)).reshape((1,224,224,3))\nimages.append(resized)\nimages_arr = np.concatenate(images)\nbatch_t = torch.stack(tuple(transform(i) for i in images_arr), dim=0).to(device)\n# run batch inference and add softmax layer\noutput = model(batch_t)\nprobs = torch.nn.functional.softmax(output, dim=1)\nsoftmax_preds = probs.detach().cpu().numpy()\n# postprocess\nall_formatted_results = []\nfor preds in softmax_preds: \nindices = np.argsort(preds)[::-1]\nclasses = [labels[idx] for idx in indices[:5]]\nscores = [float(preds[idx]) for idx in indices[:5]]\npreds = [{\"class\": \"{}\".format(label), \"score\": round(float(score),3)} for label, score in zip(classes, scores)]\npreds.sort(key = lambda x: x[\"score\"],reverse=True)\nresults = {\"classPredictions\": preds}\nall_formatted_results.append(results)\n# output list of formatted results\nreturn all_formatted_results\n</code></pre> <p>When we create our <code>ChassisModel</code> object, we will pass this batch_process function through as a parameter. NOTE: If you would also like to define a <code>process</code> function that only performs inference on a single piece of data instead of batch, you can do so as well and pass both through as parameters. In this case, our <code>batch_process</code> will work if we pass through either a single piece of data or batch.</p> <p>Now, initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions, keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up.</p> <pre><code>chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")\nchassis_model = chassis_client.create_model(batch_process_fn=batch_process,batch_size=4)\n</code></pre> <p>Test <code>chassis_model</code> locally (both single and batch data).</p> <pre><code>sample_filepath = './data/airplane.jpg'\nresults = chassis_model.test(sample_filepath)\nprint(results)\nresults = chassis_model.test_batch(sample_filepath)\nprint(results)\n</code></pre> <p>Up until this point, creating a container that can run on ARM has been exactly the same as the normal Chassisml workflow. To enable ARM support, all we need to do is turn on the <code>arm64</code> flag.</p> <p>Turn this flag on and publish our model with your specified Docker credentials.</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhob_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"PyTorch ResNet50 Image Classification\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\narm64=True\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre>"},{"location":"how-to-guides/arm-support/#enable-arm-gpu-support","title":"Enable Arm + GPU Support","text":"<p>Note</p> <p>The ARM + GPU option is in alpha and has only been tested on the NVIDIA Jetson Nano device. </p> <p>Enabling ARM &amp; GPU support requires one more flag set to true during the <code>publish</code> method. Repeate the steps outlined in the above section with the one difference being a slight change the <code>device</code> variable.</p> <pre><code>model = models.resnet50(pretrained=True)\nmodel.eval()\nlabels = pickle.load(open('./data/imagenet_labels.pkl','rb'))\ntransform = transforms.Compose([\ntransforms.ToPILImage(),\ntransforms.Resize(224),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])        \ndevice = 'cuda'\nmodel.to(device)\n</code></pre> <p>Next, publish your model with both the <code>arm64</code> and <code>gpu</code> flags turned on.</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhob_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"PyTorch ResNet50 Image Classification\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\narm64=True,\ngpu=True\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre>"},{"location":"how-to-guides/common-data-types/","title":"Support for Common Data Types","text":"<p>When using Chassis, you must define a <code>process</code> function that serves as your model inference function. The sole parameter to this function, let's call it <code>input_bytes</code>, represents the input data for your model. This parameter will always be of type <code>bytes</code>, which means the beginning of your <code>process</code> function must account for this and be able to convert it to the expected data type for your model. This guide includes examples of how to decode raw bytes for common data types.</p> <p>Assume <code>input_bytes</code> is the parameter to your <code>process</code> function for each.</p>"},{"location":"how-to-guides/common-data-types/#text","title":"Text","text":""},{"location":"how-to-guides/common-data-types/#bytes-decoding","title":"Bytes Decoding","text":"<pre><code>text = input_bytes.decode()\n</code></pre>"},{"location":"how-to-guides/common-data-types/#corresponding-process-function","title":"Corresponding <code>Process</code> Function","text":"<pre><code>def process(input_bytes):\ntext = input_bytes.decode()\n'''\n    Perform processing and inference on text\n    '''\nreturn output\n</code></pre>"},{"location":"how-to-guides/common-data-types/#imagery","title":"Imagery","text":""},{"location":"how-to-guides/common-data-types/#bytes-decoding_1","title":"Bytes Decoding","text":"<p>OpenCV <pre><code>import cv2\nimport numpy as np\nimg = cv2.imdecode(np.frombuffer(input_bytes, np.uint8), -1)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n</code></pre></p> <p>Pillow <pre><code>import io\nfrom PIL import Image\nimg = Image.open(io.BytesIO(input_bytes)).convert(\"RGB\")\n</code></pre></p> <p>See also:</p> <ul> <li>Native PyTorch function</li> <li>Native Tensorflow function</li> </ul>"},{"location":"how-to-guides/common-data-types/#corresponding-process-function_1","title":"Corresponding <code>Process</code> Function","text":"<pre><code>def process(input_bytes):\nimg = cv2.imdecode(np.frombuffer(input_bytes, np.uint8), -1)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n'''\n    Perform processing and inference on img\n    '''\nreturn output\n</code></pre>"},{"location":"how-to-guides/common-data-types/#tabular","title":"Tabular","text":""},{"location":"how-to-guides/common-data-types/#bytes-decoding_2","title":"Bytes Decoding","text":"<pre><code>from io import StringIO\nimport pandas as pd\ninput_table = pd.read_csv(StringIO(str(input_bytes, \"utf-8\")))\n</code></pre>"},{"location":"how-to-guides/common-data-types/#corresponding-process-function_2","title":"Corresponding <code>Process</code> Function","text":"<pre><code>def process(input_bytes):\ninput_table = pd.read_csv(StringIO(str(input_bytes, \"utf-8\")))\n'''\n    Perform processing and inference on input_table\n    '''\nreturn output\n</code></pre>"},{"location":"how-to-guides/explainability/","title":"Adding in Explainability","text":""},{"location":"how-to-guides/explainability/#lime","title":"LIME","text":""},{"location":"how-to-guides/explainability/#shap","title":"SHAP","text":""},{"location":"how-to-guides/frameworks/","title":"Building Containers for Common Machine Learning and Deep Learning Frameworks","text":"<p>If you build models using a common machine learning framework and you want a way to containerize them automatically, you have come to the right place! This page provides guides for leveraging Chassis to create containers out of your models built from the most popular ML frameworks available.</p> <p>NOTE: This list of frameworks is not all-inclusive; instead, it is just a collection of examples from commonly-used frameworks we have seen data scientists gravitate towards. </p> <p>Can't find the framework you are looking for? Feel free to fork this repository, add an example or two from your framework of choice, and open a PR. Or come chat with us directly on Discord!</p> <p>Requirements</p> <p>To follow these how-to guides, you must first install the <code>chassisml</code> Python SDK and connect to the Chassis service either on your local machine or to our publicly-hosted instance within your Python IDE. You also will need an account with Dockerhub. </p> <p>For help getting started, visit our Tutorials page.</p>"},{"location":"how-to-guides/frameworks/#pytorch","title":"PyTorch","text":"<p>This guide builds a simple Image Classification model with a ResNet50 architecture, avaialable directly in PyTorch's Torvision model library.</p> <p>To follow along, you can reference the Jupyter notebook example and data files here.</p> <p>Import required dependencies.</p> <pre><code>import chassisml\nimport pickle\nimport cv2\nimport torch\nimport numpy as np\nimport torchvision.models as models\nfrom torchvision import transforms\n</code></pre> <p>Load model, labels, and any other dependencies required for inference.</p> <pre><code>model = models.resnet50(pretrained=True)\nmodel.eval()\nlabels = pickle.load(open('./data/imagenet_labels.pkl','rb'))\ntransform = transforms.Compose([\ntransforms.ToPILImage(),\ntransforms.Resize(224),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])        \ndevice = 'cpu'\n</code></pre> <p>Next, define <code>process</code> function that will be passed as the input parameter to create a <code>ChassisModel</code> object.</p> <pre><code>def process(input_bytes):\n# preprocess\ndecoded = cv2.imdecode(np.frombuffer(input_bytes, np.uint8), -1)\nimg_t = transform(decoded)\nbatch_t = torch.unsqueeze(img_t, 0).to(device)\n# run inference\npredictions = model(batch_t)\n# postprocess\npercentage = torch.nn.functional.softmax(predictions, dim=1)[0]\n_, indices = torch.sort(predictions, descending=True)\ninference_result = {\n\"classPredictions\": [\n{\"class\": labels[idx.item()], \"score\": percentage[idx].item()}\nfor idx in indices[0][:5] ]\n}\nstructured_output = {\n\"data\": {\n\"result\": inference_result,\n\"explanation\": None,\n\"drift\": None,\n}\n}\nreturn structured_output\n</code></pre> <p>Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions, keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up. </p> <pre><code>chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")\nchassis_model = chassis_client.create_model(process_fn=process)\n</code></pre> <p>Test <code>chassis_model</code> locally.</p> <pre><code>sample_filepath = './data/airplane.jpg'\nresults = chassis_model.test(sample_filepath)\nprint(results)\n</code></pre> <p>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> in the environment that will be built within the container. Note: Chassis will infer the required packages, functions, or variables required to successfully run inference within the <code>process</code> function. This step ensures when the conda environment is created within the Docker container, your model will run.</p> <p>NOTE: <code>test_env</code> function not available in publicly-hosted service.</p> <pre><code>test_env_result = chassis_model.test_env(sample_filepath)\nprint(test_env_result)\n</code></pre> <p>Lastly, publish your model with your Docker credentials.</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhub_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"PyTorch ResNet50 Image Classification\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre>"},{"location":"how-to-guides/frameworks/#scikit-learn","title":"Scikit-learn","text":"<p>This guide trains a Logistic Regression classifier on the Digits Dataset and auto-containerizes via the <code>chassisml</code> Python SDK. Visit the original classification exercise here.</p> <p>To follow along, you can reference the Jupyter notebook example and data files here.</p> <p>Import required dependencies.</p> <pre><code>import chassisml\nimport numpy as np\nimport getpass\nimport json\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\n</code></pre> <p>Load dataset, perform data preprocessing, and train the model.</p> <pre><code># Import and normalize data\nX_digits, y_digits = datasets.load_digits(return_X_y=True)\nX_digits = X_digits / X_digits.max()\nn_samples = len(X_digits)\n# Split data into training and test sets\nX_train = X_digits[: int(0.9 * n_samples)]\ny_train = y_digits[: int(0.9 * n_samples)]\nX_test = X_digits[int(0.9 * n_samples) :]\ny_test = y_digits[int(0.9 * n_samples) :]\n# Train Model\nlogistic = LogisticRegression(max_iter=1000)\nprint(\n\"LogisticRegression mean accuracy score: %f\"\n% logistic.fit(X_train, y_train).score(X_test, y_test)\n)\n# Save small sample input to use for testing later\nsample = X_test[:5].tolist()\nwith open(\"data/digits_sample.json\", 'w') as out:\njson.dump(sample, out)\n</code></pre> <p>Next, prepare a <code>process</code> function that will be passed as the input parameter to create a <code>ChassisModel</code> object.</p> <pre><code>def process(input_bytes):\ninputs = np.array(json.loads(input_bytes))\ninference_results = logistic.predict(inputs)\nstructured_results = []\nfor inference_result in inference_results:\nstructured_output = {\n\"data\": {\n\"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]}\n}\n}\nstructured_results.append(structured_output)\nreturn structured_results\n</code></pre> <p>Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions, keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up. </p> <pre><code>chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")\nchassis_model = chassis_client.create_model(process_fn=process)\n</code></pre> <p>Test <code>chassis_model</code> locally.</p> <pre><code>sample_filepath = './data/digits_sample.json'\nresults = chassis_model.test(sample_filepath)\nprint(results)\n</code></pre> <p>Manually construct a conda environment to pass through as parameter to Chassis job. NOTE: By default, Chassis will infer all required dependencies, objects, and functions based on what is referenced within the <code>process</code> method. However, if you prefer to manually define your environment, you can do so as well.</p> <pre><code>env = {\n\"name\": \"sklearn-chassis\",\n\"channels\": ['conda-forge'],\n\"dependencies\": [\n\"python=3.8.5\",\n{\n\"pip\": [\n\"scikit-learn\",\n\"numpy\",\n\"chassisml\"\n] \n}\n]\n}\n</code></pre> <p>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> in the environment that will be built within the container. Note: Chassis will infer the required packages, functions, or variables required to successfully run inference within the <code>process</code> function. This step ensures when the conda environment is created within the Docker container, your model will run.</p> <p>NOTE: <code>test_env</code> function not available in publicly-hosted service.</p> <pre><code>test_env_result = chassis_model.test_env(sample_filepath)\nprint(test_env_result)\n</code></pre> <p>Lastly, publish your model with your Docker credentials.</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhub_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"Sklearn Logistic Regression Digits Image Classification\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\nconda_env=env\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre>"},{"location":"how-to-guides/frameworks/#xgboost","title":"XGBoost","text":"<p>This guide trains an XGBoost regression model that predicts housing prices based on a series of features in the Boston Housing tabular dataset. </p> <p>To follow along, you can reference the Jupyter notebook example and data files here.</p> <p>Import required dependencies.</p> <pre><code>import cv2\nimport chassisml\nfrom io import StringIO\nimport numpy as np\nimport pandas as pd\nimport getpass\nimport json\nimport xgboost as xgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n</code></pre> <p>Load dataset, build XGBoost regressor model, and train the model.</p> <pre><code># load data\nboston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = pd.Series(boston.target)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n# save sample data for testing later\nwith open(\"data/sample_house_data.csv\", \"w\") as f:\nX_test[:10].to_csv(f, index=False)\n# build XGBoost regressor\nregressor = xgb.XGBRegressor(\nn_estimators=100,\nreg_lambda=1,\ngamma=0,\nmax_depth=3\n)\n# train model\nregressor.fit(X_train, y_train)\n</code></pre> <p>The <code>fit()</code> execution will print the following in your notebook: <pre><code>XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n             gamma=0, gpu_id=-1, importance_type=None,\n             interaction_constraints='', learning_rate=0.300000012,\n             max_delta_step=0, max_depth=3, min_child_weight=1, missing=nan,\n             monotone_constraints='()', n_estimators=100, n_jobs=12,\n             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n             validate_parameters=1, verbosity=None)\n</code></pre></p> <p>Run inference on your <code>X_test</code> subset and evaluate model performance.</p> <pre><code># run inference\ny_pred = regressor.predict(X_test)\n# evaluate model\nmean_squared_error(y_test, y_pred)\n&gt;&gt;&gt;&gt; 7.658965947061991\n</code></pre> <p>Next, prepare a <code>process</code> function that will be passed as the input parameter to create a <code>ChassisModel</code> object.</p> <pre><code>def process(input_bytes):\n# load data\ninputs = pd.read_csv(StringIO(str(input_bytes, \"utf-8\")))    \n# run inference\npreds = regressor.predict(inputs)\n# structure results\ninference_result = {\n\"housePricePredictions\": [\n{\"row\": i+1, \"price\": preds[i].round(0)*1000} for i in range(len(preds))\n]\n}\nstructured_output = {\n\"data\": {\n\"result\": inference_result,\n\"explanation\": None,\n\"drift\": None,\n}\n}\nreturn structured_output\n</code></pre> <p>Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions, keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up. </p> <pre><code>chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")\nchassis_model = chassis_client.create_model(process_fn=process)\n</code></pre> <p>Test <code>chassis_model</code> locally.</p> <pre><code>sample_filepath = './data/sample_house_data.csv'\nresults = chassis_model.test(sample_filepath)\nprint(results)\n</code></pre> <p>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> in the environment that will be built within the container. Note: Chassis will infer the required packages, functions, or variables required to successfully run inference within the <code>process</code> function. This step ensures when the conda environment is created within the Docker container, your model will successfully run.</p> <p>NOTE: <code>test_env</code> function not available in publicly-hosted service.</p> <pre><code>test_env_result = chassis_model.test_env(sample_filepath)\nprint(test_env_result)\n</code></pre> <p>Lastly, publish your model with your Docker credentials.</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhub_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"XGBoost Boston Housing Price Predictions\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre>"},{"location":"how-to-guides/frameworks/#lightgbm","title":"LightGBM","text":"<p>This guide builds a LightGBM classifier model to predict the likelihood of women being diagnosed with Breast Cancer. This guide was adapted from this Kaggle notebook. </p> <p>To follow along, you can reference the Jupyter notebook example and data files here.</p> <p>Import required dependencies.</p> <pre><code>import chassisml\nimport numpy as np\nimport getpass\nimport json\nimport pandas as pd\nfrom io import StringIO\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n</code></pre> <p>Load dataset, perform data preprocessing, build and then train LightGBM classifier model.</p> <pre><code># load breast cancer dataset\ndf = pd.read_csv('./data/Breast_cancer_data.csv')\n# preprocess data\nX = df[['mean_radius','mean_texture','mean_perimeter','mean_area','mean_smoothness']]\ny = df['diagnosis']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n# build and train model\nclf = lgb.LGBMClassifier()\nclf.fit(X_train, y_train)\n</code></pre> <p>Run inference on your test subset and evaluate model accuracy <pre><code>y_pred=clf.predict(X_test)\naccuracy=accuracy_score(y_pred, y_test)\nprint('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))\n</code></pre></p> <p>You should see this in your console: <pre><code>LightGBM Model accuracy score: 0.9298\n</code></pre></p> <p>Next, prepare a <code>process</code> function that will be passed as the input parameter to create a <code>ChassisModel</code> object.</p> <pre><code>labels = [\"No Cancer\", \"Cancer\"]\ndef process(input_bytes):\ninputs = pd.read_csv(StringIO(str(input_bytes, \"utf-8\")))\npreds = clf.predict_proba(inputs)\ninference_result = {\n\"classPredictions\": [\n{\"row\": i+1, \"class\": labels[np.argmax(pred)], \"score\": np.max(pred)} for i, pred in enumerate(preds)\n]\n}\nstructured_output = {\n\"data\": {\n\"result\": inference_result,\n\"explanation\": None,\n\"drift\": None,\n}\n}\nreturn structured_output\n</code></pre> <p>Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions, keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up. </p> <pre><code>chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")\nchassis_model = chassis_client.create_model(process_fn=process)\n</code></pre> <p>Test <code>chassis_model</code> locally.</p> <pre><code>sample_filepath = 'data/sample_cancer_data.csv'\nresults = chassis_model.test(sample_filepath)\nprint(results)\n</code></pre> <p>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> in the environment that will be built within the container. Note: Chassis will infer the required packages, functions, or variables required to successfully run inference within the <code>process</code> function. This step ensures when the conda environment is created within the Docker container, your model will successfully run.</p> <p>NOTE: <code>test_env</code> function not available in publicly-hosted service.</p> <pre><code>test_env_result = chassis_model.test_env(sample_filepath)\nprint(test_env_result)\n</code></pre> <p>Lastly, publish your model with your Docker credentials.</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhub_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"Chassis LightGBM Breast Cancer Classification\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre>"},{"location":"how-to-guides/frameworks/#fastai","title":"Fastai","text":"<p>This builds off Fastai's Tabular training tutorial, which predicts the income of adults based on various education and socioeconomic factors.  </p> <p>To follow along, you can reference the Jupyter notebook example and data files here.</p> <p>Import required dependencies.</p> <pre><code>import os\nimport chassisml\nimport numpy as np\nimport getpass\nimport json\nimport pandas as pd\nfrom io import StringIO\nfrom fastai.tabular.all import TabularDataLoaders, RandomSplitter, TabularPandas, tabular_learner, Categorify, FillMissing, Normalize, range_of, accuracy\n</code></pre> <p>Begin by loading and preprocessing dataset. </p> <pre><code>df = pd.read_csv(\"./data/adult_sample/adult.csv\")\ndf.head()\ndls = TabularDataLoaders.from_csv(\"./data/adult_sample/adult.csv\", path=os.path.join(os.getcwd(), \"data/adult_sample\"), y_names=\"salary\",\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\ncont_names = ['age', 'fnlwgt', 'education-num'],\nprocs = [Categorify, FillMissing, Normalize])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nsplits = RandomSplitter(valid_pct=0.2)(range_of(df))\nto = TabularPandas(df, procs=[Categorify, FillMissing,Normalize],\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\ncont_names = ['age', 'fnlwgt', 'education-num'],\ny_names='salary',\nsplits=splits)\n# save test subset\ntest_df = df.copy()\ntest_df.drop(['salary'], axis=1, inplace=True)\ntest_df[:20].to_csv(\"./data/sample_adult_data.csv\", index=False)\n# rebuild dataloaders with preprocessed data\ndls = to.dataloaders(bs=64)\n</code></pre> <p>Now, we will build and train our model.</p> <pre><code>learn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(1)\n</code></pre> <p>The output of this training experiment should look similar to the following table:</p> epoch train_loss valid_loss accuracy time 0 0.370123 0.364591 0.832002 00:03 <p>View the full results of the training experiment.</p> <pre><code>learn.show_results()\n</code></pre> <p>Now, we will test the inference flow that works with the Fastai framework.</p> <pre><code>test_df = pd.read_csv(\"./data/sample_adult_data.csv\")\ndl = learn.dls.test_dl(test_df)\npreds = learn.get_preds(dl=dl)[0].numpy()\n</code></pre> <p>Next, prepare a <code>process</code> function that will be passed as the input parameter to create a <code>ChassisModel</code> object.</p> <pre><code>labels = ['&lt;50k', '&gt;50k']\ndef process(input_bytes):\ninputs = pd.read_csv(StringIO(str(input_bytes, \"utf-8\")))\ndl = learn.dls.test_dl(inputs)\npreds = learn.get_preds(dl=dl)[0].numpy()\ninference_result = {\n\"classPredictions\": [\n{\n\"row\": i+1,\n\"predictions\": [\n{\"class\": labels[j], \"score\": round(pred[j], 4)} for j in range(2)\n]\n} for i, pred in enumerate(preds)\n]\n}\nstructured_output = {\n\"data\": {\n\"result\": inference_result,\n\"explanation\": None,\n\"drift\": None,\n}\n}\nreturn structured_output\n</code></pre> <p>Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions, keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up. </p> <pre><code>chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")\nchassis_model = chassis_client.create_model(process_fn=process)\n</code></pre> <p>Test <code>chassis_model</code> locally.</p> <pre><code>sample_filepath = './data/sample_adult_data.csv'\nresults = chassis_model.test(sample_filepath)\nprint(results)\n</code></pre> <p>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> in the environment that will be built within the container. Note: Chassis will infer the required packages, functions, or variables required to successfully run inference within the <code>process</code> function. This step ensures when the conda environment is created within the Docker container, your model will successfully run.</p> <p>NOTE: <code>test_env</code> function not available in publicly-hosted service.</p> <pre><code>test_env_result = chassis_model.test_env(sample_filepath)\nprint(test_env_result)\n</code></pre> <p>Lastly, publish your model with your Docker credentials.</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhub_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"Fast AI Salary Prediction\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre>"},{"location":"how-to-guides/frameworks/#mxnet","title":"MXNet","text":"<p>This guide leverages the MXNet framework to build a simple Image Classification model with a MobileNet architecture, avaialable directly in MXNet's Gluon Model Zoo. This guide is an adaptation of MXNet's tutorial for using pretrained models. </p> <p>To follow along, you can reference the Jupyter notebook example and data files here.</p> <p>Import required dependencies.</p> <pre><code>import cv2\nimport chassisml\nimport numpy as np\nimport getpass\nimport json\nimport mxnet as mx\nimport mxnet.image as mxnet_img\nfrom mxnet import gluon, nd\nfrom mxnet.gluon.model_zoo import vision\nimport matplotlib.pyplot as plt\n</code></pre> <p>Load pretrained <code>MobileNet</code> model from Gluon Model Zoo</p> <pre><code>ctx = mx.cpu()\nmobileNet = vision.mobilenet0_5(pretrained=True, ctx=ctx)\n</code></pre> <p>Next, load ImageNet labels file and sample dog image for testing. We will visualize this image in our notebook for reference.</p> <pre><code># load imagenet labels for postprocessing\nimagenet_labels = np.array(json.load(open('data/image_net_labels.json', 'r')))\nprint(imagenet_labels[4])\n# load sample image\nfilename = \"data/dog.jpg\"\n# visualize image to test\nimage = mx.image.imread(filename)\nplt.imshow(image.asnumpy())\n</code></pre> <p>An image of a boxer dog should appear under this cell.</p> <p>Now, we will define an inference method specific to the MXNet framework.</p> <pre><code>def predict(model, image, categories, k=3):\npredictions = model(transform(image)).softmax()\ntop_pred = predictions.topk(k=k)[0].asnumpy()\nprobs = []\nlabels = []\nfor index in top_pred:\nprobability = predictions[0][int(index)]\nprobs.append(probability.asscalar())\ncategory = categories[int(index)]\nlabels.append(category)\nreturn probs, labels\n</code></pre> <p>Test this function locally to ensure your model is behaving how you intend it to.</p> <pre><code># test model\nprobs, labels = predict(mobileNet, image, imagenet_labels, 3)\nprint(probs)\nprint(labels)\n&gt;&gt;&gt; [0.84015656, 0.13626784, 0.006610237]\n&gt;&gt;&gt; ['boxer', 'bull mastiff', 'Rhodesian ridgeback']\n</code></pre> <p>Next, define <code>process</code> function that will be passed as the input parameter to create a <code>ChassisModel</code> object.</p> <pre><code>def process(input_bytes):\n# read image bytes\nimg = mxnet_img.imdecode(input_bytes)\n# run inference\nprobs, labels = predict(mobileNet, img, imagenet_labels, 3)\n# structure results\ninference_result = {\n\"classPredictions\": [\n{\"class\": labels[i], \"score\": probs[i]}\nfor i in range(len(probs)) ]\n}\nstructured_output = {\n\"data\": {\n\"result\": inference_result,\n\"explanation\": None,\n\"drift\": None,\n}\n}\nreturn structured_output\n</code></pre> <p>Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions, keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up. </p> <pre><code>chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")\nchassis_model = chassis_client.create_model(process_fn=process)\n</code></pre> <p>Test <code>chassis_model</code> locally.</p> <pre><code>sample_filepath = './data/dog.jpg'\nresults = chassis_model.test(sample_filepath)\nprint(results)\n</code></pre> <p>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> in the environment that will be built within the container. Note: Chassis will infer the required packages, functions, or variables required to successfully run inference within the <code>process</code> function. This step ensures when the conda environment is created within the Docker container, your model will run.</p> <p>NOTE: <code>test_env</code> function not available in publicly-hosted service.</p> <pre><code>test_env_result = chassis_model.test_env(sample_filepath)\nprint(test_env_result)\n</code></pre> <p>Lastly, publish your model with your Docker credentials.</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhub_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"MXNET MobileNet Image Classifiction\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre>"},{"location":"how-to-guides/frameworks/#onnx","title":"ONNX","text":"<p>This guide leverages the <code>chassisml</code> SDK to auto-containerize a pretrained model from the ONNX Model Zoo. </p> <p>To follow along, you can reference the Jupyter notebook example and data files here.</p> <p>Import required dependencies.</p> <pre><code>import cv2\nimport pickle\nimport tempfile\nimport chassisml\nimport numpy as np\nimport getpass\nfrom shutil import rmtree\nimport json\nimport onnx\nfrom onnx import backend\nfrom onnx import numpy_helper\nimport onnxruntime as ort\nimport matplotlib.pyplot as plt\n</code></pre> <p>Load pretrained <code>MobileNet</code> model from ONNX Model Zoo and confirm it is a valid ONNX model.</p> <pre><code>model = onnx.load(\"models/mobilenetv2-7.onnx\")\nonnx.checker.check_model(model)\n</code></pre> <p>Next, load sample data and execute a few minor preprocessing steps.</p> <pre><code># format sample data and reshape\nimg = cv2.cvtColor(cv2.imread('data/dog.jpg'), cv2.COLOR_BGR2RGB)\nimg_show = cv2.resize(img, (224,224))\nsample_img = np.reshape(img_show, (1,3,224,224)).astype(np.float32)\n# load imagenet labels\nlabels = pickle.load(open('./data/imagenet_labels.pkl','rb'))\n# visualize image\nplt.figure()\nplt.imshow(img)\nplt.colorbar()\nplt.grid(False)\nplt.show()\n</code></pre> <p>An image of a yellow lab dog should appear under this cell.</p> <p>Now, we will create an ONNX runtime inference session and test our model locally. NOTE: Some frameworks have their own ONNX runtime compatibility built in, but ONNX Runtime (ORT) can be used to run models built in many common frameworks - including PyTorch, Tensorflow, TFLite, Scikit-Learn, to name a few - but saved in the standardized ONNX format. </p> <pre><code># create onnx runtime inference session and print top prediction\nsession = ort.InferenceSession(\"models/mobilenetv2-7.onnx\")\nresults = session.run(None, {\"input\": sample_img})\nprint(\"Top Prediction: {}\".format(labels[results[0].argmax()]))\n&gt;&gt;&gt; \"Top Prediction: sunglass\"\n</code></pre> <p>Next, define <code>process</code> function that will be passed as the input parameter to create a <code>ChassisModel</code> object. One strict requirement to use Chassis is that your model must be able to be loaded into memory and perform an inference call from it's loaded, pre-trained state. Due to the unique ORT syntax that requires a model file be passed through as a parameter when creating an ORT inference session (which means it must be loaded to memory as a part of the inference session instantiation, not loaded before), we will get a little creative and leverage Python's tempfile library to take the loaded model, save it to a temporary directory as a <code>.onnx</code> file, and load it back in during inference.  </p> <pre><code>def process(input_bytes):\n# save model to filepath for inference\ntmp_dir = tempfile.mkdtemp()\nimport onnx\nonnx.save(model, \"{}/model.onnx\".format(tmp_dir))\n# preprocess data\ndecoded = cv2.cvtColor(cv2.imdecode(np.frombuffer(input_bytes, np.uint8), -1), cv2.COLOR_BGR2RGB)\nimg = cv2.resize(decoded, (224,224))\nimg = np.reshape(img, (1,3,224,224)).astype(np.float32)\n# run inference\nsession = ort.InferenceSession(\"{}/model.onnx\".format(tmp_dir))\nresults = session.run(None, {\"input\": img})\n# postprocess\ninference_result = labels[results[0].argmax()]\n# format results\nstructured_result = {\n\"data\": {\n\"result\": {\"classPredictions\": [{\"class\": str(inference_result)}]}\n}\n}\n# remove temp directory\nrmtree(tmp_dir)\nreturn structured_result\n</code></pre> <p>Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions, keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up. </p> <pre><code>chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")\nchassis_model = chassis_client.create_model(process_fn=process)\n</code></pre> <p>Test <code>chassis_model</code> locally.</p> <pre><code>sample_filepath = './data/dog.jpg'\nresults = chassis_model.test(sample_filepath)\nprint(results)\n</code></pre> <p>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> in the environment that will be built within the container. Note: Chassis will infer the required packages, functions, or variables required to successfully run inference within the <code>process</code> function. This step ensures when the conda environment is created within the Docker container, your model will run.</p> <p>NOTE: <code>test_env</code> function not available in publicly-hosted service.</p> <pre><code>test_env_result = chassis_model.test_env(sample_filepath)\nprint(test_env_result)\n</code></pre> <p>Lastly, publish your model with your Docker credentials.</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhub_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"ONNX MobileNet Image Classifiction\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre>"},{"location":"how-to-guides/frameworks/#pmml","title":"PMML","text":"<p>Predictive Model Markup Language (PMML) is an XML-based model format that is widely accepted and used to save machine learning models. This guide leverages the <code>chassisml</code> SDK to auto-containerize a pretrained model from this PMML Scikit-Learn package model library. </p> <p>To follow along, you can reference the Jupyter notebook example and data files here.</p> <p>Import required dependencies.</p> <pre><code>import chassisml\nimport numpy as np\nimport getpass\nimport json\nfrom io import StringIO\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nfrom sklearn_pmml_model.ensemble import PMMLForestClassifier\n</code></pre> <p>First, load and preprocess training data. <pre><code># Prepare data\niris = load_iris()\nX = pd.DataFrame(iris.data)\nX.columns = np.array(iris.feature_names)\ny = pd.Series(np.array(iris.target_names)[iris.target])\ny.name = \"Class\"\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.33, random_state=123)\n# Create sample data for testing later\nwith open(\"data/sample_iris.csv\", \"w\") as f:\nXte[:10].to_csv(f, index=False)\n</code></pre></p> <p>Next, load the <code>.pmml</code> model file with the <code>sklearn_pmml</code> convenience package. We can then run a sample inference test with our test subset. </p> <pre><code># Load model\nclf = PMMLForestClassifier(pmml=\"models/randomForest.pmml\")\nlabels = clf.classes_.tolist()\n# Test model\nclf.predict(Xte)\nclf.score(Xte, yte)\n</code></pre> <p>Now, we will define some pre- and post-processing methods that our model can use during inference.</p> <pre><code>def preprocess_inputs(raw_input_bytes):\n# load data\ninputs = pd.read_csv(StringIO(str(raw_input_bytes, \"utf-8\")))\nreturn inputs\ndef postprocess_outputs(raw_predictions):\n# process output\ninference_result = {\n\"result\":[\n{\n\"row\": i+1,\n\"classPredictions\": [\n{\"class\": labels[idx], \"score\": results[idx]}\nfor idx in np.argsort(results)[::-1]\n]  \n} for i, results in enumerate(raw_predictions)\n] \n}    \n# format output\nstructured_output = {\n\"data\": {\n\"result\": inference_result[\"result\"],\n\"explanation\": None,\n\"drift\": None,\n}\n}\nreturn structured_output\n</code></pre> <p>Next, define <code>process</code> function that will be passed as the input parameter to create a <code>ChassisModel</code> object. </p> <pre><code>def process(input_bytes):\n# load data\ninputs = preprocess_inputs(input_bytes)\n# make predictions\noutput = clf.predict_proba(inputs)\n# process output\nstructured_output = postprocess_outputs(output)\nreturn structured_output\n</code></pre> <p>Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions, keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up. </p> <pre><code>chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")\nchassis_model = chassis_client.create_model(process_fn=process)\n</code></pre> <p>Test <code>chassis_model</code> locally.</p> <pre><code>sample_filepath = './data/sample_iris.csv'\nresults = chassis_model.test(sample_filepath)\nprint(results)\n</code></pre> <p>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> in the environment that will be built within the container. Note: Chassis will infer the required packages, functions, or variables required to successfully run inference within the <code>process</code> function. This step ensures when the conda environment is created within the Docker container, your model will run.</p> <p>NOTE: <code>test_env</code> function not available in publicly-hosted service.</p> <pre><code>test_env_result = chassis_model.test_env(sample_filepath)\nprint(test_env_result)\n</code></pre> <p>Lastly, publish your model with your Docker credentials.</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhub_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"PMML Random Forest Iris Classification\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre>"},{"location":"how-to-guides/frameworks/#tensorflow-keras","title":"Tensorflow &amp; Keras","text":"<p>Coming Soon</p>"},{"location":"how-to-guides/frameworks/#spacy","title":"Spacy","text":"<p>Coming Soon</p>"},{"location":"how-to-guides/frameworks/#spark-mllib","title":"Spark MLlib","text":"<p>Coming Soon</p>"},{"location":"how-to-guides/gpu-support/","title":"GPU Support","text":"<p>Data scientists who spend most of their time building and training new machine learning models are likely familiar with two buzzwords that are often brought up when discussing the required resources needed to conduct such experiments: Graphics Processing Unit (GPU) and batch processing. GPUs are designed to quickly process large quantities of data concurrently and in turn makes the batch processing of data more efficient. This is pivotal process for not only model training, but also model inference.</p> <p>If you are familiar with the Chassisml service, you will know that by default, Chassisml will automatically create containers that run on CPU. But what if there is a real need for batch inference in a production setting that processes significantly quicker running on a GPU? </p> <p>This page walks through the process of implementing both GPU and batch inference support.</p> <p>To follow along, you can reference the Jupyter notebook example and data files here.</p>"},{"location":"how-to-guides/gpu-support/#enable-batch-processing","title":"Enable Batch Processing","text":"<p>Batch processing goes hand in hand with GPU support. Enabling GPU support does accerate the model inferences execution, but to truly unlock the full potential of a GPU, batch processing is critical. So, we will build a simple Image Classification model with a ResNet50 architecture, avaialable directly in PyTorch's Torvision model library, and implement a batch processing function that takes advantage of GPU access.</p> <p>Note</p> <p>To add this support, you must have access to a GPU to test locally before submitting a Chassisml job. Most Python ML frameworks will also require you to set up CUDA on your machine.  </p> <p>To get started, we will install our required dependencies.</p> <pre><code>import chassisml\nimport pickle\nimport cv2\nimport torch\nimport getpass\nimport numpy as np\nimport torchvision.models as models\nfrom torchvision import transforms\n</code></pre> <p>Next, we will load the pretrained ResNet50 model, define a data transformation object, and define a <code>device</code> variable (this is how we cast both our model and data to the GPU).</p> <pre><code>model = models.resnet50(pretrained=True)\nmodel.eval()\nlabels = pickle.load(open('./data/imagenet_labels.pkl','rb'))\ntransform = transforms.Compose([\ntransforms.ToPILImage(),\ntransforms.Resize(224),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])        \ndevice = 'cuda'\nmodel.to(device)\n</code></pre> <p>Most deep learning frameworks have built-in support for batch processing. This support includes different dataloader functionalities that will take an entire folder of data in some cases and process it in a way that can be fed to a neural network in the proper tensor form. We will define a <code>batch_process</code> function that takes a list of inputs, formats them into the structure our model expects, and runs inference on the batch of data.</p> <pre><code>def batch_process(inputs):\n# preprocess list of inputs\nimages = []\nfor input_bytes in inputs:\ndecoded = cv2.imdecode(np.frombuffer(input_bytes, np.uint8), -1)\nresized = cv2.resize(decoded, (224, 224)).reshape((1,224,224,3))\nimages.append(resized)\nimages_arr = np.concatenate(images)\nbatch_t = torch.stack(tuple(transform(i) for i in images_arr), dim=0).to(device)\n# run batch inference and add softmax layer\noutput = model(batch_t)\nprobs = torch.nn.functional.softmax(output, dim=1)\nsoftmax_preds = probs.detach().cpu().numpy()\n# postprocess\nall_formatted_results = []\nfor preds in softmax_preds: \nindices = np.argsort(preds)[::-1]\nclasses = [labels[idx] for idx in indices[:5]]\nscores = [float(preds[idx]) for idx in indices[:5]]\npreds = [{\"class\": \"{}\".format(label), \"score\": round(float(score),3)} for label, score in zip(classes, scores)]\npreds.sort(key = lambda x: x[\"score\"],reverse=True)\nresults = {\"classPredictions\": preds}\nall_formatted_results.append(results)\n# output list of formatted results\nreturn all_formatted_results\n</code></pre> <p>When we create our <code>ChassisModel</code> object, we will pass this batch_process function through as a parameter. NOTE: If you would also like to define a <code>process</code> function that only performs inference on a single piece of data instead of batch, you can do so as well and pass both through as parameters. In this case, our <code>batch_process</code> will work if we pass through either a single piece of data or batch.</p> <p>Now, initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions, keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up.</p> <pre><code>chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")\nchassis_model = chassis_client.create_model(batch_process_fn=batch_process,batch_size=4)\n</code></pre> <p>Test <code>chassis_model</code> locally (both single and batch data).</p> <pre><code>sample_filepath = './data/airplane.jpg'\nresults = chassis_model.test(sample_filepath)\nprint(results)\nresults = chassis_model.test_batch(sample_filepath)\nprint(results)\n</code></pre>"},{"location":"how-to-guides/gpu-support/#enable-gpu-support","title":"Enable GPU Support","text":"<p>Up until this point, creating a container that can run on GPU has been very similar to the normal Chassisml workflow, with the one difference being the need to define a <code>batch_process_fn</code> method. The last procedural difference is a simple flag to turn on GPU support.</p> <p>Turn this flag on and publish our model with your specified Docker credentials.</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhob_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"PyTorch ResNet50 Image Classification\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\ngpu=True\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre>"},{"location":"how-to-guides/private-registry/","title":"Private Docker Registry Support","text":"<p>By default, installing the Chassis service will push all container images to a public Docker Hub account, which requires you to have valid credentials. If instead you have access to a private Docker registry and prefer your Chassis-built containers get pushed to your own registry, this guide walks through the process (and some examples) of setting up the Chassis service with the proper configuration.</p> <p>Important Notes</p> <ul> <li>Only HTTP API v2 compliant Docker registries are supported</li> <li>This configuration is only available if you deploy and host the service. The publicly-hosted version only pushes public images to your Docker Hub account </li> </ul>"},{"location":"how-to-guides/private-registry/#generate-kubernetes-secrets","title":"Generate Kubernetes Secrets","text":"<p>We first need to generate a Kubernetes secret of type <code>dockerconfigjson</code> that contains Docker registry credentials with push/pull permissions. This command varies slightly depending on your Docker registry.</p> Docker RegistryAmazon ECRAzure Container RegistryGoogle GCR <pre><code>kubectl create secret docker-registry &lt;registry-secret-name&gt; \\\n--docker-server=&lt;private-registry-url&gt; \\\n--docker-email=&lt;private-registry-email&gt; \\\n--docker-username=&lt;private-registry-user&gt; \\\n--docker-password=&lt;private-registry-password&gt;\n</code></pre> <pre><code>kubectl create secret docker-registry &lt;registry-secret-name&gt; \\\n--docker-server=&lt;AWS-Account&gt;.dkr.ecr.&lt;AWS-region&gt;.amazonaws.com \\\n--docker-username=AWS \\\n--docker-password=$(aws ecr get-login-password)\n</code></pre> <pre><code>kubectl create secret docker-registry &lt;registry-secret-name&gt; \\\n--docker-server=&lt;container-registry-name&gt;.azurecr.io \\\n--docker-username=&lt;service-principal-ID&gt; \\\n--docker-password=&lt;service-principal-password&gt;\n</code></pre> <p>We will use the JSON key method to generate a secret with valid GCR credentials. To do so, log into your Google Cloud Console, navigate to your service account and either generate a new JSON key or download an existing JSON key file. Use this file to generate your Kubernetes secret:</p> <p><pre><code>kubectl create secret docker-registry &lt;registry-secret-name&gt; \\\n--docker-server=&lt;container-registry-name&gt;.gcr.io \\\n--docker-username=_json_key \\\n--docker-password=\"$(cat ~/json-key-file.json)\"\n</code></pre> Replace <code>~/json-key-file,json</code> with the path to your JSON key file.      </p>"},{"location":"how-to-guides/private-registry/#create-valuesyml-file","title":"Create <code>values.yml</code> file","text":"<p>After our Kubernetes secret is successfully generated, we will need to add this secret to a <code>values.yml</code> file that will ultimately be used to modify a few values in the Chassis helm chart. In this yaml file, we will also specify the URL of our private registry. See examples for the above registry types below: </p> Docker RegistryAmazon ECRAzure Container RegistryGoogle GCR values.yml<pre><code>registry:\nurl: \"&lt;private-registry-url&gt;\"\ncredentialsSecretName: \"&lt;registry-secret-name&gt;\"\nrepositoryPrefix: \"\"\nimage:\npullPolicy: IfNotPresent\ntag: \"1f20586e050416239b055faa18baf35ce5707a32\" # Commit hash for latest version of Chassis service\n</code></pre> values.yml<pre><code>registry:\nurl: \"&lt;AWS-Account&gt;.dkr.ecr.&lt;AWS-region&gt;.amazonaws.com\"\ncredentialsSecretName: \"&lt;registry-secret-name&gt;\"\nrepositoryPrefix: \"\"\nimage:\npullPolicy: IfNotPresent\ntag: \"1f20586e050416239b055faa18baf35ce5707a32\" # Commit hash for latest version of Chassis service\n</code></pre> values.yml<pre><code>registry:\nurl: \"&lt;container-registry-name&gt;.azurecr.io\"\ncredentialsSecretName: \"&lt;registry-secret-name&gt;\"\nrepositoryPrefix: \"\"\nimage:\npullPolicy: IfNotPresent\ntag: \"1f20586e050416239b055faa18baf35ce5707a32\" # Commit hash for latest version of Chassis service\n</code></pre> values.yml<pre><code>registry:\nurl: \"&lt;container-registry-name&gt;.gcr.io\"\ncredentialsSecretName: \"&lt;registry-secret-name&gt;\"\nrepositoryPrefix: \"\"\nimage:\npullPolicy: IfNotPresent\ntag: \"1f20586e050416239b055faa18baf35ce5707a32\" # Commit hash for latest version of Chassis service\n</code></pre>"},{"location":"how-to-guides/private-registry/#install-chassis-service","title":"Install <code>Chassis</code> Service","text":"<p>Now, we just need to use our newly generated <code>values.yml</code> file to install the Chassis service using <code>helm</code>.</p> <pre><code>helm install chassis chassis/chassis -f values.yml\n</code></pre> <p>Visit this Installation guide for full installation details.</p>"},{"location":"tutorials/deploy-manual/","title":"Install Service Manually","text":"<p>Different Connection Options</p> <p>Before following this guide, note that you can connect to the Chassis service in one of two ways:</p> <ol> <li>Continue following this guide to install the Chassis service locally on a private Kubernetes cluster</li> <li>Bypass this guide and follow the instructions to connect to our publicly-hosted and free instance of the service </li> </ol>"},{"location":"tutorials/deploy-manual/#install-required-dependencies","title":"Install required dependencies","text":"<ul> <li> Install Docker Desktop<ul> <li>Try to run <code>docker ps</code><ul> <li>If you get a permissions error, follow instructions here</li> </ul> </li> </ul> </li> <li> Install Helm</li> </ul> <p>Note: If you prefer to use Minikube for your Kubernetes distribution, make sure it can access the internet. Otherwise, your Chassis build jobs will fail.</p>"},{"location":"tutorials/deploy-manual/#enable-kubernetes","title":"Enable Kubernetes","text":"<p>Follow these instructions to enable Kubernetes in Docker Desktop.</p>"},{"location":"tutorials/deploy-manual/#add-the-helm-repository","title":"Add the Helm repository","text":"<pre><code>helm repo add chassis https://modzy.github.io/chassis\n</code></pre> <p>After that we just need to update the Helm repos to fetch <code>Chassis</code> data.</p> <pre><code>helm repo update\n</code></pre>"},{"location":"tutorials/deploy-manual/#configure-private-docker-registry-settings-optional","title":"Configure private Docker registry settings (Optional)","text":"<p>By default, installing the Chassis service will push all container images to a public Docker Hub account, which requires you to have valid credentials. If instead you have access to a private Docker registry, you can add a few lines of configuration before installing the <code>Chassis</code> service using helm. Note: only HTTP API v2 compliant Docker registries are supported.</p> <p>1. Generate Kuberentes secret containing registry credentials</p> <p>We first need to generate a Kubernetes secret of type <code>dockerconfigjson</code> that contains Docker registry credentials with push/pull permissions.</p> <pre><code>kubectl create secret docker-registry &lt;registry-secret-name&gt; \\\n--docker-server=&lt;private-registry-url&gt; \\\n--docker-email=&lt;private-registry-email&gt; \\\n--docker-username=&lt;private-registry-user&gt; \\\n--docker-password=&lt;private-registry-password&gt;\n</code></pre> <p>Visit Managing Secrets using kubectl for more details.</p> <p>2. Create Values File for Helm Chart</p> <p>Next, we will create a <code>values.yml</code> file to modify a few of the default values in the Chassis helm chart. This is where you can specify the base URL of your private registry and provide the name of the secret generated in the previous step. </p> values.yml<pre><code>registry:\nurl: \"https://my-private-registry.com\"\ncredentialsSecretName: \"&lt;registry-secret-name&gt;\"\nrepositoryPrefix: \"\"\nimage:\npullPolicy: IfNotPresent\ntag: \"1f20586e050416239b055faa18baf35ce5707a32\" # Commit hash for latest version of Chassis service\n</code></pre> <p>For more details and different registry examples, visit the Private Registry Support guide.</p>"},{"location":"tutorials/deploy-manual/#install-chassis-service","title":"Install <code>Chassis</code> service","text":"<p>Now we just need to install <code>Chassis</code> as normal using Helm.</p> Public Docker Hub (default)Private Registry <pre><code>helm install chassis chassis/chassis\n</code></pre> <pre><code>helm install chassis chassis/chassis -f values.yaml\n</code></pre>"},{"location":"tutorials/deploy-manual/#check-the-installation","title":"Check the installation","text":"<p>After having installed the service we can check that the <code>Chassis</code> service is correctly deployed.</p> <pre><code>kubectl get svc/chassis\n</code></pre> <p>Then you should see an output similar to this.</p> <pre><code>NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nchassis   NodePort   10.106.209.207   &lt;none&gt;        5000:30496/TCP   15s\n</code></pre> <p>We can also check that the pod that runs the service is correctly running.</p> <pre><code>kubectl get pods\n</code></pre> <p>Where we should find our pod listed.</p> <pre><code>NAME                       READY   STATUS    RESTARTS   AGE\n(...)\nchassis-5c574d459c-rclx9   1/1     Running   0          22s\n(...)\n</code></pre>"},{"location":"tutorials/deploy-manual/#query-the-service","title":"Query the service","text":"<p>To conclude, we may want to query the service just to see that it answers as we expect.</p> <p>To do that, we need to port forward the service.</p> <pre><code>kubectl port-forward svc/chassis 5000:5000\n</code></pre> <p>Now that we have access to the service we can query it.</p> <pre><code>curl localhost:5000\n</code></pre> <p>Which should output an alive message.</p>"},{"location":"tutorials/deploy-manual/#begin-using-the-service","title":"Begin Using the Service","text":"<p>Congratulations, you have now successfully deployed the service in a private Kubernetes cluster. To get started, make sure you set up a Python virtual enviornment and install the <code>chassisml</code> SDK.</p> <pre><code>pip install chassisml\n</code></pre> <p>For more resources, check out our tutorials and how-to guides</p>"},{"location":"tutorials/ds-connect/","title":"Build a Model","text":"<p>In this tutorial, we will:</p> <ul> <li> Connect to the Chassis service with the Python SDK</li> <li> Load our model into memory (model can be saved as a file to be loaded or trained and loaded from scratch)</li> <li> Define a <code>process</code> function for inferencing</li> <li> Submit a job request to the Chassis service</li> </ul> <p>After completing these steps, we will have a new container image uploaded to Docker Hub that we will be able to use locally or deploy to a serving platform of our choice.</p> <p>Please note that in addition to the tutorial on this page, there are example Jupyter notebooks available in the Chassis repo here. Instructions to run those notebooks are provided in the README in that repo subdirectory.</p>"},{"location":"tutorials/ds-connect/#install-the-sdk","title":"Install the SDK","text":"<p>First step is to install the SDK and additional packages required for this tutorial using <code>pip</code>.</p> <pre><code>pip install chassisml scikit-learn mlflow joblib requests\n</code></pre>"},{"location":"tutorials/ds-connect/#build-or-import-the-model","title":"Build or import the model","text":"<p>We can start from an existing model saved locally or create a new one, as long as the model can be loaded into memory. In this tutorial, we will create a simple classifier with the Scikit-Learn library. </p>"},{"location":"tutorials/ds-connect/#import-required-libraries","title":"Import required libraries","text":"<p>Since we are going to train our own model as an example, we need to import all the libraries that we will need to do that.</p> <pre><code>import chassisml\nimport sklearn\nimport numpy as np\nimport json\nfrom joblib import dump, load\n</code></pre>"},{"location":"tutorials/ds-connect/#create-the-model","title":"Create the model","text":"<pre><code>from sklearn import datasets, svm\nfrom sklearn.model_selection import train_test_split\ndigits = datasets.load_digits()\ndata = digits.images.reshape((len(digits.images), -1))\n# Create a classifier: a support vector classifier\nclf = svm.SVC(gamma=0.001)\n# Split data into 50% train and 50% test subsets\nX_train, X_test, y_train, y_test = train_test_split(\ndata, digits.target, test_size=0.5, shuffle=False)\n# Learn the digits on the train subset\nclf.fit(X_train, y_train)\n</code></pre>"},{"location":"tutorials/ds-connect/#define-process-method","title":"Define Process Method","text":"<p>Notice our Scikit-Learn model (<code>clf</code>) is now loaded into memory, which is exactly what we need to format it the way Chassis expects.</p> <p>This means we can now prepare the process function, which must take raw bytes as input. This function is responsible for preprocessing the bytes, running inference, and returning formatted results. It can leverage anything (variables, functions, objects) defined in our environment. Notice we reference our model <code>clf</code> as defined above:</p> <pre><code>def process(input_bytes):\ninputs = np.array(json.loads(input_bytes))/2\ninference_results = clf.predict(inputs)\nstructured_results = []\nfor inference_result in inference_results:\nstructured_output = {\n\"data\": {\n\"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]}\n}\n}\nstructured_results.append(structured_output)\nreturn structured_results\n</code></pre> <p>The process function can call other functions if needed. </p> <p>Next, we initialize our Chassis client, which we'll use to communicate with the Chassis service. Here, we assume our instance of Chassis is running locally on port 5000:</p> <pre><code>chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")\n</code></pre> <p>Now let's create a Chassis model with our process function, test it locally with a local input file, and then also test the creation of the environment and the execution of the model with a sample input file within that created environment (this will occur within the Chassis service):</p> <p>NOTE: <code>test_env</code> function not available in publicly-hosted service.</p> <pre><code># create Chassis model\nchassis_model = chassis_client.create_model(process_fn=process)\n# save sample data for testing\nsample = X_test[:1].tolist()\nwith open(\"./digits_sample.json\", 'w') as out:\njson.dump(sample, out)\n# test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here):\nsample_filepath = './digits_sample.json'\nresults = chassis_model.test(sample_filepath)\nprint(results)\n# test environment and model within Chassis service, must pass filepath here:\ntest_env_result = chassis_model.test_env(sample_filepath)\nprint(test_env_result)\n</code></pre>"},{"location":"tutorials/ds-connect/#build-the-image-and-publish-to-modzy","title":"Build the image and publish to Modzy","text":"<p>Now that we have our model in the proper Chassis format, we need to make a request against the Chassis service to build the Docker image that exposes it. You can optionally define your desired conda environment and pass it to <code>publish()</code>, but if you don't, Chassis will automatically infer the dependencies for you based on what is required to run the <code>process</code> function. We just need to provide a model name and semantic version, dockerhub credentials, and we can optionally provide a sample input file and Modzy API key if we'd like to publish the model to Modzy:</p> <pre><code>dockerhub_user = &lt;my.username&gt;\ndockerhob_pass = &lt;my.password&gt;\nresponse = chassis_model.publish(\nmodel_name=\"Sklearn Digits\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre> <p>The <code>block_until_complete</code> call will terminate once the Chassis job completes.</p>"},{"location":"tutorials/ds-connect/#pull-the-image","title":"Pull the image","text":"<p>Now that the process has completely finished we can pull and see our built image. The image name will be the <code>model_name</code> specified in the <code>publish()</code> call, but lowercase and with dashes instead of spaces. The image tag will be the <code>model_version</code>.</p> <pre><code>docker pull &lt;my.username&gt;/sklearn-digits:0.0.1\n</code></pre> <pre><code>docker images &lt;my.username&gt;/sklearn-digits:0.0.1\n</code></pre> <p>If everything has gone as expected we will see something similar to this.</p> <pre><code>REPOSITORY                        TAG       IMAGE ID       CREATED         SIZE\n&lt;my.username&gt;/sklearn-digits            latest    0e5c5815f2ec   3 minutes ago   2.19GB\n</code></pre>"},{"location":"tutorials/ds-connect/#tutorial-in-action","title":"Tutorial in Action","text":"<p>Follow along as we walk through this tutorial step by step!</p>"},{"location":"tutorials/ds-deploy/","title":"Deploy Model to KServe","text":""},{"location":"tutorials/ds-deploy/#install-required-dependencies","title":"Install Required Dependencies","text":"<ul> <li> Install Docker Desktop<ul> <li>Try to run <code>docker ps</code><ul> <li>If you get a permissions error, follow instructions here</li> </ul> </li> </ul> </li> <li> Install KServe: <pre><code>curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.7/hack/quick_install.sh\" | bash\n</code></pre></li> </ul>"},{"location":"tutorials/ds-deploy/#define-required-variables","title":"Define required variables","text":"<p>There are some environment variables that must be defined for KServe to work:</p> <ul> <li>INTERFACE: kserve</li> <li>HTTP_PORT: port where kserve will be running</li> <li>PROTOCOL: it can be v1 or v2</li> <li>MODEL_NAME: a name for the model must be defined</li> </ul>"},{"location":"tutorials/ds-deploy/#deploy-the-model","title":"Deploy the model","text":"<p>For this tutorial, we will use the Chassis-generated container image uploaded as <code>bmunday131/sklearn-digits</code>. To deploy to KServe, we will use the file that defines the <code>InferenceService</code> for the protocol v1 of KServe.</p> <p><pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\nname: chassisml-sklearn-demo\nspec:\npredictor:\ncontainers:\n- image: bmunday131/sklearn-digits:0.0.1\nname: chassisml-sklearn-demo-container\nimagePullPolicy: IfNotPresent\nenv:\n- name: INTERFACE\nvalue: kserve\n- name: HTTP_PORT\nvalue: \"8080\"\n- name: PROTOCOL\nvalue: v1\n- name: MODEL_NAME\nvalue: digits\nports:\n- containerPort: 8080\nprotocol: TCP\n</code></pre> In this case, the variable <code>MODEL_NAME</code> should not be necessary since it is defined when creating the image.</p> <pre><code>kubectl apply -f custom_v1.yaml\n</code></pre> <p>This should output a success message.</p>"},{"location":"tutorials/ds-deploy/#deploy-from-private-docker-registry","title":"Deploy from Private Docker Registry","text":"<p>In the above example, we deploy a public container image, which means we do not need to define credentials to pull the image. If, however, you set up Chassis to push container images to a private registry, you will need to add a few lines to your yaml file.</p> <p>First, create a Kubernetes <code>imagePullSecrets</code> object that contains your credentials as a list of secrets. </p> <pre><code>kubectl create secret docker-registry &lt;registry-credential-secrets&gt; \\\n--docker-server=&lt;private-registry-url&gt; \\\n--docker-email=&lt;private-registry-email&gt; \\\n--docker-username=&lt;private-registry-user&gt; \\\n--docker-password=&lt;private-registry-password&gt;\n</code></pre> <p>Visit Managing Secrets using kubectl for more details.</p> <p>Next, add the following lines to your yaml file:</p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\nname: chassisml-sklearn-demo\nspec:\npredictor:\nimagePullSecrets:\n- name: &lt;registry-credential-secrets&gt;\ncontainers:\n- image: bmunday131/sklearn-digits:0.0.1\nname: chassisml-sklearn-demo-container\nimagePullPolicy: IfNotPresent\nenv:\n- name: INTERFACE\nvalue: kserve\n- name: HTTP_PORT\nvalue: \"8080\"\n- name: PROTOCOL\nvalue: v1\n- name: MODEL_NAME\nvalue: digits\nports:\n- containerPort: 8080\nprotocol: TCP\n</code></pre> <p>Finally, apply your changes:</p> <pre><code>kubectl apply -f custom_v1.yaml\n</code></pre>"},{"location":"tutorials/ds-deploy/#define-required-variables-to-query-the-pod","title":"Define required variables to query the pod","text":"<p>This is needed in order to be able to communicate with the deployed image.</p> <p>The <code>SERVICE_NAME</code> must match the name defined in the <code>metadata.name</code> of the <code>InferenceService</code> created above.</p> <p>The <code>MODEL_NAME</code> must match the name of your model. It can be defined by the data scientist when making the request against <code>Chassis</code> service or overwritten in the <code>InferenceService</code> as defined above.</p> <p>Mac: <pre><code>minikube tunnel\n\n# in another terminal:\nexport INGRESS_HOST=localhost\nexport INGRESS_PORT=80\n</code></pre></p> <p>Linux: <pre><code>export INGRESS_HOST=$(minikube ip)\nexport INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}')\n</code></pre></p> <p>Mac or Linux: <pre><code>export SERVICE_NAME=chassisml-sklearn-demo\nexport MODEL_NAME=digits\nexport SERVICE_HOSTNAME=$(kubectl get inferenceservice ${SERVICE_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n</code></pre></p>"},{"location":"tutorials/ds-deploy/#query-the-model","title":"Query the model","text":"<p>Please note that you must base64 encode each input instance. For example:</p> <pre><code>import json\nimport base64 as b64\ninstances = [[1,2,3,4],[5,6,7,8]]\ninput_dict = {'instances': [b64.b64encode(str(entry).encode()).decode() for entry in instances]}\njson.dump(input_dict,open('kserve_input.json','w'))\n</code></pre> <p>Now you can just make a request to predict some data. Take into account that you must download <code>inputsv1.json</code> before making the request. </p> <pre><code>curl -H \"Host: ${SERVICE_HOSTNAME}\" \"http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:predict\" -d@inputsv1.json | jq\n</code></pre> <p>The output should be similar to this:</p> <pre><code>{\n\"predictions\": [\n{\n\"data\": {\n\"drift\": null,\n\"explanation\": null,\n\"result\": {\n\"classPredictions\": [\n{\n\"class\": \"4\",\n\"score\": \"1\"\n}\n]\n}\n}\n},\n{\n\"data\": {\n\"drift\": null,\n\"explanation\": null,\n\"result\": {\n\"classPredictions\": [\n{\n\"class\": \"8\",\n\"score\": \"1\"\n}\n]\n}\n}\n},\n{\n\"data\": {\n\"drift\": null,\n\"explanation\": null,\n\"result\": {\n\"classPredictions\": [\n{\n\"class\": \"8\",\n\"score\": \"1\"\n}\n]\n}\n}\n},\n{\n\"data\": {\n\"drift\": null,\n\"explanation\": null,\n\"result\": {\n\"classPredictions\": [\n{\n\"class\": \"4\",\n\"score\": \"1\"\n}\n]\n}\n}\n},\n{\n\"data\": {\n\"drift\": null,\n\"explanation\": null,\n\"result\": {\n\"classPredictions\": [\n{\n\"class\": \"8\",\n\"score\": \"1\"\n}\n]\n}\n}\n}\n]\n}\n</code></pre> <p>In this case, the data was prepared for the protocol v1, but we can deploy the image using the protocol v2 and make the request using the data for v2.</p>"},{"location":"tutorials/ds-deploy/#deploy-the-model-locally","title":"Deploy the model locally","text":"<p>The model can also be deployed locally:</p> <pre><code>docker run --rm -p 8080:8080 \\\n-e INTERFACE=kserve \\\n-e HTTP_PORT=8080 \\\n-e PROTOCOL=v2 \\\n-e MODEL_NAME=digits \\\ncarmilso/chassisml-sklearn-demo:latest\n</code></pre> <p>So we can query it this way. Take into account that you must download <code>inputsv2.json</code> before making the request:</p> <pre><code>curl localhost:8080/v2/models/digits/infer -d@inputsv2.json\n</code></pre>"},{"location":"tutorials/ds-deploy/#tutorial-in-action","title":"Tutorial in Action","text":"<p>Follow along as we walk through this tutorial step by step!</p>"},{"location":"tutorials/ds-postman/","title":"Run Model in Postman","text":"<p>After building a container with Chassisml, there are several ways to interact with it: via Modzy, KServe, or locally on your machine. This tutorial demonstrates how to make API calls to a container locally using Postman.</p> <p>Note</p> <p>To follow this tutorial, you do not need your own container as we provide an example Chassisml container for convenience. However, if you do you have your own model container, you can switch out the example container with your own.</p>"},{"location":"tutorials/ds-postman/#getting-started","title":"Getting started","text":""},{"location":"tutorials/ds-postman/#install-required-dependencies","title":"Install Required Dependencies","text":"<ul> <li> Install Docker<ul> <li>Open a terminal on your machine, and try to run <code>docker ps</code><ul> <li>If you get a permissions error, follow instructions here</li> </ul> </li> </ul> </li> <li> Install Postman</li> <li> Download proto file</li> </ul>"},{"location":"tutorials/ds-postman/#download-sample-container","title":"Download Sample Container","text":"<pre><code>docker pull modzy/chassisml-image-classification\n</code></pre>"},{"location":"tutorials/ds-postman/#spin-up-container","title":"Spin up Container","text":"<p>Before interacting with the container using Postman, we need to first spin up the container and port forward it to a port we will use in the local URL configuration in Postman.</p> <pre><code>docker run -p 5000:45000 -it modzy/chassisml-image-classification:latest\n</code></pre> <p>In this docker command, we use the following parameters:</p> <ul> <li>-p: Forwards the port serving the gRPC server inside the container (45000) to a local port (5000)</li> <li>-it: Runs container interactively, so you can see the logs from the container as you make API calls to it</li> </ul> <p>To learn more, visit the Docker run reference documentation.</p> <p>After running the container, you should see this logs message printed to your terminal:</p> <pre><code>INFO:interfaces.modzy.grpc_model.src.model_server:gRPC Server running on port 45000\n</code></pre>"},{"location":"tutorials/ds-postman/#send-grpc-requests-to-running-container","title":"Send gRPC Requests to Running Container","text":""},{"location":"tutorials/ds-postman/#request-set-up","title":"Request Set Up","text":"<p>Once the container is running locally, open the Postman app. On the upper left part of the screen, select New and open gRPC Request.</p> <p></p> <p>Enter your local server URL into the first box.</p> <p></p> <p>In the second box, select the Import protobuf definition from local file and upload the proto file you downloaded during setup.</p> <p></p> <p>Give your protobuf a name and version.</p> <p></p> <p>The three remote procedure calls defined in the protobuf file (to which our Chassisml container adheres to) will appear in the Select a method dropdown.</p> <p></p> <p>We are now ready to invoke requests to the three remote procedure calls this container can respond to as defined in the model protobuf file.</p>"},{"location":"tutorials/ds-postman/#invoke-methods","title":"Invoke Methods","text":""},{"location":"tutorials/ds-postman/#status","title":"Status()","text":"<p>First, we will invoke the <code>Status()</code> method, which will execute any model instantiation required for model inference. </p> <p>Simply select the <code>ModzyModel / Status</code> option and click Invoke.</p> <p></p> <p>In the response section on the bottom of your screen, you should see the following information returned:</p> <pre><code>{\n\"status_code\": 200,\n\"status\": \"OK\",\n\"message\": \"Model Initialized Successfully.\",\n\"model_info\": {\n\"model_name\": \"MXNET MobileNet Image Classifiction\",\n\"model_version\": \"0.0.1\",\n\"model_author\": \"Chassis\",\n\"model_type\": \"grpc\"\n},\n\"description\": {\n\"summary\": \"Chassis model.\",\n\"details\": \"Chassis model.\",\n\"technical\": \"Chassis model.\"\n},\n\"inputs\": [\n{\n\"filename\": \"input\",\n\"accepted_media_types\": [\n\"application/json\"\n],\n\"max_size\": \"5M\",\n\"description\": \"Input file.\"\n}\n],\n\"outputs\": [\n{\n\"filename\": \"results.json\",\n\"media_type\": \"application/json\",\n\"max_size\": \"1M\",\n\"description\": \"Output file.\"\n}\n],\n\"resources\": {},\n\"timeout\": {\n\"status\": \"60s\",\n\"run\": \"60s\"\n},\n\"features\": {\n\"batch_size\": 1\n}\n}\n</code></pre> <p>The 200 <code>status_code</code> tells us the model was successfully spun up and instantiated. Notice the response also includes some metadata about our model.</p>"},{"location":"tutorials/ds-postman/#run","title":"Run()","text":"<p>Next, we will invoke the <code>Run()</code> method, which expects raw bytes as input to perform inference on, and in turn will respond with the model predictions.</p> <p>To do so, first change the method from <code>ModzyModel / Status</code> to <code>ModzyModel / Run</code>.</p> <p></p> <p>As defined by the model protobuf file, this method expects a request that contains three inputs:</p> <pre><code>message RunRequest {\nrepeated InputItem inputs     = 1;\nbool detect_drift             = 2;\nbool explain                  = 3;\n}\n</code></pre> <p>Where the <code>InputItem</code> is defined as:</p> <pre><code>message InputItem {\nmap&lt;string, bytes&gt; input      = 1;\n}\n</code></pre> <p>Conveniently, Postman makes it very easy to create a properly-formatted input request message. At the bottom of the Request section, click Generate Example Message button.</p> <p></p> <p>Replace the example content in the \"Message\" box (right above Generate Example Message button) with this content:</p> <pre><code>{\n\"detect_drift\": false,\n\"explain\": false,\n\"inputs\": [\n{\n\"input\": {\"input\": \"base64-encoded-image\"}\n}\n]\n}\n</code></pre> <p>NOTE: Before invoking the <code>Run()</code> method, you must first replace the contents of the \"input\" string with an actual raw byte representation of an image. For the sake of this tutorial, we did not paste the full base64 encoding of an image, but to replicate the results listed below, download this image and upload it to this base64 encoder to get the full encoding. Replace \"base-64-encoded-image\" with the base64 encoded representation of the image (wrapped in quotes). </p> <p>After pasting in the base64 equivalent of the image into the value string inside the request message, click Invoke to run a sample inference.</p> <p>You should see the following JSON printed in the Response section.</p> <pre><code>{\n\"status_code\": 200,\n\"status\": \"OK\",\n\"message\": \"Inference executed\",\n\"outputs\": [\n{\n\"output\": {\n\"results.json\": \"eyJkYXRhIjp7InJlc3VsdCI6eyJjbGFzc1ByZWRpY3Rpb25zIjpbeyJjbGFzcyI6ImJveGVyIiwic2NvcmUiOjAuODE0MzAyNDQ0NDU4MDA3OH0seyJjbGFzcyI6ImJ1bGwgbWFzdGlmZiIsInNjb3JlIjowLjE2MjA1NTQ2MjU5ODgwMDY2fSx7ImNsYXNzIjoiU3RhZmZvcmRzaGlyZSBidWxsdGVycmllciwgU3RhZmZvcmRzaGlyZSBidWxsIHRlcnJpZXIiLCJzY29yZSI6MC4wMDYxOTQ1Nzg5NDkzNjIwMzk2fV19LCJleHBsYW5hdGlvbiI6bnVsbCwiZHJpZnQiOm51bGx9fQ==\"\n},\n\"success\": true\n}\n]\n}\n</code></pre> <p>You will noticed the contents of the <code>output</code> object are also base64 encoded, as defined in the protobuf file. Decode the raw base64 string here. The decoded result should look as follows: </p> <pre><code>{\n\"data\": {\n\"result\": {\n\"classPredictions\": [\n{\n\"class\": \"boxer\",\n\"score\": 0.8143024444580078\n},\n{\n\"class\": \"bull mastiff\",\n\"score\": 0.16205546259880066\n},\n{\n\"class\": \"Staffordshire bullterrier, Staffordshire bull terrier\",\n\"score\": 0.0061945789493620396\n}\n]\n},\n\"explanation\": null,\n\"drift\": null\n}\n}\n</code></pre> <p>In practice, encoding and decoding data is easy to do in your program of choice, so these manual steps normally would not be required in a production API application. However, to conveniently construct and test gRPC APIs in postman, these steps are required. </p>"},{"location":"tutorials/ds-postman/#shutdown","title":"Shutdown()","text":"<p>Lastly, invoke the <code>Shutdown()</code> method the same way you invoked the <code>Status()</code> method. This will shutdown the container running locally on your machine.</p>"},{"location":"tutorials/ds-postman/#tutorial-in-action","title":"Tutorial in Action","text":"<p>Follow along as we walk through this tutorial step by step!</p>"}]}