{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Model Containerizer for K8s Build models directly into DevOps-ready container images for inference (using MLflow under the hood) Supports parallel builds in Kubernetes jobs, using Kaniko , no Docker socket required! Generates Open Model Interface compatible images that are multi-purpose and portable, they work on multiple platforms: KFServing and Modzy Try the test drive today, then deploy our Helm chart to your K8s cluster to use it for real Test Drive The fastest way to get started is to use the test drive functionality provided by Testfaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive Talk & Demo .video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; } Getting Started Follow one of our tutorials to easily get started and see how Chassis works: Tech Support Our tech support is graciously hosted on Modzy's Discord server in the \"Open Source\" channels Install with Helm into a Kubernetes cluster Build a container image from your model Deploy to KFServing the built image Shameless plug Chassis is developed by Modzy , a commercial ModelOps platform designed to run any kind of machine learning and artifical intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of Chassis grew out of Modzy's internal research and development to provide a way to easily containerize MLflow models to publish into the Modzy catalog and to support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense. Contributors A full list of contributors can be found here .","title":"Model Containerizer for K8s"},{"location":"#model-containerizer-for-k8s","text":"Build models directly into DevOps-ready container images for inference (using MLflow under the hood) Supports parallel builds in Kubernetes jobs, using Kaniko , no Docker socket required! Generates Open Model Interface compatible images that are multi-purpose and portable, they work on multiple platforms: KFServing and Modzy Try the test drive today, then deploy our Helm chart to your K8s cluster to use it for real","title":"Model Containerizer for K8s"},{"location":"#test-drive","text":"The fastest way to get started is to use the test drive functionality provided by Testfaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive","title":"Test Drive"},{"location":"#talk-demo","text":".video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Talk &amp; Demo"},{"location":"#getting-started","text":"Follow one of our tutorials to easily get started and see how Chassis works: Tech Support Our tech support is graciously hosted on Modzy's Discord server in the \"Open Source\" channels Install with Helm into a Kubernetes cluster Build a container image from your model Deploy to KFServing the built image","title":"Getting Started"},{"location":"#shameless-plug","text":"Chassis is developed by Modzy , a commercial ModelOps platform designed to run any kind of machine learning and artifical intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of Chassis grew out of Modzy's internal research and development to provide a way to easily containerize MLflow models to publish into the Modzy catalog and to support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense.","title":"Shameless plug"},{"location":"#contributors","text":"A full list of contributors can be found here .","title":"Contributors"},{"location":"chassisml-sdk-reference/","text":"Chassisml Python SDK Introduction The Chassisml Python SDK offers convenience functions that interact with the Chassisml service to automate the containerization and deployment of models to your preferred model serving platform. It is organized into two classes: ChassisClient : interacts with the Chassis routes that build a user's container from their model artifacts ChasissModel : creates a chassis compliant model out of a few lines of Python code from user supplied model artifacts First, install the Chassisml SDK to get started: pip install chassisml To import the library into your editor: import chassisml Usage ChassisClient The Chassis Client object. This class is used to interact with the Kaniko service. Attributes: Name Type Description base_url str The base url for the API. Source code in chassisml-sdk/chassisml/chassisml.py class ChassisClient : \"\"\"The Chassis Client object. This class is used to interact with the Kaniko service. Attributes: base_url (str): The base url for the API. \"\"\" def __init__ ( self , base_url = 'http://localhost:5000' ): self . base_url = base_url def get_job_status ( self , job_id ): ''' Checks the status of a chassis job Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method Returns: Dict: JSON Chassis job status Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') job_status = chassis_client.get_job_status(job_id) ``` ''' route = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } ' res = requests . get ( route ) data = res . json () return data def block_until_complete ( self , job_id , timeout = 1800 , poll_interval = 5 ): ''' Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method timeout (int): Timeout threshold in seconds poll_intervall (int): Amount of time to wait in between API polls to check status of job Returns: Dict: final job status returned by `ChassisClient.block_until_complete` method Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') final_status = chassis_client.block_until_complete(job_id) ``` ''' endby = time . time () + timeout if ( timeout is not None ) else None while True : status = self . get_job_status ( job_id ) if status [ 'status' ][ 'succeeded' ] or status [ 'status' ][ 'failed' ]: return status if ( endby is not None ) and ( time . time () > endby - poll_interval ): print ( 'Timed out before completion.' ) return False time . sleep ( poll_interval ) def download_tar ( self , job_id , output_filename ): ''' Downloads container image as tar archive Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method output_filename (str): Local output filepath to save container image Returns: None: This method does not return an object Examples: ```python # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id) chassis_client.download_tar(job_id, \"./chassis-model.tar\") ``` ''' url = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } /download-tar' r = requests . get ( url ) if r . status_code == 200 : with open ( output_filename , 'wb' ) as f : f . write ( r . content ) else : print ( f 'Error download tar: { r . text } ' ) def create_model ( self , context , process_fn = None , batch_process_fn = None , batch_size = None ): ''' Builds chassis model locally Args: context (dict): Dictionary that will contain the trained model object and any other inference-specific dependencies that will persist while model container is running. Should include all objects that only need to be loaded one time (e.g., loaded model, labels file(s), data transformation objects, etc.) that can be accessed during inference. process_fn (function): Python function that must accept a single piece of input data in raw bytes form and `context` dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_process_fn (function): Python function that must accept a batch of input data in raw bytes form and `context` dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_size (int): Maximum batch size if `batch_process_fn` is defined Returns: ChassisModel: Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this [example](https://docs.modzy.com/docs/chassis-ml). ```python # Import and normalize data X_digits, y_digits = datasets.load_digits(return_X_y=True) X_digits = X_digits / X_digits.max() n_samples = len(X_digits) # Split data into training and test sets X_train = X_digits[: int(0.9 * n_samples)] y_train = y_digits[: int(0.9 * n_samples)] X_test = X_digits[int(0.9 * n_samples) :] y_test = y_digits[int(0.9 * n_samples) :] # Train Model logistic = LogisticRegression(max_iter=1000) print( \"LogisticRegression mean accuracy score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test) ) # Save small sample input to use for testing later sample = X_test[:5].tolist() with open(\"digits_sample.json\", 'w') as out: json.dump(sample, out) # 1. Define Context Dictionary context = {\"model\": logistic} # 2. Define Process function def process(input_bytes,context): inputs = np.array(json.loads(input_bytes)) inference_results = context[\"model\"].predict(inputs) structured_results = [] for inference_result in inference_results: structured_output = { \"data\": { \"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]} } } structured_results.append(structured_output) return structured_results # create Chassis model chassis_model = chassis_client.create_model(context=context,process_fn=process) ``` ''' if not ( process_fn or batch_process_fn ): raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) if ( batch_process_fn and not batch_size ) or ( batch_size and not batch_process_fn ): raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) return ChassisModel ( context , process_fn , batch_process_fn , batch_size , self . base_url ) block_until_complete ( self , job_id , timeout = 1800 , poll_interval = 5 ) Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required timeout int Timeout threshold in seconds 1800 poll_intervall int Amount of time to wait in between API polls to check status of job required Returns: Type Description Dict final job status returned by ChassisClient.block_until_complete method Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) Source code in chassisml-sdk/chassisml/chassisml.py def block_until_complete ( self , job_id , timeout = 1800 , poll_interval = 5 ): ''' Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method timeout (int): Timeout threshold in seconds poll_intervall (int): Amount of time to wait in between API polls to check status of job Returns: Dict: final job status returned by `ChassisClient.block_until_complete` method Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') final_status = chassis_client.block_until_complete(job_id) ``` ''' endby = time . time () + timeout if ( timeout is not None ) else None while True : status = self . get_job_status ( job_id ) if status [ 'status' ][ 'succeeded' ] or status [ 'status' ][ 'failed' ]: return status if ( endby is not None ) and ( time . time () > endby - poll_interval ): print ( 'Timed out before completion.' ) return False time . sleep ( poll_interval ) create_model ( self , context , process_fn = None , batch_process_fn = None , batch_size = None ) Builds chassis model locally Parameters: Name Type Description Default context dict Dictionary that will contain the trained model object and any other inference-specific dependencies that will persist while model container is running. Should include all objects that only need to be loaded one time (e.g., loaded model, labels file(s), data transformation objects, etc.) that can be accessed during inference. required process_fn function Python function that must accept a single piece of input data in raw bytes form and context dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the process method None batch_process_fn function Python function that must accept a batch of input data in raw bytes form and context dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the process method None batch_size int Maximum batch size if batch_process_fn is defined None Returns: Type Description ChassisModel Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this example . # Import and normalize data X_digits , y_digits = datasets . load_digits ( return_X_y = True ) X_digits = X_digits / X_digits . max () n_samples = len ( X_digits ) # Split data into training and test sets X_train = X_digits [: int ( 0.9 * n_samples )] y_train = y_digits [: int ( 0.9 * n_samples )] X_test = X_digits [ int ( 0.9 * n_samples ) :] y_test = y_digits [ int ( 0.9 * n_samples ) :] # Train Model logistic = LogisticRegression ( max_iter = 1000 ) print ( \"LogisticRegression mean accuracy score: %f \" % logistic . fit ( X_train , y_train ) . score ( X_test , y_test ) ) # Save small sample input to use for testing later sample = X_test [: 5 ] . tolist () with open ( \"digits_sample.json\" , 'w' ) as out : json . dump ( sample , out ) # 1. Define Context Dictionary context = { \"model\" : logistic } # 2. Define Process function def process ( input_bytes , context ): inputs = np . array ( json . loads ( input_bytes )) inference_results = context [ \"model\" ] . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results # create Chassis model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) Source code in chassisml-sdk/chassisml/chassisml.py def create_model ( self , context , process_fn = None , batch_process_fn = None , batch_size = None ): ''' Builds chassis model locally Args: context (dict): Dictionary that will contain the trained model object and any other inference-specific dependencies that will persist while model container is running. Should include all objects that only need to be loaded one time (e.g., loaded model, labels file(s), data transformation objects, etc.) that can be accessed during inference. process_fn (function): Python function that must accept a single piece of input data in raw bytes form and `context` dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_process_fn (function): Python function that must accept a batch of input data in raw bytes form and `context` dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_size (int): Maximum batch size if `batch_process_fn` is defined Returns: ChassisModel: Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this [example](https://docs.modzy.com/docs/chassis-ml). ```python # Import and normalize data X_digits, y_digits = datasets.load_digits(return_X_y=True) X_digits = X_digits / X_digits.max() n_samples = len(X_digits) # Split data into training and test sets X_train = X_digits[: int(0.9 * n_samples)] y_train = y_digits[: int(0.9 * n_samples)] X_test = X_digits[int(0.9 * n_samples) :] y_test = y_digits[int(0.9 * n_samples) :] # Train Model logistic = LogisticRegression(max_iter=1000) print( \"LogisticRegression mean accuracy score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test) ) # Save small sample input to use for testing later sample = X_test[:5].tolist() with open(\"digits_sample.json\", 'w') as out: json.dump(sample, out) # 1. Define Context Dictionary context = {\"model\": logistic} # 2. Define Process function def process(input_bytes,context): inputs = np.array(json.loads(input_bytes)) inference_results = context[\"model\"].predict(inputs) structured_results = [] for inference_result in inference_results: structured_output = { \"data\": { \"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]} } } structured_results.append(structured_output) return structured_results # create Chassis model chassis_model = chassis_client.create_model(context=context,process_fn=process) ``` ''' if not ( process_fn or batch_process_fn ): raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) if ( batch_process_fn and not batch_size ) or ( batch_size and not batch_process_fn ): raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) return ChassisModel ( context , process_fn , batch_process_fn , batch_size , self . base_url ) download_tar ( self , job_id , output_filename ) Downloads container image as tar archive Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required output_filename str Local output filepath to save container image required Returns: Type Description None This method does not return an object Examples: # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id) chassis_client . download_tar ( job_id , \"./chassis-model.tar\" ) Source code in chassisml-sdk/chassisml/chassisml.py def download_tar ( self , job_id , output_filename ): ''' Downloads container image as tar archive Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method output_filename (str): Local output filepath to save container image Returns: None: This method does not return an object Examples: ```python # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id) chassis_client.download_tar(job_id, \"./chassis-model.tar\") ``` ''' url = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } /download-tar' r = requests . get ( url ) if r . status_code == 200 : with open ( output_filename , 'wb' ) as f : f . write ( r . content ) else : print ( f 'Error download tar: { r . text } ' ) get_job_status ( self , job_id ) Checks the status of a chassis job Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required Returns: Type Description Dict JSON Chassis job status Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) job_status = chassis_client . get_job_status ( job_id ) Source code in chassisml-sdk/chassisml/chassisml.py def get_job_status ( self , job_id ): ''' Checks the status of a chassis job Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method Returns: Dict: JSON Chassis job status Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') job_status = chassis_client.get_job_status(job_id) ``` ''' route = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } ' res = requests . get ( route ) data = res . json () return data ChassisModel ( PythonModel ) The Chassis Model object. This class inherits from mlflow.pyfunc.PythonModel and adds Chassis functionality. Attributes: Name Type Description predict function MLflow pyfunc compatible predict function. Will wrap user-provided function which takes two arguments: model_input (bytes) and model_context (dict). chassis_build_url str The build url for the Chassis API. Source code in chassisml-sdk/chassisml/chassisml.py class ChassisModel ( mlflow . pyfunc . PythonModel ): \"\"\"The Chassis Model object. This class inherits from `mlflow.pyfunc.PythonModel` and adds Chassis functionality. Attributes: predict (function): MLflow pyfunc compatible predict function. Will wrap user-provided function which takes two arguments: model_input (bytes) and model_context (dict). chassis_build_url (str): The build url for the Chassis API. \"\"\" def __init__ ( self , model_context , process_fn , batch_process_fn , batch_size , chassis_base_url ): if process_fn and batch_process_fn : if not batch_size : raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) self . predict = self . _gen_predict_method ( process_fn , model_context ) self . batch_predict = self . _gen_predict_method ( batch_process_fn , model_context , batch = True ) self . batch_input = True self . batch_size = batch_size elif process_fn and not batch_process_fn : self . predict = self . _gen_predict_method ( process_fn , model_context ) self . batch_input = False self . batch_size = None elif batch_process_fn and not process_fn : if not batch_size : raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) self . predict = self . _gen_predict_method ( batch_process_fn , model_context , batch_to_single = True ) self . batch_predict = self . _gen_predict_method ( batch_process_fn , model_context , batch = True ) self . batch_input = True self . batch_size = batch_size else : raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) self . chassis_build_url = urllib . parse . urljoin ( chassis_base_url , routes [ 'build' ]) self . chassis_test_url = urllib . parse . urljoin ( chassis_base_url , routes [ 'test' ]) def _gen_predict_method ( self , process_fn , model_context , batch = False , batch_to_single = False ): def predict ( _ , model_input ): if batch_to_single : output = process_fn ([ model_input ], model_context )[ 0 ] else : output = process_fn ( model_input , model_context ) if batch : return [ json . dumps ( out , separators = ( \",\" , \":\" ), cls = NumpyEncoder ) . encode () for out in output ] else : return json . dumps ( output , separators = ( \",\" , \":\" ), cls = NumpyEncoder ) . encode () return predict def test ( self , test_input ): ''' Runs a sample inference test on a single input on chassis model locally Args: test_input (Union[str, bytes, BufferedReader]): Single sample input data to test model Returns: bytes: raw model predictions returned by `process_fn` method Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test(sample_filepath) ``` ''' if isinstance ( test_input , _io . BufferedReader ): result = self . predict ( None , test_input . read ()) elif isinstance ( test_input , bytes ): result = self . predict ( None , test_input ) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): result = self . predict ( None , open ( test_input , 'rb' ) . read ()) else : result = self . predict ( None , bytes ( test_input , encoding = 'utf8' )) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return result def test_batch ( self , test_input ): ''' Takes a single input file, creates a batch of size `batch_size` defined in `ChassisModel.create_model`, and runs a batch job against chassis model locally if `batch_process_fn` is defined. Args: test_input (Union[str, bytes, BufferedReader]): Batch of sample input data to test model Returns: bytes: raw model predictions returned by `batch_process_fn` method Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_input = sample_filepath = './sample_data.json' results = chassis_model.test_batch(sample_filepath) ``` ''' if not self . batch_input : raise NotImplementedError ( \"Batch inference not implemented.\" ) if hasattr ( self , 'batch_predict' ): batch_method = self . batch_predict else : batch_method = self . predict if isinstance ( test_input , _io . BufferedReader ): results = batch_method ( None ,[ test_input . read () for _ in range ( self . batch_size )]) elif isinstance ( test_input , bytes ): results = batch_method ( None ,[ test_input for _ in range ( self . batch_size )]) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): results = batch_method ( None ,[ open ( test_input , 'rb' ) . read () for _ in range ( self . batch_size )]) else : results = batch_method ( None ,[ bytes ( test_input , encoding = 'utf8' ) for _ in range ( self . batch_size )]) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return results def test_env ( self , test_input_path , conda_env = None , fix_env = True ): ''' Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. Args: test_input_path (str): Filepath to sample input data conda_env (str): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build Returns: Dict: raw model predictions returned by `process_fn` or `batch_process_fn` run from within chassis service Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test_env(sample_filepath) ``` ''' model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as model_f , \\ open ( test_input_path , 'rb' ) as test_input_f : files = [ ( 'sample_input' , test_input_f ), ( 'model' , model_f ) ] print ( 'Starting test job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_test_url , files = files ) res . raise_for_status () print ( 'Ok!' ) shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () def save ( self , path , conda_env = None , overwrite = False ): ''' Saves a copy of ChassisModel to local filepath Args: path (str): Filepath to save chassis model as local MLflow model conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment overwrite (bool): If True, overwrites existing contents of `path` parameter Returns: None: This method does not return an object Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) chassis_model.save(\"local_model_directory\") ``` ''' if overwrite and os . path . exists ( path ): shutil . rmtree ( path ) mlflow . pyfunc . save_model ( path = path , python_model = self , conda_env = conda_env ) print ( \"Chassis model saved.\" ) def publish ( self , model_name , model_version , registry_user , registry_pass , conda_env = None , fix_env = True , gpu = False , modzy_sample_input_path = None , modzy_api_key = None ): ''' Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Args: model_name (str): Model name that serves as model's name in Modzy and docker registry repository name. **Note**: this string cannot include punctuation model_version (str): Version of model registry_user (str): Docker registry username registry_pass (str): Docker registry password conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build gpu (bool): If True, builds container image that runs on GPU hardware modzy_sample_input_path (str): Filepath to sample input data. Required to deploy model to Modzy modzy_api_key (str): Valid Modzy API Key Returns: Dict: Response to Chassis `/build` endpoint Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) ``` ''' if ( modzy_sample_input_path or modzy_api_key ) and not \\ ( modzy_sample_input_path and modzy_api_key ): raise ValueError ( '\"modzy_sample_input_path\", and \"modzy_api_key\" must both be provided to publish to Modzy.' ) try : model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) image_name = \"-\" . join ( model_name . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () . split ()) image_data = { 'name' : \" {} / {} \" . format ( registry_user , \" {} : {} \" . format ( image_name , model_version )), 'model_name' : model_name , 'model_path' : tmppath , 'registry_auth' : base64 . b64encode ( \" {} : {} \" . format ( registry_user , registry_pass ) . encode ( \"utf-8\" )) . decode ( \"utf-8\" ), 'publish' : True , 'gpu' : gpu } if modzy_sample_input_path and modzy_api_key : modzy_metadata_path = os . path . join ( tmppath , MODZY_YAML_NAME ) modzy_data = { 'metadata_path' : modzy_metadata_path , 'sample_input_path' : modzy_sample_input_path , 'deploy' : True , 'api_key' : modzy_api_key } write_modzy_yaml ( model_name , model_version , modzy_metadata_path , batch_size = self . batch_size , gpu = gpu ) else : modzy_data = {} with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as f : files = [ ( 'image_data' , json . dumps ( image_data )), ( 'modzy_data' , json . dumps ( modzy_data )), ( 'model' , f ) ] file_pointers = [] for key , file_key in [( 'metadata_path' , 'modzy_metadata_data' ), ( 'sample_input_path' , 'modzy_sample_input_data' )]: value = modzy_data . get ( key ) if value : fp = open ( value , 'rb' ) file_pointers . append ( fp ) files . append (( file_key , fp )) print ( 'Starting build job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_build_url , files = files ) res . raise_for_status () print ( 'Ok!' ) for fp in file_pointers : fp . close () shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () except Exception as e : print ( e ) if os . path . exists ( tmppath ): shutil . rmtree ( tmppath ) if os . path . exists ( model_directory ): shutil . rmtree ( model_directory ) return False publish ( self , model_name , model_version , registry_user , registry_pass , conda_env = None , fix_env = True , gpu = False , modzy_sample_input_path = None , modzy_api_key = None ) Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Parameters: Name Type Description Default model_name str Model name that serves as model's name in Modzy and docker registry repository name. Note : this string cannot include punctuation required model_version str Version of model required registry_user str Docker registry username required registry_pass str Docker registry password required conda_env Union[str, dict] Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None fix_env bool Modifies conda or pip-installable packages into list of dependencies to be installed during the container build True gpu bool If True, builds container image that runs on GPU hardware False modzy_sample_input_path str Filepath to sample input data. Required to deploy model to Modzy None modzy_api_key str Valid Modzy API Key None Returns: Type Description Dict Response to Chassis /build endpoint Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) Source code in chassisml-sdk/chassisml/chassisml.py def publish ( self , model_name , model_version , registry_user , registry_pass , conda_env = None , fix_env = True , gpu = False , modzy_sample_input_path = None , modzy_api_key = None ): ''' Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Args: model_name (str): Model name that serves as model's name in Modzy and docker registry repository name. **Note**: this string cannot include punctuation model_version (str): Version of model registry_user (str): Docker registry username registry_pass (str): Docker registry password conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build gpu (bool): If True, builds container image that runs on GPU hardware modzy_sample_input_path (str): Filepath to sample input data. Required to deploy model to Modzy modzy_api_key (str): Valid Modzy API Key Returns: Dict: Response to Chassis `/build` endpoint Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) ``` ''' if ( modzy_sample_input_path or modzy_api_key ) and not \\ ( modzy_sample_input_path and modzy_api_key ): raise ValueError ( '\"modzy_sample_input_path\", and \"modzy_api_key\" must both be provided to publish to Modzy.' ) try : model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) image_name = \"-\" . join ( model_name . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () . split ()) image_data = { 'name' : \" {} / {} \" . format ( registry_user , \" {} : {} \" . format ( image_name , model_version )), 'model_name' : model_name , 'model_path' : tmppath , 'registry_auth' : base64 . b64encode ( \" {} : {} \" . format ( registry_user , registry_pass ) . encode ( \"utf-8\" )) . decode ( \"utf-8\" ), 'publish' : True , 'gpu' : gpu } if modzy_sample_input_path and modzy_api_key : modzy_metadata_path = os . path . join ( tmppath , MODZY_YAML_NAME ) modzy_data = { 'metadata_path' : modzy_metadata_path , 'sample_input_path' : modzy_sample_input_path , 'deploy' : True , 'api_key' : modzy_api_key } write_modzy_yaml ( model_name , model_version , modzy_metadata_path , batch_size = self . batch_size , gpu = gpu ) else : modzy_data = {} with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as f : files = [ ( 'image_data' , json . dumps ( image_data )), ( 'modzy_data' , json . dumps ( modzy_data )), ( 'model' , f ) ] file_pointers = [] for key , file_key in [( 'metadata_path' , 'modzy_metadata_data' ), ( 'sample_input_path' , 'modzy_sample_input_data' )]: value = modzy_data . get ( key ) if value : fp = open ( value , 'rb' ) file_pointers . append ( fp ) files . append (( file_key , fp )) print ( 'Starting build job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_build_url , files = files ) res . raise_for_status () print ( 'Ok!' ) for fp in file_pointers : fp . close () shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () except Exception as e : print ( e ) if os . path . exists ( tmppath ): shutil . rmtree ( tmppath ) if os . path . exists ( model_directory ): shutil . rmtree ( model_directory ) return False save ( self , path , conda_env = None , overwrite = False ) Saves a copy of ChassisModel to local filepath Parameters: Name Type Description Default path str Filepath to save chassis model as local MLflow model required conda_env Union[str, dict] Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None overwrite bool If True, overwrites existing contents of path parameter False Returns: Type Description None This method does not return an object Examples: chassis_model = chassis_client . create_model ( context = context , process_fn = process ) chassis_model . save ( \"local_model_directory\" ) Source code in chassisml-sdk/chassisml/chassisml.py def save ( self , path , conda_env = None , overwrite = False ): ''' Saves a copy of ChassisModel to local filepath Args: path (str): Filepath to save chassis model as local MLflow model conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment overwrite (bool): If True, overwrites existing contents of `path` parameter Returns: None: This method does not return an object Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) chassis_model.save(\"local_model_directory\") ``` ''' if overwrite and os . path . exists ( path ): shutil . rmtree ( path ) mlflow . pyfunc . save_model ( path = path , python_model = self , conda_env = conda_env ) print ( \"Chassis model saved.\" ) test ( self , test_input ) Runs a sample inference test on a single input on chassis model locally Parameters: Name Type Description Default test_input Union[str, bytes, BufferedReader] Single sample input data to test model required Returns: Type Description bytes raw model predictions returned by process_fn method Examples: chassis_model = chassis_client . create_model ( context = context , process_fn = process ) sample_filepath = './sample_data.json' results = chassis_model . test ( sample_filepath ) Source code in chassisml-sdk/chassisml/chassisml.py def test ( self , test_input ): ''' Runs a sample inference test on a single input on chassis model locally Args: test_input (Union[str, bytes, BufferedReader]): Single sample input data to test model Returns: bytes: raw model predictions returned by `process_fn` method Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test(sample_filepath) ``` ''' if isinstance ( test_input , _io . BufferedReader ): result = self . predict ( None , test_input . read ()) elif isinstance ( test_input , bytes ): result = self . predict ( None , test_input ) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): result = self . predict ( None , open ( test_input , 'rb' ) . read ()) else : result = self . predict ( None , bytes ( test_input , encoding = 'utf8' )) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return result test_batch ( self , test_input ) Takes a single input file, creates a batch of size batch_size defined in ChassisModel.create_model , and runs a batch job against chassis model locally if batch_process_fn is defined. Parameters: Name Type Description Default test_input Union[str, bytes, BufferedReader] Batch of sample input data to test model required Returns: Type Description bytes raw model predictions returned by batch_process_fn method Examples: chassis_model = chassis_client . create_model ( context = context , process_fn = process ) sample_input = sample_filepath = './sample_data.json' results = chassis_model . test_batch ( sample_filepath ) Source code in chassisml-sdk/chassisml/chassisml.py def test_batch ( self , test_input ): ''' Takes a single input file, creates a batch of size `batch_size` defined in `ChassisModel.create_model`, and runs a batch job against chassis model locally if `batch_process_fn` is defined. Args: test_input (Union[str, bytes, BufferedReader]): Batch of sample input data to test model Returns: bytes: raw model predictions returned by `batch_process_fn` method Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_input = sample_filepath = './sample_data.json' results = chassis_model.test_batch(sample_filepath) ``` ''' if not self . batch_input : raise NotImplementedError ( \"Batch inference not implemented.\" ) if hasattr ( self , 'batch_predict' ): batch_method = self . batch_predict else : batch_method = self . predict if isinstance ( test_input , _io . BufferedReader ): results = batch_method ( None ,[ test_input . read () for _ in range ( self . batch_size )]) elif isinstance ( test_input , bytes ): results = batch_method ( None ,[ test_input for _ in range ( self . batch_size )]) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): results = batch_method ( None ,[ open ( test_input , 'rb' ) . read () for _ in range ( self . batch_size )]) else : results = batch_method ( None ,[ bytes ( test_input , encoding = 'utf8' ) for _ in range ( self . batch_size )]) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return results test_env ( self , test_input_path , conda_env = None , fix_env = True ) Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. Parameters: Name Type Description Default test_input_path str Filepath to sample input data required conda_env str Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None fix_env bool Modifies conda or pip-installable packages into list of dependencies to be installed during the container build True Returns: Type Description Dict raw model predictions returned by process_fn or batch_process_fn run from within chassis service Examples: chassis_model = chassis_client . create_model ( context = context , process_fn = process ) sample_filepath = './sample_data.json' results = chassis_model . test_env ( sample_filepath ) Source code in chassisml-sdk/chassisml/chassisml.py def test_env ( self , test_input_path , conda_env = None , fix_env = True ): ''' Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. Args: test_input_path (str): Filepath to sample input data conda_env (str): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build Returns: Dict: raw model predictions returned by `process_fn` or `batch_process_fn` run from within chassis service Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test_env(sample_filepath) ``` ''' model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as model_f , \\ open ( test_input_path , 'rb' ) as test_input_f : files = [ ( 'sample_input' , test_input_f ), ( 'model' , model_f ) ] print ( 'Starting test job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_test_url , files = files ) res . raise_for_status () print ( 'Ok!' ) shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json ()","title":"Python SDK"},{"location":"chassisml-sdk-reference/#chassisml-python-sdk","text":"","title":"Chassisml Python SDK"},{"location":"chassisml-sdk-reference/#introduction","text":"The Chassisml Python SDK offers convenience functions that interact with the Chassisml service to automate the containerization and deployment of models to your preferred model serving platform. It is organized into two classes: ChassisClient : interacts with the Chassis routes that build a user's container from their model artifacts ChasissModel : creates a chassis compliant model out of a few lines of Python code from user supplied model artifacts First, install the Chassisml SDK to get started: pip install chassisml To import the library into your editor: import chassisml","title":"Introduction"},{"location":"chassisml-sdk-reference/#usage","text":"","title":"Usage"},{"location":"chassisml-sdk-reference/#chassisml-sdk.chassisml.chassisml.ChassisClient","text":"The Chassis Client object. This class is used to interact with the Kaniko service. Attributes: Name Type Description base_url str The base url for the API. Source code in chassisml-sdk/chassisml/chassisml.py class ChassisClient : \"\"\"The Chassis Client object. This class is used to interact with the Kaniko service. Attributes: base_url (str): The base url for the API. \"\"\" def __init__ ( self , base_url = 'http://localhost:5000' ): self . base_url = base_url def get_job_status ( self , job_id ): ''' Checks the status of a chassis job Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method Returns: Dict: JSON Chassis job status Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') job_status = chassis_client.get_job_status(job_id) ``` ''' route = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } ' res = requests . get ( route ) data = res . json () return data def block_until_complete ( self , job_id , timeout = 1800 , poll_interval = 5 ): ''' Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method timeout (int): Timeout threshold in seconds poll_intervall (int): Amount of time to wait in between API polls to check status of job Returns: Dict: final job status returned by `ChassisClient.block_until_complete` method Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') final_status = chassis_client.block_until_complete(job_id) ``` ''' endby = time . time () + timeout if ( timeout is not None ) else None while True : status = self . get_job_status ( job_id ) if status [ 'status' ][ 'succeeded' ] or status [ 'status' ][ 'failed' ]: return status if ( endby is not None ) and ( time . time () > endby - poll_interval ): print ( 'Timed out before completion.' ) return False time . sleep ( poll_interval ) def download_tar ( self , job_id , output_filename ): ''' Downloads container image as tar archive Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method output_filename (str): Local output filepath to save container image Returns: None: This method does not return an object Examples: ```python # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id) chassis_client.download_tar(job_id, \"./chassis-model.tar\") ``` ''' url = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } /download-tar' r = requests . get ( url ) if r . status_code == 200 : with open ( output_filename , 'wb' ) as f : f . write ( r . content ) else : print ( f 'Error download tar: { r . text } ' ) def create_model ( self , context , process_fn = None , batch_process_fn = None , batch_size = None ): ''' Builds chassis model locally Args: context (dict): Dictionary that will contain the trained model object and any other inference-specific dependencies that will persist while model container is running. Should include all objects that only need to be loaded one time (e.g., loaded model, labels file(s), data transformation objects, etc.) that can be accessed during inference. process_fn (function): Python function that must accept a single piece of input data in raw bytes form and `context` dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_process_fn (function): Python function that must accept a batch of input data in raw bytes form and `context` dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_size (int): Maximum batch size if `batch_process_fn` is defined Returns: ChassisModel: Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this [example](https://docs.modzy.com/docs/chassis-ml). ```python # Import and normalize data X_digits, y_digits = datasets.load_digits(return_X_y=True) X_digits = X_digits / X_digits.max() n_samples = len(X_digits) # Split data into training and test sets X_train = X_digits[: int(0.9 * n_samples)] y_train = y_digits[: int(0.9 * n_samples)] X_test = X_digits[int(0.9 * n_samples) :] y_test = y_digits[int(0.9 * n_samples) :] # Train Model logistic = LogisticRegression(max_iter=1000) print( \"LogisticRegression mean accuracy score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test) ) # Save small sample input to use for testing later sample = X_test[:5].tolist() with open(\"digits_sample.json\", 'w') as out: json.dump(sample, out) # 1. Define Context Dictionary context = {\"model\": logistic} # 2. Define Process function def process(input_bytes,context): inputs = np.array(json.loads(input_bytes)) inference_results = context[\"model\"].predict(inputs) structured_results = [] for inference_result in inference_results: structured_output = { \"data\": { \"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]} } } structured_results.append(structured_output) return structured_results # create Chassis model chassis_model = chassis_client.create_model(context=context,process_fn=process) ``` ''' if not ( process_fn or batch_process_fn ): raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) if ( batch_process_fn and not batch_size ) or ( batch_size and not batch_process_fn ): raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) return ChassisModel ( context , process_fn , batch_process_fn , batch_size , self . base_url )","title":"ChassisClient"},{"location":"chassisml-sdk-reference/#chassisml-sdk.chassisml.chassisml.ChassisClient.block_until_complete","text":"Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required timeout int Timeout threshold in seconds 1800 poll_intervall int Amount of time to wait in between API polls to check status of job required Returns: Type Description Dict final job status returned by ChassisClient.block_until_complete method Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) Source code in chassisml-sdk/chassisml/chassisml.py def block_until_complete ( self , job_id , timeout = 1800 , poll_interval = 5 ): ''' Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method timeout (int): Timeout threshold in seconds poll_intervall (int): Amount of time to wait in between API polls to check status of job Returns: Dict: final job status returned by `ChassisClient.block_until_complete` method Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') final_status = chassis_client.block_until_complete(job_id) ``` ''' endby = time . time () + timeout if ( timeout is not None ) else None while True : status = self . get_job_status ( job_id ) if status [ 'status' ][ 'succeeded' ] or status [ 'status' ][ 'failed' ]: return status if ( endby is not None ) and ( time . time () > endby - poll_interval ): print ( 'Timed out before completion.' ) return False time . sleep ( poll_interval )","title":"block_until_complete()"},{"location":"chassisml-sdk-reference/#chassisml-sdk.chassisml.chassisml.ChassisClient.create_model","text":"Builds chassis model locally Parameters: Name Type Description Default context dict Dictionary that will contain the trained model object and any other inference-specific dependencies that will persist while model container is running. Should include all objects that only need to be loaded one time (e.g., loaded model, labels file(s), data transformation objects, etc.) that can be accessed during inference. required process_fn function Python function that must accept a single piece of input data in raw bytes form and context dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the process method None batch_process_fn function Python function that must accept a batch of input data in raw bytes form and context dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the process method None batch_size int Maximum batch size if batch_process_fn is defined None Returns: Type Description ChassisModel Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this example . # Import and normalize data X_digits , y_digits = datasets . load_digits ( return_X_y = True ) X_digits = X_digits / X_digits . max () n_samples = len ( X_digits ) # Split data into training and test sets X_train = X_digits [: int ( 0.9 * n_samples )] y_train = y_digits [: int ( 0.9 * n_samples )] X_test = X_digits [ int ( 0.9 * n_samples ) :] y_test = y_digits [ int ( 0.9 * n_samples ) :] # Train Model logistic = LogisticRegression ( max_iter = 1000 ) print ( \"LogisticRegression mean accuracy score: %f \" % logistic . fit ( X_train , y_train ) . score ( X_test , y_test ) ) # Save small sample input to use for testing later sample = X_test [: 5 ] . tolist () with open ( \"digits_sample.json\" , 'w' ) as out : json . dump ( sample , out ) # 1. Define Context Dictionary context = { \"model\" : logistic } # 2. Define Process function def process ( input_bytes , context ): inputs = np . array ( json . loads ( input_bytes )) inference_results = context [ \"model\" ] . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results # create Chassis model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) Source code in chassisml-sdk/chassisml/chassisml.py def create_model ( self , context , process_fn = None , batch_process_fn = None , batch_size = None ): ''' Builds chassis model locally Args: context (dict): Dictionary that will contain the trained model object and any other inference-specific dependencies that will persist while model container is running. Should include all objects that only need to be loaded one time (e.g., loaded model, labels file(s), data transformation objects, etc.) that can be accessed during inference. process_fn (function): Python function that must accept a single piece of input data in raw bytes form and `context` dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_process_fn (function): Python function that must accept a batch of input data in raw bytes form and `context` dictionary as input parameters. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_size (int): Maximum batch size if `batch_process_fn` is defined Returns: ChassisModel: Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this [example](https://docs.modzy.com/docs/chassis-ml). ```python # Import and normalize data X_digits, y_digits = datasets.load_digits(return_X_y=True) X_digits = X_digits / X_digits.max() n_samples = len(X_digits) # Split data into training and test sets X_train = X_digits[: int(0.9 * n_samples)] y_train = y_digits[: int(0.9 * n_samples)] X_test = X_digits[int(0.9 * n_samples) :] y_test = y_digits[int(0.9 * n_samples) :] # Train Model logistic = LogisticRegression(max_iter=1000) print( \"LogisticRegression mean accuracy score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test) ) # Save small sample input to use for testing later sample = X_test[:5].tolist() with open(\"digits_sample.json\", 'w') as out: json.dump(sample, out) # 1. Define Context Dictionary context = {\"model\": logistic} # 2. Define Process function def process(input_bytes,context): inputs = np.array(json.loads(input_bytes)) inference_results = context[\"model\"].predict(inputs) structured_results = [] for inference_result in inference_results: structured_output = { \"data\": { \"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]} } } structured_results.append(structured_output) return structured_results # create Chassis model chassis_model = chassis_client.create_model(context=context,process_fn=process) ``` ''' if not ( process_fn or batch_process_fn ): raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) if ( batch_process_fn and not batch_size ) or ( batch_size and not batch_process_fn ): raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) return ChassisModel ( context , process_fn , batch_process_fn , batch_size , self . base_url )","title":"create_model()"},{"location":"chassisml-sdk-reference/#chassisml-sdk.chassisml.chassisml.ChassisClient.download_tar","text":"Downloads container image as tar archive Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required output_filename str Local output filepath to save container image required Returns: Type Description None This method does not return an object Examples: # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id) chassis_client . download_tar ( job_id , \"./chassis-model.tar\" ) Source code in chassisml-sdk/chassisml/chassisml.py def download_tar ( self , job_id , output_filename ): ''' Downloads container image as tar archive Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method output_filename (str): Local output filepath to save container image Returns: None: This method does not return an object Examples: ```python # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id) chassis_client.download_tar(job_id, \"./chassis-model.tar\") ``` ''' url = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } /download-tar' r = requests . get ( url ) if r . status_code == 200 : with open ( output_filename , 'wb' ) as f : f . write ( r . content ) else : print ( f 'Error download tar: { r . text } ' )","title":"download_tar()"},{"location":"chassisml-sdk-reference/#chassisml-sdk.chassisml.chassisml.ChassisClient.get_job_status","text":"Checks the status of a chassis job Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required Returns: Type Description Dict JSON Chassis job status Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) job_status = chassis_client . get_job_status ( job_id ) Source code in chassisml-sdk/chassisml/chassisml.py def get_job_status ( self , job_id ): ''' Checks the status of a chassis job Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method Returns: Dict: JSON Chassis job status Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') job_status = chassis_client.get_job_status(job_id) ``` ''' route = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } ' res = requests . get ( route ) data = res . json () return data","title":"get_job_status()"},{"location":"chassisml-sdk-reference/#chassisml-sdk.chassisml.chassisml.ChassisModel","text":"The Chassis Model object. This class inherits from mlflow.pyfunc.PythonModel and adds Chassis functionality. Attributes: Name Type Description predict function MLflow pyfunc compatible predict function. Will wrap user-provided function which takes two arguments: model_input (bytes) and model_context (dict). chassis_build_url str The build url for the Chassis API. Source code in chassisml-sdk/chassisml/chassisml.py class ChassisModel ( mlflow . pyfunc . PythonModel ): \"\"\"The Chassis Model object. This class inherits from `mlflow.pyfunc.PythonModel` and adds Chassis functionality. Attributes: predict (function): MLflow pyfunc compatible predict function. Will wrap user-provided function which takes two arguments: model_input (bytes) and model_context (dict). chassis_build_url (str): The build url for the Chassis API. \"\"\" def __init__ ( self , model_context , process_fn , batch_process_fn , batch_size , chassis_base_url ): if process_fn and batch_process_fn : if not batch_size : raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) self . predict = self . _gen_predict_method ( process_fn , model_context ) self . batch_predict = self . _gen_predict_method ( batch_process_fn , model_context , batch = True ) self . batch_input = True self . batch_size = batch_size elif process_fn and not batch_process_fn : self . predict = self . _gen_predict_method ( process_fn , model_context ) self . batch_input = False self . batch_size = None elif batch_process_fn and not process_fn : if not batch_size : raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) self . predict = self . _gen_predict_method ( batch_process_fn , model_context , batch_to_single = True ) self . batch_predict = self . _gen_predict_method ( batch_process_fn , model_context , batch = True ) self . batch_input = True self . batch_size = batch_size else : raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) self . chassis_build_url = urllib . parse . urljoin ( chassis_base_url , routes [ 'build' ]) self . chassis_test_url = urllib . parse . urljoin ( chassis_base_url , routes [ 'test' ]) def _gen_predict_method ( self , process_fn , model_context , batch = False , batch_to_single = False ): def predict ( _ , model_input ): if batch_to_single : output = process_fn ([ model_input ], model_context )[ 0 ] else : output = process_fn ( model_input , model_context ) if batch : return [ json . dumps ( out , separators = ( \",\" , \":\" ), cls = NumpyEncoder ) . encode () for out in output ] else : return json . dumps ( output , separators = ( \",\" , \":\" ), cls = NumpyEncoder ) . encode () return predict def test ( self , test_input ): ''' Runs a sample inference test on a single input on chassis model locally Args: test_input (Union[str, bytes, BufferedReader]): Single sample input data to test model Returns: bytes: raw model predictions returned by `process_fn` method Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test(sample_filepath) ``` ''' if isinstance ( test_input , _io . BufferedReader ): result = self . predict ( None , test_input . read ()) elif isinstance ( test_input , bytes ): result = self . predict ( None , test_input ) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): result = self . predict ( None , open ( test_input , 'rb' ) . read ()) else : result = self . predict ( None , bytes ( test_input , encoding = 'utf8' )) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return result def test_batch ( self , test_input ): ''' Takes a single input file, creates a batch of size `batch_size` defined in `ChassisModel.create_model`, and runs a batch job against chassis model locally if `batch_process_fn` is defined. Args: test_input (Union[str, bytes, BufferedReader]): Batch of sample input data to test model Returns: bytes: raw model predictions returned by `batch_process_fn` method Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_input = sample_filepath = './sample_data.json' results = chassis_model.test_batch(sample_filepath) ``` ''' if not self . batch_input : raise NotImplementedError ( \"Batch inference not implemented.\" ) if hasattr ( self , 'batch_predict' ): batch_method = self . batch_predict else : batch_method = self . predict if isinstance ( test_input , _io . BufferedReader ): results = batch_method ( None ,[ test_input . read () for _ in range ( self . batch_size )]) elif isinstance ( test_input , bytes ): results = batch_method ( None ,[ test_input for _ in range ( self . batch_size )]) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): results = batch_method ( None ,[ open ( test_input , 'rb' ) . read () for _ in range ( self . batch_size )]) else : results = batch_method ( None ,[ bytes ( test_input , encoding = 'utf8' ) for _ in range ( self . batch_size )]) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return results def test_env ( self , test_input_path , conda_env = None , fix_env = True ): ''' Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. Args: test_input_path (str): Filepath to sample input data conda_env (str): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build Returns: Dict: raw model predictions returned by `process_fn` or `batch_process_fn` run from within chassis service Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test_env(sample_filepath) ``` ''' model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as model_f , \\ open ( test_input_path , 'rb' ) as test_input_f : files = [ ( 'sample_input' , test_input_f ), ( 'model' , model_f ) ] print ( 'Starting test job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_test_url , files = files ) res . raise_for_status () print ( 'Ok!' ) shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () def save ( self , path , conda_env = None , overwrite = False ): ''' Saves a copy of ChassisModel to local filepath Args: path (str): Filepath to save chassis model as local MLflow model conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment overwrite (bool): If True, overwrites existing contents of `path` parameter Returns: None: This method does not return an object Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) chassis_model.save(\"local_model_directory\") ``` ''' if overwrite and os . path . exists ( path ): shutil . rmtree ( path ) mlflow . pyfunc . save_model ( path = path , python_model = self , conda_env = conda_env ) print ( \"Chassis model saved.\" ) def publish ( self , model_name , model_version , registry_user , registry_pass , conda_env = None , fix_env = True , gpu = False , modzy_sample_input_path = None , modzy_api_key = None ): ''' Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Args: model_name (str): Model name that serves as model's name in Modzy and docker registry repository name. **Note**: this string cannot include punctuation model_version (str): Version of model registry_user (str): Docker registry username registry_pass (str): Docker registry password conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build gpu (bool): If True, builds container image that runs on GPU hardware modzy_sample_input_path (str): Filepath to sample input data. Required to deploy model to Modzy modzy_api_key (str): Valid Modzy API Key Returns: Dict: Response to Chassis `/build` endpoint Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) ``` ''' if ( modzy_sample_input_path or modzy_api_key ) and not \\ ( modzy_sample_input_path and modzy_api_key ): raise ValueError ( '\"modzy_sample_input_path\", and \"modzy_api_key\" must both be provided to publish to Modzy.' ) try : model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) image_name = \"-\" . join ( model_name . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () . split ()) image_data = { 'name' : \" {} / {} \" . format ( registry_user , \" {} : {} \" . format ( image_name , model_version )), 'model_name' : model_name , 'model_path' : tmppath , 'registry_auth' : base64 . b64encode ( \" {} : {} \" . format ( registry_user , registry_pass ) . encode ( \"utf-8\" )) . decode ( \"utf-8\" ), 'publish' : True , 'gpu' : gpu } if modzy_sample_input_path and modzy_api_key : modzy_metadata_path = os . path . join ( tmppath , MODZY_YAML_NAME ) modzy_data = { 'metadata_path' : modzy_metadata_path , 'sample_input_path' : modzy_sample_input_path , 'deploy' : True , 'api_key' : modzy_api_key } write_modzy_yaml ( model_name , model_version , modzy_metadata_path , batch_size = self . batch_size , gpu = gpu ) else : modzy_data = {} with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as f : files = [ ( 'image_data' , json . dumps ( image_data )), ( 'modzy_data' , json . dumps ( modzy_data )), ( 'model' , f ) ] file_pointers = [] for key , file_key in [( 'metadata_path' , 'modzy_metadata_data' ), ( 'sample_input_path' , 'modzy_sample_input_data' )]: value = modzy_data . get ( key ) if value : fp = open ( value , 'rb' ) file_pointers . append ( fp ) files . append (( file_key , fp )) print ( 'Starting build job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_build_url , files = files ) res . raise_for_status () print ( 'Ok!' ) for fp in file_pointers : fp . close () shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () except Exception as e : print ( e ) if os . path . exists ( tmppath ): shutil . rmtree ( tmppath ) if os . path . exists ( model_directory ): shutil . rmtree ( model_directory ) return False","title":"ChassisModel"},{"location":"chassisml-sdk-reference/#chassisml-sdk.chassisml.chassisml.ChassisModel.publish","text":"Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Parameters: Name Type Description Default model_name str Model name that serves as model's name in Modzy and docker registry repository name. Note : this string cannot include punctuation required model_version str Version of model required registry_user str Docker registry username required registry_pass str Docker registry password required conda_env Union[str, dict] Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None fix_env bool Modifies conda or pip-installable packages into list of dependencies to be installed during the container build True gpu bool If True, builds container image that runs on GPU hardware False modzy_sample_input_path str Filepath to sample input data. Required to deploy model to Modzy None modzy_api_key str Valid Modzy API Key None Returns: Type Description Dict Response to Chassis /build endpoint Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) Source code in chassisml-sdk/chassisml/chassisml.py def publish ( self , model_name , model_version , registry_user , registry_pass , conda_env = None , fix_env = True , gpu = False , modzy_sample_input_path = None , modzy_api_key = None ): ''' Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Args: model_name (str): Model name that serves as model's name in Modzy and docker registry repository name. **Note**: this string cannot include punctuation model_version (str): Version of model registry_user (str): Docker registry username registry_pass (str): Docker registry password conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build gpu (bool): If True, builds container image that runs on GPU hardware modzy_sample_input_path (str): Filepath to sample input data. Required to deploy model to Modzy modzy_api_key (str): Valid Modzy API Key Returns: Dict: Response to Chassis `/build` endpoint Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(context=context,process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) ``` ''' if ( modzy_sample_input_path or modzy_api_key ) and not \\ ( modzy_sample_input_path and modzy_api_key ): raise ValueError ( '\"modzy_sample_input_path\", and \"modzy_api_key\" must both be provided to publish to Modzy.' ) try : model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) image_name = \"-\" . join ( model_name . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () . split ()) image_data = { 'name' : \" {} / {} \" . format ( registry_user , \" {} : {} \" . format ( image_name , model_version )), 'model_name' : model_name , 'model_path' : tmppath , 'registry_auth' : base64 . b64encode ( \" {} : {} \" . format ( registry_user , registry_pass ) . encode ( \"utf-8\" )) . decode ( \"utf-8\" ), 'publish' : True , 'gpu' : gpu } if modzy_sample_input_path and modzy_api_key : modzy_metadata_path = os . path . join ( tmppath , MODZY_YAML_NAME ) modzy_data = { 'metadata_path' : modzy_metadata_path , 'sample_input_path' : modzy_sample_input_path , 'deploy' : True , 'api_key' : modzy_api_key } write_modzy_yaml ( model_name , model_version , modzy_metadata_path , batch_size = self . batch_size , gpu = gpu ) else : modzy_data = {} with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as f : files = [ ( 'image_data' , json . dumps ( image_data )), ( 'modzy_data' , json . dumps ( modzy_data )), ( 'model' , f ) ] file_pointers = [] for key , file_key in [( 'metadata_path' , 'modzy_metadata_data' ), ( 'sample_input_path' , 'modzy_sample_input_data' )]: value = modzy_data . get ( key ) if value : fp = open ( value , 'rb' ) file_pointers . append ( fp ) files . append (( file_key , fp )) print ( 'Starting build job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_build_url , files = files ) res . raise_for_status () print ( 'Ok!' ) for fp in file_pointers : fp . close () shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () except Exception as e : print ( e ) if os . path . exists ( tmppath ): shutil . rmtree ( tmppath ) if os . path . exists ( model_directory ): shutil . rmtree ( model_directory ) return False","title":"publish()"},{"location":"chassisml-sdk-reference/#chassisml-sdk.chassisml.chassisml.ChassisModel.save","text":"Saves a copy of ChassisModel to local filepath Parameters: Name Type Description Default path str Filepath to save chassis model as local MLflow model required conda_env Union[str, dict] Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None overwrite bool If True, overwrites existing contents of path parameter False Returns: Type Description None This method does not return an object Examples: chassis_model = chassis_client . create_model ( context = context , process_fn = process ) chassis_model . save ( \"local_model_directory\" ) Source code in chassisml-sdk/chassisml/chassisml.py def save ( self , path , conda_env = None , overwrite = False ): ''' Saves a copy of ChassisModel to local filepath Args: path (str): Filepath to save chassis model as local MLflow model conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment overwrite (bool): If True, overwrites existing contents of `path` parameter Returns: None: This method does not return an object Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) chassis_model.save(\"local_model_directory\") ``` ''' if overwrite and os . path . exists ( path ): shutil . rmtree ( path ) mlflow . pyfunc . save_model ( path = path , python_model = self , conda_env = conda_env ) print ( \"Chassis model saved.\" )","title":"save()"},{"location":"chassisml-sdk-reference/#chassisml-sdk.chassisml.chassisml.ChassisModel.test","text":"Runs a sample inference test on a single input on chassis model locally Parameters: Name Type Description Default test_input Union[str, bytes, BufferedReader] Single sample input data to test model required Returns: Type Description bytes raw model predictions returned by process_fn method Examples: chassis_model = chassis_client . create_model ( context = context , process_fn = process ) sample_filepath = './sample_data.json' results = chassis_model . test ( sample_filepath ) Source code in chassisml-sdk/chassisml/chassisml.py def test ( self , test_input ): ''' Runs a sample inference test on a single input on chassis model locally Args: test_input (Union[str, bytes, BufferedReader]): Single sample input data to test model Returns: bytes: raw model predictions returned by `process_fn` method Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test(sample_filepath) ``` ''' if isinstance ( test_input , _io . BufferedReader ): result = self . predict ( None , test_input . read ()) elif isinstance ( test_input , bytes ): result = self . predict ( None , test_input ) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): result = self . predict ( None , open ( test_input , 'rb' ) . read ()) else : result = self . predict ( None , bytes ( test_input , encoding = 'utf8' )) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return result","title":"test()"},{"location":"chassisml-sdk-reference/#chassisml-sdk.chassisml.chassisml.ChassisModel.test_batch","text":"Takes a single input file, creates a batch of size batch_size defined in ChassisModel.create_model , and runs a batch job against chassis model locally if batch_process_fn is defined. Parameters: Name Type Description Default test_input Union[str, bytes, BufferedReader] Batch of sample input data to test model required Returns: Type Description bytes raw model predictions returned by batch_process_fn method Examples: chassis_model = chassis_client . create_model ( context = context , process_fn = process ) sample_input = sample_filepath = './sample_data.json' results = chassis_model . test_batch ( sample_filepath ) Source code in chassisml-sdk/chassisml/chassisml.py def test_batch ( self , test_input ): ''' Takes a single input file, creates a batch of size `batch_size` defined in `ChassisModel.create_model`, and runs a batch job against chassis model locally if `batch_process_fn` is defined. Args: test_input (Union[str, bytes, BufferedReader]): Batch of sample input data to test model Returns: bytes: raw model predictions returned by `batch_process_fn` method Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_input = sample_filepath = './sample_data.json' results = chassis_model.test_batch(sample_filepath) ``` ''' if not self . batch_input : raise NotImplementedError ( \"Batch inference not implemented.\" ) if hasattr ( self , 'batch_predict' ): batch_method = self . batch_predict else : batch_method = self . predict if isinstance ( test_input , _io . BufferedReader ): results = batch_method ( None ,[ test_input . read () for _ in range ( self . batch_size )]) elif isinstance ( test_input , bytes ): results = batch_method ( None ,[ test_input for _ in range ( self . batch_size )]) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): results = batch_method ( None ,[ open ( test_input , 'rb' ) . read () for _ in range ( self . batch_size )]) else : results = batch_method ( None ,[ bytes ( test_input , encoding = 'utf8' ) for _ in range ( self . batch_size )]) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return results","title":"test_batch()"},{"location":"chassisml-sdk-reference/#chassisml-sdk.chassisml.chassisml.ChassisModel.test_env","text":"Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. Parameters: Name Type Description Default test_input_path str Filepath to sample input data required conda_env str Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None fix_env bool Modifies conda or pip-installable packages into list of dependencies to be installed during the container build True Returns: Type Description Dict raw model predictions returned by process_fn or batch_process_fn run from within chassis service Examples: chassis_model = chassis_client . create_model ( context = context , process_fn = process ) sample_filepath = './sample_data.json' results = chassis_model . test_env ( sample_filepath ) Source code in chassisml-sdk/chassisml/chassisml.py def test_env ( self , test_input_path , conda_env = None , fix_env = True ): ''' Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. Args: test_input_path (str): Filepath to sample input data conda_env (str): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build Returns: Dict: raw model predictions returned by `process_fn` or `batch_process_fn` run from within chassis service Examples: ```python chassis_model = chassis_client.create_model(context=context,process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test_env(sample_filepath) ``` ''' model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as model_f , \\ open ( test_input_path , 'rb' ) as test_input_f : files = [ ( 'sample_input' , test_input_f ), ( 'model' , model_f ) ] print ( 'Starting test job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_test_url , files = files ) res . raise_for_status () print ( 'Ok!' ) shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json ()","title":"test_env()"},{"location":"design/","text":"Design & Architecture Chassis makes it easy to create a deployable docker image from your trained ML model. The idea behind this project is to provide Data Scientists with a way to package their models into a Docker image. This image will manage to build the inference service compatible with several common platforms for free. At the moment, Chassis images are compatible with KFServing and Modzy gRPC. This means you can deploy your built image into these platforms once it has been built. Deploy Chassis, send your model to it and start using the built container image to run inference on your data. Architecture This diagram shows the overall architecture of the Chassis system: This diagram zooms in on the generated container, showing the pluggable interface, configurable at runtime: Goals Chassis is a Kubernetes service that can be deployed in your preferred cluster using Helm. It works by creating jobs that can be run in parallel to create Docker images that package ML models. It provides integration with most common deployment platforms so your model will be ready to be deployed in a simple way. It also provides a python SDK that makes it very easy to communicate with Chassis service in order to build your image. Simple Just make a request to build your image using the python SDK Small set of dependencies: mlflow, flask Supports multiple deployment platforms No DevOps knowledge needed Fast Start building the image as soon as you make the request Automatically upload the image to Docker Hub Image ready to be deployed Secure Using Kaniko to securely build the image Non-goals Some non-goals of this project are: Deploy the built image - this is up to the platform that you deploy the container into, like KFServing or Modzy","title":"Design & Architecture"},{"location":"design/#design-architecture","text":"Chassis makes it easy to create a deployable docker image from your trained ML model. The idea behind this project is to provide Data Scientists with a way to package their models into a Docker image. This image will manage to build the inference service compatible with several common platforms for free. At the moment, Chassis images are compatible with KFServing and Modzy gRPC. This means you can deploy your built image into these platforms once it has been built. Deploy Chassis, send your model to it and start using the built container image to run inference on your data.","title":"Design &amp; Architecture"},{"location":"design/#architecture","text":"This diagram shows the overall architecture of the Chassis system: This diagram zooms in on the generated container, showing the pluggable interface, configurable at runtime:","title":"Architecture"},{"location":"design/#goals","text":"Chassis is a Kubernetes service that can be deployed in your preferred cluster using Helm. It works by creating jobs that can be run in parallel to create Docker images that package ML models. It provides integration with most common deployment platforms so your model will be ready to be deployed in a simple way. It also provides a python SDK that makes it very easy to communicate with Chassis service in order to build your image. Simple Just make a request to build your image using the python SDK Small set of dependencies: mlflow, flask Supports multiple deployment platforms No DevOps knowledge needed Fast Start building the image as soon as you make the request Automatically upload the image to Docker Hub Image ready to be deployed Secure Using Kaniko to securely build the image","title":"Goals"},{"location":"design/#non-goals","text":"Some non-goals of this project are: Deploy the built image - this is up to the platform that you deploy the container into, like KFServing or Modzy","title":"Non-goals"},{"location":"faqs/","text":"FAQs Why the name Chassis? If your model is the engine, we provide a chassis for your model to drive!","title":"FAQs"},{"location":"faqs/#faqs","text":"","title":"FAQs"},{"location":"faqs/#why-the-name-chassis","text":"If your model is the engine, we provide a chassis for your model to drive!","title":"Why the name Chassis?"},{"location":"get-involved/","text":"Get Involved Feel free to fork and open pull requests to contribute to modzy/chassis . Join the #chassisml channel on Modzy's Discord Server where our maintainers meet to plan changes and improvements. We also have a #chassis-model-builder Slack channel on the MLOps.community Slack !","title":"Get Involved"},{"location":"get-involved/#get-involved","text":"Feel free to fork and open pull requests to contribute to modzy/chassis . Join the #chassisml channel on Modzy's Discord Server where our maintainers meet to plan changes and improvements. We also have a #chassis-model-builder Slack channel on the MLOps.community Slack !","title":"Get Involved"},{"location":"help/","text":"Help Join our #chassis-model-builder Slack channel on the MLOps.community Slack and we'll gladly try to help you! Use of Cookies We use Google Analytics cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for.","title":"Help"},{"location":"help/#help","text":"Join our #chassis-model-builder Slack channel on the MLOps.community Slack and we'll gladly try to help you!","title":"Help"},{"location":"help/#use-of-cookies","text":"We use Google Analytics cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for.","title":"Use of Cookies"},{"location":"release-notes/","text":"Release Notes See also: PyPI . v0.0.1 First working release of Chassis.","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"See also: PyPI .","title":"Release Notes"},{"location":"release-notes/#v001","text":"First working release of Chassis.","title":"v0.0.1"},{"location":"service-reference/","text":"Chassisml API Reference Welcome to the Chassisml API Reference documentation homepage. The API is designed using the principles of REST services using standard HTTP verbs and status codes, implemented with Flask . On this page, you will find documentation for: Available REST endpoints in API Service Methods implemented within the each endpoint Endpoints /health (GET) Confirms Chassis service is up and running /build (POST) Kicks off the container image build process /job/{job_id} (GET) Retrieves the status of a chassis /build job /job/{job_id}/download-tar (GET) Retrieves docker image tar archive from a volume attached to the Kubernetes cluster hosting chassis and downloads it to a local filepath /test (POST) Creates a conda environment as specified by the user's model artifacts and runs the ChassisModel to ensure the model code can run within the provided conda environment Functions build_image () This method is run by the /build endpoint. It generates a model image based upon a POST request. The request.files structure can be seen in the Python SDK docs. Parameters: Name Type Description Default None None This method does not take any parameters required Returns: Type Description Dict information about whether or not the image build resulted in an error Source code in service/app.py def build_image (): ''' This method is run by the `/build` endpoint. It generates a model image based upon a POST request. The `request.files` structure can be seen in the Python SDK docs. Args: None (None): This method does not take any parameters Returns: Dict: information about whether or not the image build resulted in an error ''' if not ( 'image_data' in request . files and 'model' in request . files ): return 'Both model and image_data are required' , 500 # retrieve image_data and populate variables accordingly image_data = json . load ( request . files . get ( 'image_data' )) model_name = image_data . get ( 'model_name' ) image_name = image_data . get ( 'name' ) gpu = image_data . get ( 'gpu' ) publish = image_data . get ( 'publish' , False ) publish = True if publish else '' registry_auth = image_data . get ( 'registry_auth' ) # retrieve binary representations for all three variables model = request . files . get ( 'model' ) modzy_metadata_data = request . files . get ( 'modzy_metadata_data' ) modzy_sample_input_data = request . files . get ( 'modzy_sample_input_data' ) # json string loaded into variable modzy_data = json . load ( request . files . get ( 'modzy_data' ) or {}) # This is a future proofing variable in case we encounter a model that cannot be converted into mlflow. # It will remain hardcoded for now. module_name = 'mlflow' # This name is a random id used to ensure that all jobs are uniquely named and traceable. random_name = str ( uuid . uuid4 ()) # Unzip model archive unzip_model ( model , module_name , random_name ) # User can build the image but not deploy it to Modzy. So no input_sample is mandatory. # On the other hand, the model.yaml is needed to build the image so proceed with it. # save the sample input to the modzy_sample_input_path directory if modzy_data : modzy_sample_input_path = extract_modzy_sample_input ( modzy_sample_input_data , module_name , random_name ) modzy_data [ 'modzy_sample_input_path' ] = modzy_sample_input_path # TODO: this probably should only be done if modzy_data is true. modzy_metadata_path = extract_modzy_metadata ( modzy_metadata_data , module_name , random_name ) modzy_data [ 'modzy_metadata_path' ] = modzy_metadata_path # this path is the local location that kaniko will store the image it creates path_to_tar_file = f ' { DATA_DIR } /kaniko_image- { random_name } .tar' logger . debug ( f 'Request data: { image_name } , { module_name } , { model_name } , { path_to_tar_file } ' ) error = run_kaniko ( image_name , module_name , model_name , path_to_tar_file , random_name , modzy_data , publish , registry_auth , gpu ) if error : return { 'error' : error , 'job_id' : None } return { 'error' : False , 'job_id' : f ' { K_JOB_NAME } - { random_name } ' } create_job_object ( image_name , module_name , model_name , path_to_tar_file , random_name , modzy_data , publish , registry_auth , gpu = False ) This utility method sets up all the required objects needed to create a model image and is run within the run_kaniko method. Parameters: Name Type Description Default image_name str container image name required module_name str reference module to locate location within service input is saved required model_name str name of model to package required path_to_tar_file str filepath destination to save docker image tar file required random_name str random id generated during build process that is used to ensure that all jobs are uniquely named and traceable required modzy_data str modzy_metadata_path returned from extract_modzy_metadata method required publish bool determines if image will be published to Docker registry required registry_auth dict Docker registry authorization credentials required gpu bool If True , will build container image that runs on GPU False Returns: Type Description Job Chassis job object Source code in service/app.py def create_job_object ( image_name , module_name , model_name , path_to_tar_file , random_name , modzy_data , publish , registry_auth , gpu = False ): ''' This utility method sets up all the required objects needed to create a model image and is run within the `run_kaniko` method. Args: image_name (str): container image name module_name (str): reference module to locate location within service input is saved model_name (str): name of model to package path_to_tar_file (str): filepath destination to save docker image tar file random_name (str): random id generated during build process that is used to ensure that all jobs are uniquely named and traceable modzy_data (str): modzy_metadata_path returned from `extract_modzy_metadata` method publish (bool): determines if image will be published to Docker registry registry_auth (dict): Docker registry authorization credentials gpu (bool): If `True`, will build container image that runs on GPU Returns: Job: Chassis job object ''' # job_name = f ' { K_JOB_NAME } - { random_name } ' # credential setup for Docker Hub. # json for holding registry credentials that will access docker hub. # reference: https://github.com/GoogleContainerTools/kaniko#pushing-to-docker-hub registry_credentials = f ' {{ \"auths\": {{ \"https://index.docker.io/v1/\": {{ \"auth\":\" { registry_auth } \" }}}}}} ' # mount path leads to /data # this is a mount point. NOT the volume itself. # name aligns with a volume defined below. data_volume_mount = client . V1VolumeMount ( mount_path = MOUNT_PATH_DIR , name = \"local-volume-code\" ) if CHASSIS_DEV else client . V1VolumeMount ( mount_path = MOUNT_PATH_DIR , name = K_DATA_VOLUME_NAME ) # This volume will be used by init container to populate registry credentials. # mount leads to /tmp/credentials # this is a mount point. NOT the volume itself. # name aligns with a volume defined below. init_empty_dir_volume_mount = client . V1VolumeMount ( mount_path = K_INIT_EMPTY_DIR_PATH , name = K_EMPTY_DIR_NAME ) # This volume will be used by kaniko container to get registry credentials. # mount path leads to /kaniko/.docker per kaniko reference documentation # this is a mount point. NOT the volume itself. # name aligns with a volume defined below. kaniko_empty_dir_volume_mount = client . V1VolumeMount ( mount_path = K_KANIKO_EMPTY_DIR_PATH , name = K_EMPTY_DIR_NAME ) # This container is used to populate registry credentials. # it only runs the single command in shell to echo our credentials into their proper file # per the reference documentation for Docker Hub # TODO: add credentials for Cloud Providers init_container = client . V1Container ( name = 'credentials' , image = 'busybox' , volume_mounts = [ init_empty_dir_volume_mount ], command = [ '/bin/sh' , '-c' , f 'echo \\' { registry_credentials } \\' > { K_INIT_EMPTY_DIR_PATH } /config.json' ] ) # This is the kaniko container used to build the final image. kaniko_args = [ f '--dockerfile= { DATA_DIR } /flavours/ { module_name } /Dockerfile { \".gpu\" if gpu else \"\" } ' , '' if publish else '--no-push' , f '--tarPath= { path_to_tar_file } ' , f '--destination= { image_name }{ \"\" if \":\" in image_name else \":latest\" } ' , f '--context= { DATA_DIR } ' , f '--build-arg=MODEL_DIR=model- { random_name } ' , f '--build-arg=MODZY_METADATA_PATH= { modzy_data . get ( \"modzy_metadata_path\" ) } ' , f '--build-arg=MODEL_NAME= { model_name } ' , f '--build-arg=MODEL_CLASS= { module_name } ' , # Modzy is the default interface. '--build-arg=INTERFACE=modzy' , ] init_container_kaniko = client . V1Container ( name = 'kaniko' , image = 'gcr.io/kaniko-project/executor:latest' , volume_mounts = [ data_volume_mount , kaniko_empty_dir_volume_mount ], args = kaniko_args ) modzy_uploader_container = client . V1Container ( name = 'modzy-uploader' , image = MODZY_UPLOADER_REPOSITORY , volume_mounts = [ data_volume_mount ], env = [ client . V1EnvVar ( name = 'JOB_NAME' , value = job_name ), client . V1EnvVar ( name = 'ENVIRONMENT' , value = ENVIRONMENT ) ], args = [ f '--api_key= { modzy_data . get ( \"api_key\" ) } ' , f '--deploy= { True if modzy_data . get ( \"deploy\" ) else \"\" } ' , f '--sample_input_path= { modzy_data . get ( \"modzy_sample_input_path\" ) } ' , f '--metadata_path= { DATA_DIR } / { modzy_data . get ( \"modzy_metadata_path\" ) } ' , f '--image_tag= { image_name }{ \"\" if \":\" in image_name else \":latest\" } ' , ] ) # volume claim data_pv_claim = client . V1PersistentVolumeClaimVolumeSource ( claim_name = \"dir-claim-chassis\" ) if CHASSIS_DEV else client . V1PersistentVolumeClaimVolumeSource ( claim_name = K_DATA_VOLUME_NAME ) # volume holding data data_volume = client . V1Volume ( name = \"local-volume-code\" , persistent_volume_claim = data_pv_claim ) if CHASSIS_DEV else client . V1Volume ( name = K_DATA_VOLUME_NAME , persistent_volume_claim = data_pv_claim ) # volume holding credentials empty_dir_volume = client . V1Volume ( name = K_EMPTY_DIR_NAME , empty_dir = client . V1EmptyDirVolumeSource () ) # Pod spec for the image build process pod_spec = client . V1PodSpec ( service_account_name = K_SERVICE_ACCOUNT_NAME , restart_policy = 'Never' , init_containers = [ init_container , init_container_kaniko ], containers = [ modzy_uploader_container ], volumes = [ data_volume , empty_dir_volume ] ) # setup and initiate model image build template = client . V1PodTemplateSpec ( metadata = client . V1ObjectMeta ( name = job_name ), spec = pod_spec ) spec = client . V1JobSpec ( backoff_limit = 0 , template = template ) job = client . V1Job ( api_version = 'batch/v1' , kind = 'Job' , metadata = client . V1ObjectMeta ( name = job_name , ), spec = spec ) return job download_tar ( job_id ) This method is run by the /job/{job_id}/download-tar endpoint. It downloads the container image from kaniko, built during the chassis job with the name job_id Parameters: Name Type Description Default job_id str valid Chassis job identifier, generated by create_job method required Returns: Type Description Dict response from download_tar endpoint Source code in service/app.py def download_tar ( job_id ): ''' This method is run by the `/job/{job_id}/download-tar` endpoint. It downloads the container image from kaniko, built during the chassis job with the name `job_id` Args: job_id (str): valid Chassis job identifier, generated by `create_job` method Returns: Dict: response from `download_tar` endpoint ''' uid = job_id . split ( f ' { K_JOB_NAME } -' )[ 1 ] return send_from_directory ( DATA_DIR , path = f 'kaniko_image- { uid } .tar' , as_attachment = False ) get_job_status ( job_id ) This method is run by the /job/{job_id} endpoint. Based on a GET request, it retrieves the status of the Kaniko job and the results if the job has completed. Parameters: Name Type Description Default job_id str valid Chassis job identifier, generated by create_job method required Returns: Type Description Dict Dictionary containing corresponding job data of job job_id Source code in service/app.py def get_job_status ( job_id ): ''' This method is run by the `/job/{job_id}` endpoint. Based on a GET request, it retrieves the status of the Kaniko job and the results if the job has completed. Args: job_id (str): valid Chassis job identifier, generated by `create_job` method Returns: Dict: Dictionary containing corresponding job data of job `job_id` ''' if CHASSIS_DEV : # if you are doing local dev you need to point at the local kubernetes cluster with your config file kubefile = os . getenv ( \"CHASSIS_KUBECONFIG\" ) config . load_kube_config ( kubefile ) else : # if the service is running inside a cluster during production then the config can be inherited config . load_incluster_config () batch_v1 = client . BatchV1Api () try : job = batch_v1 . read_namespaced_job ( job_id , ENVIRONMENT ) annotations = job . metadata . annotations or {} result = annotations . get ( 'result' ) result = json . loads ( result ) if result else None status = job . status job_data = { 'result' : result , 'status' : status . to_dict () } return job_data except ApiException as e : logger . error ( f 'Exception when getting job status: { e } ' ) return e . body test_model () This method is run by the /test endpoint. It creates a new conda environment from the provided conda.yaml file and then tests the provided model in that conda environment with provided test input file. Parameters: Name Type Description Default None None This method does not take any parameters required Returns: Type Description Dict model response to /test endpoint. Should contain either successful predictions or error message Source code in service/app.py def test_model (): ''' This method is run by the `/test` endpoint. It creates a new conda environment from the provided `conda.yaml` file and then tests the provided model in that conda environment with provided test input file. Args: None (None): This method does not take any parameters Returns: Dict: model response to `/test` endpoint. Should contain either successful predictions or error message ''' if not ( 'sample_input' in request . files and 'model' in request . files ): return 'Both sample input and model are required' , 500 output_dict = {} # retrieve binary representations for both variables model = request . files . get ( 'model' ) sample_input = request . files . get ( 'sample_input' ) # This is a future proofing variable in case we encounter a model that cannot be converted into mlflow. # It will remain hardcoded for now. module_name = 'mlflow' # This name is a random id used to ensure that all jobs are uniquely named and traceable. random_name = str ( uuid . uuid4 ()) # Unzip model archive unzipped_path = unzip_model ( model , module_name , random_name ) # get sample input path sample_input_path = extract_modzy_sample_input ( sample_input , module_name , random_name ) # create conda env, return error if fails try : tmp_env_name = str ( time . time ()) rm_env_cmd = \"conda env remove --name {} \" . format ( tmp_env_name ) yaml_path = os . path . join ( unzipped_path , \"conda.yaml\" ) create_env_cmd = \"conda env create -f {} -n {} \" . format ( yaml_path , tmp_env_name ) subprocess . run ( create_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' , check = True ) except subprocess . CalledProcessError as e : print ( e ) subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) output_dict [ \"env_error\" ] = e . stderr . decode () return output_dict # test model in env with sample input file, return error if fails try : test_model_cmd = \"\"\" source activate {} ; python test_chassis_model.py {} {} \"\"\" . format ( tmp_env_name , unzipped_path , sample_input_path ) test_ret = subprocess . run ( test_model_cmd , capture_output = True , shell = True , executable = '/bin/bash' , check = True ) output_dict [ \"model_output\" ] = test_ret . stdout . decode () except subprocess . CalledProcessError as e : subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) error_output = e . stderr . decode () output_dict [ \"model_error\" ] = error_output [ error_output . find ( 'in process' ):] return output_dict # if we make it here, test was successful, remove env and return output subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) return output_dict","title":"Chassisml API Reference"},{"location":"service-reference/#chassisml-api-reference","text":"Welcome to the Chassisml API Reference documentation homepage. The API is designed using the principles of REST services using standard HTTP verbs and status codes, implemented with Flask . On this page, you will find documentation for: Available REST endpoints in API Service Methods implemented within the each endpoint","title":"Chassisml API Reference"},{"location":"service-reference/#endpoints","text":"/health (GET) Confirms Chassis service is up and running /build (POST) Kicks off the container image build process /job/{job_id} (GET) Retrieves the status of a chassis /build job /job/{job_id}/download-tar (GET) Retrieves docker image tar archive from a volume attached to the Kubernetes cluster hosting chassis and downloads it to a local filepath /test (POST) Creates a conda environment as specified by the user's model artifacts and runs the ChassisModel to ensure the model code can run within the provided conda environment","title":"Endpoints"},{"location":"service-reference/#service.app-functions","text":"","title":"Functions"},{"location":"service-reference/#service.app.build_image","text":"This method is run by the /build endpoint. It generates a model image based upon a POST request. The request.files structure can be seen in the Python SDK docs. Parameters: Name Type Description Default None None This method does not take any parameters required Returns: Type Description Dict information about whether or not the image build resulted in an error Source code in service/app.py def build_image (): ''' This method is run by the `/build` endpoint. It generates a model image based upon a POST request. The `request.files` structure can be seen in the Python SDK docs. Args: None (None): This method does not take any parameters Returns: Dict: information about whether or not the image build resulted in an error ''' if not ( 'image_data' in request . files and 'model' in request . files ): return 'Both model and image_data are required' , 500 # retrieve image_data and populate variables accordingly image_data = json . load ( request . files . get ( 'image_data' )) model_name = image_data . get ( 'model_name' ) image_name = image_data . get ( 'name' ) gpu = image_data . get ( 'gpu' ) publish = image_data . get ( 'publish' , False ) publish = True if publish else '' registry_auth = image_data . get ( 'registry_auth' ) # retrieve binary representations for all three variables model = request . files . get ( 'model' ) modzy_metadata_data = request . files . get ( 'modzy_metadata_data' ) modzy_sample_input_data = request . files . get ( 'modzy_sample_input_data' ) # json string loaded into variable modzy_data = json . load ( request . files . get ( 'modzy_data' ) or {}) # This is a future proofing variable in case we encounter a model that cannot be converted into mlflow. # It will remain hardcoded for now. module_name = 'mlflow' # This name is a random id used to ensure that all jobs are uniquely named and traceable. random_name = str ( uuid . uuid4 ()) # Unzip model archive unzip_model ( model , module_name , random_name ) # User can build the image but not deploy it to Modzy. So no input_sample is mandatory. # On the other hand, the model.yaml is needed to build the image so proceed with it. # save the sample input to the modzy_sample_input_path directory if modzy_data : modzy_sample_input_path = extract_modzy_sample_input ( modzy_sample_input_data , module_name , random_name ) modzy_data [ 'modzy_sample_input_path' ] = modzy_sample_input_path # TODO: this probably should only be done if modzy_data is true. modzy_metadata_path = extract_modzy_metadata ( modzy_metadata_data , module_name , random_name ) modzy_data [ 'modzy_metadata_path' ] = modzy_metadata_path # this path is the local location that kaniko will store the image it creates path_to_tar_file = f ' { DATA_DIR } /kaniko_image- { random_name } .tar' logger . debug ( f 'Request data: { image_name } , { module_name } , { model_name } , { path_to_tar_file } ' ) error = run_kaniko ( image_name , module_name , model_name , path_to_tar_file , random_name , modzy_data , publish , registry_auth , gpu ) if error : return { 'error' : error , 'job_id' : None } return { 'error' : False , 'job_id' : f ' { K_JOB_NAME } - { random_name } ' }","title":"build_image()"},{"location":"service-reference/#service.app.create_job_object","text":"This utility method sets up all the required objects needed to create a model image and is run within the run_kaniko method. Parameters: Name Type Description Default image_name str container image name required module_name str reference module to locate location within service input is saved required model_name str name of model to package required path_to_tar_file str filepath destination to save docker image tar file required random_name str random id generated during build process that is used to ensure that all jobs are uniquely named and traceable required modzy_data str modzy_metadata_path returned from extract_modzy_metadata method required publish bool determines if image will be published to Docker registry required registry_auth dict Docker registry authorization credentials required gpu bool If True , will build container image that runs on GPU False Returns: Type Description Job Chassis job object Source code in service/app.py def create_job_object ( image_name , module_name , model_name , path_to_tar_file , random_name , modzy_data , publish , registry_auth , gpu = False ): ''' This utility method sets up all the required objects needed to create a model image and is run within the `run_kaniko` method. Args: image_name (str): container image name module_name (str): reference module to locate location within service input is saved model_name (str): name of model to package path_to_tar_file (str): filepath destination to save docker image tar file random_name (str): random id generated during build process that is used to ensure that all jobs are uniquely named and traceable modzy_data (str): modzy_metadata_path returned from `extract_modzy_metadata` method publish (bool): determines if image will be published to Docker registry registry_auth (dict): Docker registry authorization credentials gpu (bool): If `True`, will build container image that runs on GPU Returns: Job: Chassis job object ''' # job_name = f ' { K_JOB_NAME } - { random_name } ' # credential setup for Docker Hub. # json for holding registry credentials that will access docker hub. # reference: https://github.com/GoogleContainerTools/kaniko#pushing-to-docker-hub registry_credentials = f ' {{ \"auths\": {{ \"https://index.docker.io/v1/\": {{ \"auth\":\" { registry_auth } \" }}}}}} ' # mount path leads to /data # this is a mount point. NOT the volume itself. # name aligns with a volume defined below. data_volume_mount = client . V1VolumeMount ( mount_path = MOUNT_PATH_DIR , name = \"local-volume-code\" ) if CHASSIS_DEV else client . V1VolumeMount ( mount_path = MOUNT_PATH_DIR , name = K_DATA_VOLUME_NAME ) # This volume will be used by init container to populate registry credentials. # mount leads to /tmp/credentials # this is a mount point. NOT the volume itself. # name aligns with a volume defined below. init_empty_dir_volume_mount = client . V1VolumeMount ( mount_path = K_INIT_EMPTY_DIR_PATH , name = K_EMPTY_DIR_NAME ) # This volume will be used by kaniko container to get registry credentials. # mount path leads to /kaniko/.docker per kaniko reference documentation # this is a mount point. NOT the volume itself. # name aligns with a volume defined below. kaniko_empty_dir_volume_mount = client . V1VolumeMount ( mount_path = K_KANIKO_EMPTY_DIR_PATH , name = K_EMPTY_DIR_NAME ) # This container is used to populate registry credentials. # it only runs the single command in shell to echo our credentials into their proper file # per the reference documentation for Docker Hub # TODO: add credentials for Cloud Providers init_container = client . V1Container ( name = 'credentials' , image = 'busybox' , volume_mounts = [ init_empty_dir_volume_mount ], command = [ '/bin/sh' , '-c' , f 'echo \\' { registry_credentials } \\' > { K_INIT_EMPTY_DIR_PATH } /config.json' ] ) # This is the kaniko container used to build the final image. kaniko_args = [ f '--dockerfile= { DATA_DIR } /flavours/ { module_name } /Dockerfile { \".gpu\" if gpu else \"\" } ' , '' if publish else '--no-push' , f '--tarPath= { path_to_tar_file } ' , f '--destination= { image_name }{ \"\" if \":\" in image_name else \":latest\" } ' , f '--context= { DATA_DIR } ' , f '--build-arg=MODEL_DIR=model- { random_name } ' , f '--build-arg=MODZY_METADATA_PATH= { modzy_data . get ( \"modzy_metadata_path\" ) } ' , f '--build-arg=MODEL_NAME= { model_name } ' , f '--build-arg=MODEL_CLASS= { module_name } ' , # Modzy is the default interface. '--build-arg=INTERFACE=modzy' , ] init_container_kaniko = client . V1Container ( name = 'kaniko' , image = 'gcr.io/kaniko-project/executor:latest' , volume_mounts = [ data_volume_mount , kaniko_empty_dir_volume_mount ], args = kaniko_args ) modzy_uploader_container = client . V1Container ( name = 'modzy-uploader' , image = MODZY_UPLOADER_REPOSITORY , volume_mounts = [ data_volume_mount ], env = [ client . V1EnvVar ( name = 'JOB_NAME' , value = job_name ), client . V1EnvVar ( name = 'ENVIRONMENT' , value = ENVIRONMENT ) ], args = [ f '--api_key= { modzy_data . get ( \"api_key\" ) } ' , f '--deploy= { True if modzy_data . get ( \"deploy\" ) else \"\" } ' , f '--sample_input_path= { modzy_data . get ( \"modzy_sample_input_path\" ) } ' , f '--metadata_path= { DATA_DIR } / { modzy_data . get ( \"modzy_metadata_path\" ) } ' , f '--image_tag= { image_name }{ \"\" if \":\" in image_name else \":latest\" } ' , ] ) # volume claim data_pv_claim = client . V1PersistentVolumeClaimVolumeSource ( claim_name = \"dir-claim-chassis\" ) if CHASSIS_DEV else client . V1PersistentVolumeClaimVolumeSource ( claim_name = K_DATA_VOLUME_NAME ) # volume holding data data_volume = client . V1Volume ( name = \"local-volume-code\" , persistent_volume_claim = data_pv_claim ) if CHASSIS_DEV else client . V1Volume ( name = K_DATA_VOLUME_NAME , persistent_volume_claim = data_pv_claim ) # volume holding credentials empty_dir_volume = client . V1Volume ( name = K_EMPTY_DIR_NAME , empty_dir = client . V1EmptyDirVolumeSource () ) # Pod spec for the image build process pod_spec = client . V1PodSpec ( service_account_name = K_SERVICE_ACCOUNT_NAME , restart_policy = 'Never' , init_containers = [ init_container , init_container_kaniko ], containers = [ modzy_uploader_container ], volumes = [ data_volume , empty_dir_volume ] ) # setup and initiate model image build template = client . V1PodTemplateSpec ( metadata = client . V1ObjectMeta ( name = job_name ), spec = pod_spec ) spec = client . V1JobSpec ( backoff_limit = 0 , template = template ) job = client . V1Job ( api_version = 'batch/v1' , kind = 'Job' , metadata = client . V1ObjectMeta ( name = job_name , ), spec = spec ) return job","title":"create_job_object()"},{"location":"service-reference/#service.app.download_tar","text":"This method is run by the /job/{job_id}/download-tar endpoint. It downloads the container image from kaniko, built during the chassis job with the name job_id Parameters: Name Type Description Default job_id str valid Chassis job identifier, generated by create_job method required Returns: Type Description Dict response from download_tar endpoint Source code in service/app.py def download_tar ( job_id ): ''' This method is run by the `/job/{job_id}/download-tar` endpoint. It downloads the container image from kaniko, built during the chassis job with the name `job_id` Args: job_id (str): valid Chassis job identifier, generated by `create_job` method Returns: Dict: response from `download_tar` endpoint ''' uid = job_id . split ( f ' { K_JOB_NAME } -' )[ 1 ] return send_from_directory ( DATA_DIR , path = f 'kaniko_image- { uid } .tar' , as_attachment = False )","title":"download_tar()"},{"location":"service-reference/#service.app.get_job_status","text":"This method is run by the /job/{job_id} endpoint. Based on a GET request, it retrieves the status of the Kaniko job and the results if the job has completed. Parameters: Name Type Description Default job_id str valid Chassis job identifier, generated by create_job method required Returns: Type Description Dict Dictionary containing corresponding job data of job job_id Source code in service/app.py def get_job_status ( job_id ): ''' This method is run by the `/job/{job_id}` endpoint. Based on a GET request, it retrieves the status of the Kaniko job and the results if the job has completed. Args: job_id (str): valid Chassis job identifier, generated by `create_job` method Returns: Dict: Dictionary containing corresponding job data of job `job_id` ''' if CHASSIS_DEV : # if you are doing local dev you need to point at the local kubernetes cluster with your config file kubefile = os . getenv ( \"CHASSIS_KUBECONFIG\" ) config . load_kube_config ( kubefile ) else : # if the service is running inside a cluster during production then the config can be inherited config . load_incluster_config () batch_v1 = client . BatchV1Api () try : job = batch_v1 . read_namespaced_job ( job_id , ENVIRONMENT ) annotations = job . metadata . annotations or {} result = annotations . get ( 'result' ) result = json . loads ( result ) if result else None status = job . status job_data = { 'result' : result , 'status' : status . to_dict () } return job_data except ApiException as e : logger . error ( f 'Exception when getting job status: { e } ' ) return e . body","title":"get_job_status()"},{"location":"service-reference/#service.app.test_model","text":"This method is run by the /test endpoint. It creates a new conda environment from the provided conda.yaml file and then tests the provided model in that conda environment with provided test input file. Parameters: Name Type Description Default None None This method does not take any parameters required Returns: Type Description Dict model response to /test endpoint. Should contain either successful predictions or error message Source code in service/app.py def test_model (): ''' This method is run by the `/test` endpoint. It creates a new conda environment from the provided `conda.yaml` file and then tests the provided model in that conda environment with provided test input file. Args: None (None): This method does not take any parameters Returns: Dict: model response to `/test` endpoint. Should contain either successful predictions or error message ''' if not ( 'sample_input' in request . files and 'model' in request . files ): return 'Both sample input and model are required' , 500 output_dict = {} # retrieve binary representations for both variables model = request . files . get ( 'model' ) sample_input = request . files . get ( 'sample_input' ) # This is a future proofing variable in case we encounter a model that cannot be converted into mlflow. # It will remain hardcoded for now. module_name = 'mlflow' # This name is a random id used to ensure that all jobs are uniquely named and traceable. random_name = str ( uuid . uuid4 ()) # Unzip model archive unzipped_path = unzip_model ( model , module_name , random_name ) # get sample input path sample_input_path = extract_modzy_sample_input ( sample_input , module_name , random_name ) # create conda env, return error if fails try : tmp_env_name = str ( time . time ()) rm_env_cmd = \"conda env remove --name {} \" . format ( tmp_env_name ) yaml_path = os . path . join ( unzipped_path , \"conda.yaml\" ) create_env_cmd = \"conda env create -f {} -n {} \" . format ( yaml_path , tmp_env_name ) subprocess . run ( create_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' , check = True ) except subprocess . CalledProcessError as e : print ( e ) subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) output_dict [ \"env_error\" ] = e . stderr . decode () return output_dict # test model in env with sample input file, return error if fails try : test_model_cmd = \"\"\" source activate {} ; python test_chassis_model.py {} {} \"\"\" . format ( tmp_env_name , unzipped_path , sample_input_path ) test_ret = subprocess . run ( test_model_cmd , capture_output = True , shell = True , executable = '/bin/bash' , check = True ) output_dict [ \"model_output\" ] = test_ret . stdout . decode () except subprocess . CalledProcessError as e : subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) error_output = e . stderr . decode () output_dict [ \"model_error\" ] = error_output [ error_output . find ( 'in process' ):] return output_dict # if we make it here, test was successful, remove env and return output subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) return output_dict","title":"test_model()"},{"location":"tutorials/devops-deploy/","text":"Deploy Chassis (DevOps) Note If you just want to try Chassis, you can use the test drive, which will deploy it for you: Launch Test Drive Install required dependencies Install Docker Try to run docker ps If you get a permissions error, follow instructions here Install Kubectl Install Helm Install Minikube Start cluster: minikube start Add the Helm repository helm repo add chassis https://modzy.github.io/chassis After that we just need to update the Helm repos to fetch Chassis data. helm repo update Install Chassis service Now we just need to install Chassis as normal using Helm. helm install chassis chassis/chassis Check the installation After having installed the service we can check that the Chassis service is correctly deployed. kubectl get svc/chassis Then you should see an output similar to this. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE chassis NodePort 10 .106.209.207 <none> 5000 :30496/TCP 15s We can also check that the pod that runs the service is correctly running. kubectl get pods Where we should find our pod listed. NAME READY STATUS RESTARTS AGE ( ... ) chassis-5c574d459c-rclx9 1 /1 Running 0 22s ( ... ) Query the service To conclude, we may want to query the service just to see that it answers as we expect. To do that, we need to port forward the service. kubectl port-forward svc/chassis 5000 :5000 Now that we have access to the service we can query it. curl localhost:5000 Which should output an alive message.","title":"Deploy Chassis (DevOps)"},{"location":"tutorials/devops-deploy/#deploy-chassis-devops","text":"Note If you just want to try Chassis, you can use the test drive, which will deploy it for you: Launch Test Drive","title":"Deploy Chassis (DevOps)"},{"location":"tutorials/devops-deploy/#install-required-dependencies","text":"Install Docker Try to run docker ps If you get a permissions error, follow instructions here Install Kubectl Install Helm Install Minikube Start cluster: minikube start","title":"Install required dependencies"},{"location":"tutorials/devops-deploy/#add-the-helm-repository","text":"helm repo add chassis https://modzy.github.io/chassis After that we just need to update the Helm repos to fetch Chassis data. helm repo update","title":"Add the Helm repository"},{"location":"tutorials/devops-deploy/#install-chassis-service","text":"Now we just need to install Chassis as normal using Helm. helm install chassis chassis/chassis","title":"Install Chassis service"},{"location":"tutorials/devops-deploy/#check-the-installation","text":"After having installed the service we can check that the Chassis service is correctly deployed. kubectl get svc/chassis Then you should see an output similar to this. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE chassis NodePort 10 .106.209.207 <none> 5000 :30496/TCP 15s We can also check that the pod that runs the service is correctly running. kubectl get pods Where we should find our pod listed. NAME READY STATUS RESTARTS AGE ( ... ) chassis-5c574d459c-rclx9 1 /1 Running 0 22s ( ... )","title":"Check the installation"},{"location":"tutorials/devops-deploy/#query-the-service","text":"To conclude, we may want to query the service just to see that it answers as we expect. To do that, we need to port forward the service. kubectl port-forward svc/chassis 5000 :5000 Now that we have access to the service we can query it. curl localhost:5000 Which should output an alive message.","title":"Query the service"},{"location":"tutorials/ds-connect/","text":"Build a Model (Data Scientists) Note If you just want to try Chassis, you can use the test drive, which will deploy it for you so that you can go straight to building a MLflow model into a Docker container and pushing it to your Docker Hub account in the svc_demo.ipynb sample notebook: Launch Test Drive In order to connect to Chassis service we are going to use the SDK. We will transform our model into MLFlow format and we will upload it by making a request. After that, the image that have been created will be uploaded to Docker Hub and we will be able to use it. Please note that in addition to the tutorial on this page, there are example Jupyter notebooks available in the Chassis repo here . Instructions to run those notebooks are provided in the README in that repo subdirectory. Install the SDK First step is to install the SDK and additional packages required for this tutorial using pip . pip install chassisml scikit-learn mlflow joblib requests Build or import the model We can start from an existing model or create a new one. After that, we will need to transform it to MLFlow format so Chassis service will be able to manage it. Import required libraries Since we are going to train our own model as an example, we need to import all the libraries that we will need to do that. import chassisml import sklearn from joblib import dump , load Create the model Just as an example we are going to create and train a simple SKLearn model. from sklearn import datasets , svm from sklearn.model_selection import train_test_split digits = datasets . load_digits () data = digits . images . reshape (( len ( digits . images ), - 1 )) # Create a classifier: a support vector classifier clf = svm . SVC ( gamma = 0.001 ) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) # Learn the digits on the train subset clf . fit ( X_train , y_train ) Transform the model to Chassis format Once that we have our model we transform it to Chassis format. First we prepare the context dict, initializing anything here that should persist across inference runs. In this case, just the model: context = { \"model\" : clf } Notice that the SKLearn model that we created before is loaded into memory so that it will be packaged inside the MLFlow model. Next, we prepare the process function, which must take input file bytes and the context dict we prepared as input. It is responsible for preprocessing the bytes, running inference, and returning formatted results. It can leverage anything in the context dict to do so: def process ( input_bytes , context ): inputs = np . array ( json . loads ( input_bytes )) / 2 inference_results = context [ \"model\" ] . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results The process function can call other functions if needed. Next, we initialize our Chassis client, which we'll use to communicate with the Chassis service. Here, we assume our instance of Chassis is running locally on port 5000: chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) Now let's create a Chassis model with our context dict and process function, test it locally with a local input file, and then also test the creation of the environment and the execution of the model with a sample input file within that created environment (this will occur within the Chassis service): # create Chassis model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) # test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here): sample_filepath = './examples/modzy/input_sample.json' results = chassis_model . test ( sample_filepath ) print ( results ) # test environment and model within Chassis service, must pass filepath here: test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Build the image and publish to Modzy Now that we have our model in Chassis format we need to make a request against the Chassis service to build the Docker image that exposes it. You can optionally define your desired conda environment and pass it to publish() , but if you don't the dependencies will be automatically inferred for you. We'll let Chassis handle inferring dependencies in this case. We just need to provide a model name and semantic version, dockerhub credentials, and we can optionally provide a sample input file and Modzy API key if we'd like to publish the model to Modzy: response = chassis_model . publish ( model_name = \"Sklearn Digits\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , modzy_sample_input_path = sample_filepath , modzy_api_key = modzy_api_key ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) The block_until_complete call will terminate once the Chassis job completes. Pull the image Now that the process has completely finished we can pull and see our built image. The image name will be the model_name specified in the publish() call, but lowercase and with dashes instead of spaces. The image tag will be the model_version . docker pull <user>/sklearn-digits:0.0.1 docker images <user>/sklearn-digits:0.0.1 If everything has gone as expected we will see something similar to this. REPOSITORY TAG IMAGE ID CREATED SIZE <user>/sklearn-digits latest 0e5c5815f2ec 3 minutes ago 2 .19GB Run an inference job in Modzy If you provided the required arguments to publish() to publish the model to Modzy, you can use the Modzy SDK to submit an inference job to your newly-published model. from modzy import ApiClient client = ApiClient ( base_url = 'https://your.modzy.com/api' , api_key = modzy_api_key ) input_name = final_status [ 'result' ][ 'inputs' ][ 0 ][ 'name' ] model_id = final_status [ 'result' ] . get ( \"model\" ) . get ( \"modelId\" ) model_version = final_status [ 'result' ] . get ( \"version\" ) inference_job = client . jobs . submit_file ( model_id , model_version , { input_name : sample_filepath }) inference_job_result = client . results . block_until_complete ( inference_job , timeout = None ) inference_job_results_json = inference_job_result . get_first_outputs ()[ 'results.json' ] print ( inference_job_results_json )","title":"Build a Model (Data Scientists)"},{"location":"tutorials/ds-connect/#build-a-model-data-scientists","text":"Note If you just want to try Chassis, you can use the test drive, which will deploy it for you so that you can go straight to building a MLflow model into a Docker container and pushing it to your Docker Hub account in the svc_demo.ipynb sample notebook: Launch Test Drive In order to connect to Chassis service we are going to use the SDK. We will transform our model into MLFlow format and we will upload it by making a request. After that, the image that have been created will be uploaded to Docker Hub and we will be able to use it. Please note that in addition to the tutorial on this page, there are example Jupyter notebooks available in the Chassis repo here . Instructions to run those notebooks are provided in the README in that repo subdirectory.","title":"Build a Model (Data Scientists)"},{"location":"tutorials/ds-connect/#install-the-sdk","text":"First step is to install the SDK and additional packages required for this tutorial using pip . pip install chassisml scikit-learn mlflow joblib requests","title":"Install the SDK"},{"location":"tutorials/ds-connect/#build-or-import-the-model","text":"We can start from an existing model or create a new one. After that, we will need to transform it to MLFlow format so Chassis service will be able to manage it.","title":"Build or import the model"},{"location":"tutorials/ds-connect/#import-required-libraries","text":"Since we are going to train our own model as an example, we need to import all the libraries that we will need to do that. import chassisml import sklearn from joblib import dump , load","title":"Import required libraries"},{"location":"tutorials/ds-connect/#create-the-model","text":"Just as an example we are going to create and train a simple SKLearn model. from sklearn import datasets , svm from sklearn.model_selection import train_test_split digits = datasets . load_digits () data = digits . images . reshape (( len ( digits . images ), - 1 )) # Create a classifier: a support vector classifier clf = svm . SVC ( gamma = 0.001 ) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) # Learn the digits on the train subset clf . fit ( X_train , y_train )","title":"Create the model"},{"location":"tutorials/ds-connect/#transform-the-model-to-chassis-format","text":"Once that we have our model we transform it to Chassis format. First we prepare the context dict, initializing anything here that should persist across inference runs. In this case, just the model: context = { \"model\" : clf } Notice that the SKLearn model that we created before is loaded into memory so that it will be packaged inside the MLFlow model. Next, we prepare the process function, which must take input file bytes and the context dict we prepared as input. It is responsible for preprocessing the bytes, running inference, and returning formatted results. It can leverage anything in the context dict to do so: def process ( input_bytes , context ): inputs = np . array ( json . loads ( input_bytes )) / 2 inference_results = context [ \"model\" ] . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results The process function can call other functions if needed. Next, we initialize our Chassis client, which we'll use to communicate with the Chassis service. Here, we assume our instance of Chassis is running locally on port 5000: chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) Now let's create a Chassis model with our context dict and process function, test it locally with a local input file, and then also test the creation of the environment and the execution of the model with a sample input file within that created environment (this will occur within the Chassis service): # create Chassis model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) # test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here): sample_filepath = './examples/modzy/input_sample.json' results = chassis_model . test ( sample_filepath ) print ( results ) # test environment and model within Chassis service, must pass filepath here: test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result )","title":"Transform the model to Chassis format"},{"location":"tutorials/ds-connect/#build-the-image-and-publish-to-modzy","text":"Now that we have our model in Chassis format we need to make a request against the Chassis service to build the Docker image that exposes it. You can optionally define your desired conda environment and pass it to publish() , but if you don't the dependencies will be automatically inferred for you. We'll let Chassis handle inferring dependencies in this case. We just need to provide a model name and semantic version, dockerhub credentials, and we can optionally provide a sample input file and Modzy API key if we'd like to publish the model to Modzy: response = chassis_model . publish ( model_name = \"Sklearn Digits\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , modzy_sample_input_path = sample_filepath , modzy_api_key = modzy_api_key ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) The block_until_complete call will terminate once the Chassis job completes.","title":"Build the image and publish to Modzy"},{"location":"tutorials/ds-connect/#pull-the-image","text":"Now that the process has completely finished we can pull and see our built image. The image name will be the model_name specified in the publish() call, but lowercase and with dashes instead of spaces. The image tag will be the model_version . docker pull <user>/sklearn-digits:0.0.1 docker images <user>/sklearn-digits:0.0.1 If everything has gone as expected we will see something similar to this. REPOSITORY TAG IMAGE ID CREATED SIZE <user>/sklearn-digits latest 0e5c5815f2ec 3 minutes ago 2 .19GB","title":"Pull the image"},{"location":"tutorials/ds-connect/#run-an-inference-job-in-modzy","text":"If you provided the required arguments to publish() to publish the model to Modzy, you can use the Modzy SDK to submit an inference job to your newly-published model. from modzy import ApiClient client = ApiClient ( base_url = 'https://your.modzy.com/api' , api_key = modzy_api_key ) input_name = final_status [ 'result' ][ 'inputs' ][ 0 ][ 'name' ] model_id = final_status [ 'result' ] . get ( \"model\" ) . get ( \"modelId\" ) model_version = final_status [ 'result' ] . get ( \"version\" ) inference_job = client . jobs . submit_file ( model_id , model_version , { input_name : sample_filepath }) inference_job_result = client . results . block_until_complete ( inference_job , timeout = None ) inference_job_results_json = inference_job_result . get_first_outputs ()[ 'results.json' ] print ( inference_job_results_json )","title":"Run an inference job in Modzy"},{"location":"tutorials/ds-deploy-modzy/","text":"Deploy Model to Modzy Note If you just want to try Chassis, you can use the test drive, which will deploy Chassis and KFServing for you so you can use Chassis to containerize an MLflow model, push it to your Docker Hub account, and then publish it to the KFServing instance running inside the test drive, in the kfserving.ipynb sample notebook: Launch Test Drive Create the model The first steps we must follow here are the same ones that we can read in the Build a Model tutorial. We should follow exactly the same steps in case we don't have already our MLFlow model. In other case we can go directly to the step where we build the image. Basically we should: Install the SDK Import required libraries Create the model Transform the model to MLFlow Build the image Once we have finished the previous steps, we need to define not only the data that Chassis needs to build the image, but also the data that Modzy will need to deploy our model. So, we are going to assume that we have already generated our registry_auth for uploading the image to Docker Hub and we just have the data required for Chassis. image_data = { 'name' : '<user>/chassisml-sklearn-demo:latest' , 'model_name' : 'digits' , 'model_path' : './mlflow_custom_pyfunc_svm' , 'registry_auth' : 'XxXxXxXx' } Define Modzy data At this point, we are going to define the data that we need to deploy the model to Modzy. These are the fields that we need to take into account. metadata_path : this is the path to the model.yaml file that is needed to define all information about the model. Chassis has a default one, but you should define your own based on this example sample_input_path : this is the path to the sample input that is needed when deploying the model. An example that fits the model we built in this tutorial can be found here deploy : if it is True , then Chassis will manage to deploy the model into Modzy platform. Otherwise you can do this manually through the Modzy UI api_key : you should have your own api key from Modzy in order to let Chassis deploy the model for you modzy_data = { 'metadata_path' : './modzy/model.yaml' , 'sample_input_path' : './modzy/input_sample.json' , 'deploy' : True , 'api_key' : 'XxXxXxXx.XxXxXxXx' } Notice that if deploy is False this means that you can avoid defining the rest of the fields. Anyway, metadata_path should be defined in case you will eventually deploy the model to Modzy. This is important because the model will use this information when being deployed to Modzy, so it needs to be updated. Make the request Now that we have defined both the data for Chassis and the data for Modzy, we are able to make the request against Chassis service to build the image and deploy it to Modzy. Take into account that base_url should point to the address of the cluster where Chassis is running. In this case, that we want the built image to be deployed to Modzy, the cluster where Chassis is running needs to have access (VPN) to Modzy server. Otherwise, Modzy server will be unreachable by Chassis and it will not be possible to deploy the model. res = chassisml . publish ( image_data = image_data , modzy_data = modzy_data , upload = True , # ^ Needed because Chassis will deploy the model # to Modzy based on the Docker Hub uploaded image base_url = 'http://localhost:5000' ) error = res . get ( 'error' ) job_id = res . get ( 'job_id' ) if error : print ( 'Error:' , error ) else : print ( 'Job ID:' , job_id ) If everything has gone well we should see something similar to this. Publishing container... Ok! Job ID: chassis-builder-job-a3864869-a509-4658-986b-25cb4ddd604d Check the job status Notice that unlike in the KFServing tutorial , here we don't need to define the INTERFACE environment variable since Modzy is the default one. We must wait until it has finished, which may take several minutes. We can use the job id to check the status of the job. Once it has finished we should see something similar to this. { 'result' : { 'containerImage' : { 'containerImageSize' : 0 , ( ... ) 'uploadStatus' : 'IN_PROGRESS' }, 'container_url' : 'https://integration.modzy.engineering/models/<model_id>/<model_version>' , ( ... ) 'inputs' : [{ 'acceptedMediaTypes' : 'application/json' , 'description' : 'numpy array 2d' , 'maximumSize' : 1024000 , 'name' : 'input.json' }], 'isActive' : False , 'isAvailable' : True , 'longDescription' : 'It classifies digits.' , 'model' : { 'author' : 'Integration' , 'modelId' : '<model_id>' 'createdByEmail' : 'cmillan@sciling.com' , ( ... ) 'visibility' : { 'scope' : 'ALL' }}, 'outputs' : [{ 'description' : 'numpy 2d array' , 'maximumSize' : 1024000 , 'mediaType' : 'application/json' , 'name' : 'results.json' }], ( ... ) 'timeout' : { 'run' : 60000 , 'status' : 60000 }, 'version' : '0.1.0' }, 'status' : { 'active' : None , 'completion_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'conditions' : [{ 'last_probe_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'last_transition_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'message' : None , 'reason' : None , 'status' : 'True' , 'type' : 'Complete' }], 'failed' : None , 'start_time' : 'Thu, 05 Aug 2021 16:14:38 GMT' , 'succeeded' : 1 }} Two top keys can be seen: result : in case deploy was True this will contain some information related to the model deployed in Modzy. In particular we can see the full url to the model if we access the container_url key. status : this contains the information about the Kubernetes job that has built the image (and uploaded it to Modzy in case deploy was True as explained above) Query the model Now we are ready to query our model deployed in Modzy. Again, we will need VPN access to Modzy server to make a request against our model and to get the results. If we already have access, then we can use this json as a request to the model . Make the request This should be the contents of our request_body.json file. { \"model\" : { \"identifier\" : \"<model_id>\" , \"version\" : \"<model_version>\" }, \"input\" : { \"type\" : \"text\" , \"sources\" : { \"input\" : { \"<input_name>\" : \"<data_that_we_want_to_predict>\" } } } } Here there are some fields that we need to define. model_id : we can get this from the result obtained above model_version : we can get this from the result obtained above data_that_we_want_to_predict : this is the data to predict, that in our example can be the data sample So we can make the request like this. curl -X POST \\ -H 'Authorization: ApiKey <api_key>' \\ -H 'Content-Type: application/json' \\ -d@request_body.json \\ \"https://integration.modzy.engineering/api/jobs\" Which should output some information about the job that will run. Inside this information we can see the job_id , which we will use to get the results. { \"model\" : { \"identifier\" : \"<model_id>\" , \"version\" : \"<model_version>\" , (...) }, \"status\" : \"SUBMITTED\" , \"totalInputs\" : 1 , \"jobIdentifier\" : \"<job_identifier>\" , (...) \"team\" :{ (...) }, \"user\" :{ (...) }, \"jobInputs\" :{ \"identifier\" :[ \"input\" ] }, (...) } Get the results Once the job has finished, we can make a request agains Modzy again to see the results. curl -X GET \\ -H 'Authorization: ApiKey <api_key>' \\ \"https://integration.modzy.engineering/api/results/<job_id>\" And we will see the inference output. { \"jobIdentifier\" : \"<job_id>\" , \"total\" : 1 , \"completed\" : 1 , \"failed\" : 0 , \"finished\" : true , (...) \"results\" : { \"input\" : { \"status\" : \"SUCCESSFUL\" , (...) \"results.json\" : [{ \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"4\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"4\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }], \"voting\" : { (...) } } } }","title":"Deploy Model to Modzy"},{"location":"tutorials/ds-deploy-modzy/#deploy-model-to-modzy","text":"Note If you just want to try Chassis, you can use the test drive, which will deploy Chassis and KFServing for you so you can use Chassis to containerize an MLflow model, push it to your Docker Hub account, and then publish it to the KFServing instance running inside the test drive, in the kfserving.ipynb sample notebook: Launch Test Drive","title":"Deploy Model to Modzy"},{"location":"tutorials/ds-deploy-modzy/#create-the-model","text":"The first steps we must follow here are the same ones that we can read in the Build a Model tutorial. We should follow exactly the same steps in case we don't have already our MLFlow model. In other case we can go directly to the step where we build the image. Basically we should: Install the SDK Import required libraries Create the model Transform the model to MLFlow","title":"Create the model"},{"location":"tutorials/ds-deploy-modzy/#build-the-image","text":"Once we have finished the previous steps, we need to define not only the data that Chassis needs to build the image, but also the data that Modzy will need to deploy our model. So, we are going to assume that we have already generated our registry_auth for uploading the image to Docker Hub and we just have the data required for Chassis. image_data = { 'name' : '<user>/chassisml-sklearn-demo:latest' , 'model_name' : 'digits' , 'model_path' : './mlflow_custom_pyfunc_svm' , 'registry_auth' : 'XxXxXxXx' }","title":"Build the image"},{"location":"tutorials/ds-deploy-modzy/#define-modzy-data","text":"At this point, we are going to define the data that we need to deploy the model to Modzy. These are the fields that we need to take into account. metadata_path : this is the path to the model.yaml file that is needed to define all information about the model. Chassis has a default one, but you should define your own based on this example sample_input_path : this is the path to the sample input that is needed when deploying the model. An example that fits the model we built in this tutorial can be found here deploy : if it is True , then Chassis will manage to deploy the model into Modzy platform. Otherwise you can do this manually through the Modzy UI api_key : you should have your own api key from Modzy in order to let Chassis deploy the model for you modzy_data = { 'metadata_path' : './modzy/model.yaml' , 'sample_input_path' : './modzy/input_sample.json' , 'deploy' : True , 'api_key' : 'XxXxXxXx.XxXxXxXx' } Notice that if deploy is False this means that you can avoid defining the rest of the fields. Anyway, metadata_path should be defined in case you will eventually deploy the model to Modzy. This is important because the model will use this information when being deployed to Modzy, so it needs to be updated.","title":"Define Modzy data"},{"location":"tutorials/ds-deploy-modzy/#make-the-request","text":"Now that we have defined both the data for Chassis and the data for Modzy, we are able to make the request against Chassis service to build the image and deploy it to Modzy. Take into account that base_url should point to the address of the cluster where Chassis is running. In this case, that we want the built image to be deployed to Modzy, the cluster where Chassis is running needs to have access (VPN) to Modzy server. Otherwise, Modzy server will be unreachable by Chassis and it will not be possible to deploy the model. res = chassisml . publish ( image_data = image_data , modzy_data = modzy_data , upload = True , # ^ Needed because Chassis will deploy the model # to Modzy based on the Docker Hub uploaded image base_url = 'http://localhost:5000' ) error = res . get ( 'error' ) job_id = res . get ( 'job_id' ) if error : print ( 'Error:' , error ) else : print ( 'Job ID:' , job_id ) If everything has gone well we should see something similar to this. Publishing container... Ok! Job ID: chassis-builder-job-a3864869-a509-4658-986b-25cb4ddd604d","title":"Make the request"},{"location":"tutorials/ds-deploy-modzy/#check-the-job-status","text":"Notice that unlike in the KFServing tutorial , here we don't need to define the INTERFACE environment variable since Modzy is the default one. We must wait until it has finished, which may take several minutes. We can use the job id to check the status of the job. Once it has finished we should see something similar to this. { 'result' : { 'containerImage' : { 'containerImageSize' : 0 , ( ... ) 'uploadStatus' : 'IN_PROGRESS' }, 'container_url' : 'https://integration.modzy.engineering/models/<model_id>/<model_version>' , ( ... ) 'inputs' : [{ 'acceptedMediaTypes' : 'application/json' , 'description' : 'numpy array 2d' , 'maximumSize' : 1024000 , 'name' : 'input.json' }], 'isActive' : False , 'isAvailable' : True , 'longDescription' : 'It classifies digits.' , 'model' : { 'author' : 'Integration' , 'modelId' : '<model_id>' 'createdByEmail' : 'cmillan@sciling.com' , ( ... ) 'visibility' : { 'scope' : 'ALL' }}, 'outputs' : [{ 'description' : 'numpy 2d array' , 'maximumSize' : 1024000 , 'mediaType' : 'application/json' , 'name' : 'results.json' }], ( ... ) 'timeout' : { 'run' : 60000 , 'status' : 60000 }, 'version' : '0.1.0' }, 'status' : { 'active' : None , 'completion_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'conditions' : [{ 'last_probe_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'last_transition_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'message' : None , 'reason' : None , 'status' : 'True' , 'type' : 'Complete' }], 'failed' : None , 'start_time' : 'Thu, 05 Aug 2021 16:14:38 GMT' , 'succeeded' : 1 }} Two top keys can be seen: result : in case deploy was True this will contain some information related to the model deployed in Modzy. In particular we can see the full url to the model if we access the container_url key. status : this contains the information about the Kubernetes job that has built the image (and uploaded it to Modzy in case deploy was True as explained above)","title":"Check the job status"},{"location":"tutorials/ds-deploy-modzy/#query-the-model","text":"Now we are ready to query our model deployed in Modzy. Again, we will need VPN access to Modzy server to make a request against our model and to get the results. If we already have access, then we can use this json as a request to the model .","title":"Query the model"},{"location":"tutorials/ds-deploy-modzy/#make-the-request_1","text":"This should be the contents of our request_body.json file. { \"model\" : { \"identifier\" : \"<model_id>\" , \"version\" : \"<model_version>\" }, \"input\" : { \"type\" : \"text\" , \"sources\" : { \"input\" : { \"<input_name>\" : \"<data_that_we_want_to_predict>\" } } } } Here there are some fields that we need to define. model_id : we can get this from the result obtained above model_version : we can get this from the result obtained above data_that_we_want_to_predict : this is the data to predict, that in our example can be the data sample So we can make the request like this. curl -X POST \\ -H 'Authorization: ApiKey <api_key>' \\ -H 'Content-Type: application/json' \\ -d@request_body.json \\ \"https://integration.modzy.engineering/api/jobs\" Which should output some information about the job that will run. Inside this information we can see the job_id , which we will use to get the results. { \"model\" : { \"identifier\" : \"<model_id>\" , \"version\" : \"<model_version>\" , (...) }, \"status\" : \"SUBMITTED\" , \"totalInputs\" : 1 , \"jobIdentifier\" : \"<job_identifier>\" , (...) \"team\" :{ (...) }, \"user\" :{ (...) }, \"jobInputs\" :{ \"identifier\" :[ \"input\" ] }, (...) }","title":"Make the request"},{"location":"tutorials/ds-deploy-modzy/#get-the-results","text":"Once the job has finished, we can make a request agains Modzy again to see the results. curl -X GET \\ -H 'Authorization: ApiKey <api_key>' \\ \"https://integration.modzy.engineering/api/results/<job_id>\" And we will see the inference output. { \"jobIdentifier\" : \"<job_id>\" , \"total\" : 1 , \"completed\" : 1 , \"failed\" : 0 , \"finished\" : true , (...) \"results\" : { \"input\" : { \"status\" : \"SUCCESSFUL\" , (...) \"results.json\" : [{ \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"4\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"4\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }], \"voting\" : { (...) } } } }","title":"Get the results"},{"location":"tutorials/ds-deploy/","text":"Deploy Model to KFServing Note If you just want to try Chassis, you can use the test drive, which will deploy Chassis and KFServing for you so you can use Chassis to containerize an MLflow model, push it to your Docker Hub account, and then publish it to the KFServing instance running inside the test drive, in the kfserving.ipynb sample notebook: Launch Test Drive Install KFServing in minikube You just need to clone the KFServing repository and run the quick_install.sh script. # Install KFServing. Versions above 0.5.1 doesn't work, so force it. git clone --single-branch --branch v0.5.1 git@github.com:kubeflow/kfserving.git ./kfserving/hack/quick_install.sh Required variables There are some environment variables that must be defined for KFServing to work: INTERFACE: kfserving HTTP_PORT: port where kfserving will be running PROTOCOL: it can be v1 or v2 MODEL_NAME: a name for the model must be defined Deploy the model Assuming the image generated by ChassisML has been uploaded to a repository called carmilso/chassisml-sklearn-demo:latest , just deploy the file that defines the InferenceService for the protocol v1 of KFServing apiVersion : \"serving.kubeflow.org/v1beta1\" kind : \"InferenceService\" metadata : name : chassisml-sklearn-demo spec : predictor : containers : - image : carmilso/chassisml-sklearn-demo:latest name : chassisml-sklearn-demo-container env : - name : INTERFACE value : kfserving - name : HTTP_PORT value : \"8080\" - name : PROTOCOL value : v1 - name : MODEL_NAME value : digits ports : - containerPort : 8080 protocol : TCP In this case, the variable MODEL_NAME should not be necessary since it's defined when creating the image. kubectl apply -f custom_v1.yaml This should output a success message. Define required variables to query the pod This is needed in order to be able to communicate with the deployed image. The SERVICE_NAME must match the name defined in the metadata.name of the InferenceService created above. The MODEL_NAME must match the name of your model. It can be defined by the data scientist when making the request against Chassis service or overwritten in the InferenceService as defined above. export INGRESS_HOST = $( minikube ip ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) export SERVICE_NAME = chassisml-sklearn-demo export MODEL_NAME = digits export SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ SERVICE_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) Query the model Please note that you must base64 encode each input instance. For example: import json import base64 as b64 instances = [[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ]] input_dict = { 'instances' : [ b64 . b64encode ( str ( entry ) . encode ()) . decode () for entry in instances ]} json . dump ( input_dict , open ( 'kserve_input.json' , 'w' )) Now you can just make a request to predict some data. Take into account that you must download inputsv1.json before making the request. curl -H \"Host: ${ SERVICE_HOSTNAME } \" \"http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict\" -d@inputsv1.json | jq The output should be similar to this: { \"predictions\" : [ { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } } ] } In this case, the data was prepared for the protocol v1, but we can deploy the image using the protocol v2 and make the request using the data for v2 . Deploy the model locally The model can also be deployed locally: docker run --rm -p 8080 :8080 \\ -e INTERFACE = kfserving \\ -e HTTP_PORT = 8080 \\ -e PROTOCOL = v2 \\ -e MODEL_NAME = digits \\ carmilso/chassisml-sklearn-demo:latest So we can query it this way. Take into account that you must download inputsv2.json before making the request: curl localhost:8080/v2/models/digits/infer -d@inputsv2.json","title":"Deploy Model to KFServing"},{"location":"tutorials/ds-deploy/#deploy-model-to-kfserving","text":"Note If you just want to try Chassis, you can use the test drive, which will deploy Chassis and KFServing for you so you can use Chassis to containerize an MLflow model, push it to your Docker Hub account, and then publish it to the KFServing instance running inside the test drive, in the kfserving.ipynb sample notebook: Launch Test Drive","title":"Deploy Model to KFServing"},{"location":"tutorials/ds-deploy/#install-kfserving-in-minikube","text":"You just need to clone the KFServing repository and run the quick_install.sh script. # Install KFServing. Versions above 0.5.1 doesn't work, so force it. git clone --single-branch --branch v0.5.1 git@github.com:kubeflow/kfserving.git ./kfserving/hack/quick_install.sh","title":"Install KFServing in minikube"},{"location":"tutorials/ds-deploy/#required-variables","text":"There are some environment variables that must be defined for KFServing to work: INTERFACE: kfserving HTTP_PORT: port where kfserving will be running PROTOCOL: it can be v1 or v2 MODEL_NAME: a name for the model must be defined","title":"Required variables"},{"location":"tutorials/ds-deploy/#deploy-the-model","text":"Assuming the image generated by ChassisML has been uploaded to a repository called carmilso/chassisml-sklearn-demo:latest , just deploy the file that defines the InferenceService for the protocol v1 of KFServing apiVersion : \"serving.kubeflow.org/v1beta1\" kind : \"InferenceService\" metadata : name : chassisml-sklearn-demo spec : predictor : containers : - image : carmilso/chassisml-sklearn-demo:latest name : chassisml-sklearn-demo-container env : - name : INTERFACE value : kfserving - name : HTTP_PORT value : \"8080\" - name : PROTOCOL value : v1 - name : MODEL_NAME value : digits ports : - containerPort : 8080 protocol : TCP In this case, the variable MODEL_NAME should not be necessary since it's defined when creating the image. kubectl apply -f custom_v1.yaml This should output a success message.","title":"Deploy the model"},{"location":"tutorials/ds-deploy/#define-required-variables-to-query-the-pod","text":"This is needed in order to be able to communicate with the deployed image. The SERVICE_NAME must match the name defined in the metadata.name of the InferenceService created above. The MODEL_NAME must match the name of your model. It can be defined by the data scientist when making the request against Chassis service or overwritten in the InferenceService as defined above. export INGRESS_HOST = $( minikube ip ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) export SERVICE_NAME = chassisml-sklearn-demo export MODEL_NAME = digits export SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ SERVICE_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 )","title":"Define required variables to query the pod"},{"location":"tutorials/ds-deploy/#query-the-model","text":"Please note that you must base64 encode each input instance. For example: import json import base64 as b64 instances = [[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ]] input_dict = { 'instances' : [ b64 . b64encode ( str ( entry ) . encode ()) . decode () for entry in instances ]} json . dump ( input_dict , open ( 'kserve_input.json' , 'w' )) Now you can just make a request to predict some data. Take into account that you must download inputsv1.json before making the request. curl -H \"Host: ${ SERVICE_HOSTNAME } \" \"http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict\" -d@inputsv1.json | jq The output should be similar to this: { \"predictions\" : [ { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } } ] } In this case, the data was prepared for the protocol v1, but we can deploy the image using the protocol v2 and make the request using the data for v2 .","title":"Query the model"},{"location":"tutorials/ds-deploy/#deploy-the-model-locally","text":"The model can also be deployed locally: docker run --rm -p 8080 :8080 \\ -e INTERFACE = kfserving \\ -e HTTP_PORT = 8080 \\ -e PROTOCOL = v2 \\ -e MODEL_NAME = digits \\ carmilso/chassisml-sklearn-demo:latest So we can query it this way. Take into account that you must download inputsv2.json before making the request: curl localhost:8080/v2/models/digits/infer -d@inputsv2.json","title":"Deploy the model locally"}]}