{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Model Containerizer for K8s Build models directly into DevOps-ready container images for inference (using MLflow under the hood) Supports parallel builds in Kubernetes jobs, using Kaniko , no Docker socket required! Generates Open Model Interface compatible images that are multi-purpose and portable, they work on multiple platforms: KFServing and Modzy Try the test drive today, then deploy our Helm chart to your K8s cluster to use it for real Test Drive The fastest way to get started is to use the test drive functionality provided by Testfaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive Talk & Demo .video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; } Getting Started Follow one of our tutorials to easily get started and see how Chassis works: Tech Support Our tech support is graciously hosted on Modzy's Discord server in the \"Open Source\" channels Install with Helm into a Kubernetes cluster Build a container image from your model Deploy to KFServing the built image Shameless plug Chassis is developed by Modzy , a commercial ModelOps platform designed to run any kind of machine learning and artifical intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of Chassis grew out of Modzy's internal research and development to provide a way to easily containerize MLflow models to publish into the Modzy catalog and to support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense. Contributors A full list of contributors can be found here .","title":"Model Containerizer for K8s"},{"location":"#model-containerizer-for-k8s","text":"Build models directly into DevOps-ready container images for inference (using MLflow under the hood) Supports parallel builds in Kubernetes jobs, using Kaniko , no Docker socket required! Generates Open Model Interface compatible images that are multi-purpose and portable, they work on multiple platforms: KFServing and Modzy Try the test drive today, then deploy our Helm chart to your K8s cluster to use it for real","title":"Model Containerizer for K8s"},{"location":"#test-drive","text":"The fastest way to get started is to use the test drive functionality provided by Testfaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive","title":"Test Drive"},{"location":"#talk-demo","text":".video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Talk &amp; Demo"},{"location":"#getting-started","text":"Follow one of our tutorials to easily get started and see how Chassis works: Tech Support Our tech support is graciously hosted on Modzy's Discord server in the \"Open Source\" channels Install with Helm into a Kubernetes cluster Build a container image from your model Deploy to KFServing the built image","title":"Getting Started"},{"location":"#shameless-plug","text":"Chassis is developed by Modzy , a commercial ModelOps platform designed to run any kind of machine learning and artifical intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of Chassis grew out of Modzy's internal research and development to provide a way to easily containerize MLflow models to publish into the Modzy catalog and to support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense.","title":"Shameless plug"},{"location":"#contributors","text":"A full list of contributors can be found here .","title":"Contributors"},{"location":"design/","text":"Design & Architecture Chassis makes it easy to create a deployable docker image from your trained ML model. The idea behind this project is to provide Data Scientists with a way to package their models into a Docker image. This image will manage to build the inference service compatible with several common platforms for free. At the moment, Chassis images are compatible with KFServing and Modzy gRPC. This means you can deploy your built image into these platforms once it has been built. Deploy Chassis, send your model to it and start using the built container image to run inference on your data. Architecture This diagram shows the overall architecture of the Chassis system: This diagram zooms in on the generated container, showing the pluggable interface, configurable at runtime: Goals Chassis is a Kubernetes service that can be deployed in your preferred cluster using Helm. It works by creating jobs that can be run in parallel to create Docker images that package ML models. It provides integration with most common deployment platforms so your model will be ready to be deployed in a simple way. It also provides a python SDK that makes it very easy to communicate with Chassis service in order to build your image. Simple Just make a request to build your image using the python SDK Small set of dependencies: mlflow, flask Supports multiple deployment platforms No DevOps knowledge needed Fast Start building the image as soon as you make the request Automatically upload the image to Docker Hub Image ready to be deployed Secure Using Kaniko to securely build the image Non-goals Some non-goals of this project are: Deploy the built image - this is up to the platform that you deploy the container into, like KFServing or Modzy","title":"Design & Architecture"},{"location":"design/#design-architecture","text":"Chassis makes it easy to create a deployable docker image from your trained ML model. The idea behind this project is to provide Data Scientists with a way to package their models into a Docker image. This image will manage to build the inference service compatible with several common platforms for free. At the moment, Chassis images are compatible with KFServing and Modzy gRPC. This means you can deploy your built image into these platforms once it has been built. Deploy Chassis, send your model to it and start using the built container image to run inference on your data.","title":"Design &amp; Architecture"},{"location":"design/#architecture","text":"This diagram shows the overall architecture of the Chassis system: This diagram zooms in on the generated container, showing the pluggable interface, configurable at runtime:","title":"Architecture"},{"location":"design/#goals","text":"Chassis is a Kubernetes service that can be deployed in your preferred cluster using Helm. It works by creating jobs that can be run in parallel to create Docker images that package ML models. It provides integration with most common deployment platforms so your model will be ready to be deployed in a simple way. It also provides a python SDK that makes it very easy to communicate with Chassis service in order to build your image. Simple Just make a request to build your image using the python SDK Small set of dependencies: mlflow, flask Supports multiple deployment platforms No DevOps knowledge needed Fast Start building the image as soon as you make the request Automatically upload the image to Docker Hub Image ready to be deployed Secure Using Kaniko to securely build the image","title":"Goals"},{"location":"design/#non-goals","text":"Some non-goals of this project are: Deploy the built image - this is up to the platform that you deploy the container into, like KFServing or Modzy","title":"Non-goals"},{"location":"faqs/","text":"FAQs Why the name Chassis? If your model is the engine, we provide a chassis for your model to drive!","title":"FAQs"},{"location":"faqs/#faqs","text":"","title":"FAQs"},{"location":"faqs/#why-the-name-chassis","text":"If your model is the engine, we provide a chassis for your model to drive!","title":"Why the name Chassis?"},{"location":"get-involved/","text":"Get Involved Feel free to fork and open pull requests to contribute to modzy/chassis . Join the #chassisml channel on Modzy's Discord Server where our maintainers meet to plan changes and improvements. We also have a #chassis-model-builder Slack channel on the MLOps.community Slack !","title":"Get Involved"},{"location":"get-involved/#get-involved","text":"Feel free to fork and open pull requests to contribute to modzy/chassis . Join the #chassisml channel on Modzy's Discord Server where our maintainers meet to plan changes and improvements. We also have a #chassis-model-builder Slack channel on the MLOps.community Slack !","title":"Get Involved"},{"location":"help/","text":"Help Join our #chassis-model-builder Slack channel on the MLOps.community Slack and we'll gladly try to help you! Use of Cookies We use Google Analytics cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for.","title":"Help"},{"location":"help/#help","text":"Join our #chassis-model-builder Slack channel on the MLOps.community Slack and we'll gladly try to help you!","title":"Help"},{"location":"help/#use-of-cookies","text":"We use Google Analytics cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for.","title":"Use of Cookies"},{"location":"release-notes/","text":"Release Notes See also: PyPI . v0.0.1 First working release of Chassis.","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"See also: PyPI .","title":"Release Notes"},{"location":"release-notes/#v001","text":"First working release of Chassis.","title":"v0.0.1"},{"location":"tutorials/devops-deploy/","text":"Deploy Chassis (DevOps) Note If you just want to try Chassis, you can use the test drive, which will deploy it for you: Launch Test Drive Install required dependencies Install Docker Try to run docker ps If you get a permissions error, follow instructions here Install Kubectl Install Helm Install Minikube Start cluster: minikube start Add the Helm repository helm repo add chassis https://modzy.github.io/chassis After that we just need to update the Helm repos to fetch Chassis data. helm repo update Install Chassis service Now we just need to install Chassis as normal using Helm. helm install chassis chassis/chassis Check the installation After having installed the service we can check that the Chassis service is correctly deployed. kubectl get svc/chassis Then you should see an output similar to this. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE chassis NodePort 10 .106.209.207 <none> 5000 :30496/TCP 15s We can also check that the pod that runs the service is correctly running. kubectl get pods Where we should find our pod listed. NAME READY STATUS RESTARTS AGE ( ... ) chassis-5c574d459c-rclx9 1 /1 Running 0 22s ( ... ) Query the service To conclude, we may want to query the service just to see that it answers as we expect. To do that, we need to port forward the service. kubectl port-forward svc/chassis 5000 :5000 Now that we have access to the service we can query it. curl localhost:5000 Which should output an alive message.","title":"Deploy Chassis (DevOps)"},{"location":"tutorials/devops-deploy/#deploy-chassis-devops","text":"Note If you just want to try Chassis, you can use the test drive, which will deploy it for you: Launch Test Drive","title":"Deploy Chassis (DevOps)"},{"location":"tutorials/devops-deploy/#install-required-dependencies","text":"Install Docker Try to run docker ps If you get a permissions error, follow instructions here Install Kubectl Install Helm Install Minikube Start cluster: minikube start","title":"Install required dependencies"},{"location":"tutorials/devops-deploy/#add-the-helm-repository","text":"helm repo add chassis https://modzy.github.io/chassis After that we just need to update the Helm repos to fetch Chassis data. helm repo update","title":"Add the Helm repository"},{"location":"tutorials/devops-deploy/#install-chassis-service","text":"Now we just need to install Chassis as normal using Helm. helm install chassis chassis/chassis","title":"Install Chassis service"},{"location":"tutorials/devops-deploy/#check-the-installation","text":"After having installed the service we can check that the Chassis service is correctly deployed. kubectl get svc/chassis Then you should see an output similar to this. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE chassis NodePort 10 .106.209.207 <none> 5000 :30496/TCP 15s We can also check that the pod that runs the service is correctly running. kubectl get pods Where we should find our pod listed. NAME READY STATUS RESTARTS AGE ( ... ) chassis-5c574d459c-rclx9 1 /1 Running 0 22s ( ... )","title":"Check the installation"},{"location":"tutorials/devops-deploy/#query-the-service","text":"To conclude, we may want to query the service just to see that it answers as we expect. To do that, we need to port forward the service. kubectl port-forward svc/chassis 5000 :5000 Now that we have access to the service we can query it. curl localhost:5000 Which should output an alive message.","title":"Query the service"},{"location":"tutorials/ds-connect/","text":"Build a Model (Data Scientists) Note If you just want to try Chassis, you can use the test drive, which will deploy it for you so that you can go straight to building a MLflow model into a Docker container and pushing it to your Docker Hub account in the svc_demo.ipynb sample notebook: Launch Test Drive In order to connect to Chassis service we are going to use the SDK. We will transform our model into MLFlow format and we will upload it by making a request. After that, the image that have been created will be uploaded to Docker Hub and we will be able to use it. Please note that in addition to the tutorial on this page, there are example Jupyter notebooks available in the Chassis repo here . Instructions to run those notebooks are provided in the README in that repo subdirectory. Install the SDK First step is to install the SDK and additional packages required for this tutorial using pip . pip install chassisml scikit-learn mlflow joblib requests Build or import the model We can start from an existing model or create a new one. After that, we will need to transform it to MLFlow format so Chassis service will be able to manage it. Import required libraries Since we are going to train our own model as an example, we need to import all the libraries that we will need to do that. import chassisml import sklearn from joblib import dump , load Create the model Just as an example we are going to create and train a simple SKLearn model. from sklearn import datasets , svm from sklearn.model_selection import train_test_split digits = datasets . load_digits () data = digits . images . reshape (( len ( digits . images ), - 1 )) # Create a classifier: a support vector classifier clf = svm . SVC ( gamma = 0.001 ) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) # Learn the digits on the train subset clf . fit ( X_train , y_train ) Transform the model to Chassis format Once that we have our model we transform it to Chassis format. First we prepare the context dict, initializing anything here that should persist across inference runs. In this case, just the model: context = { \"model\" : clf } Notice that the SKLearn model that we created before is loaded into memory so that it will be packaged inside the MLFlow model. Next, we prepare the process function, which must take input file bytes and the context dict we prepared as input. It is responsible for preprocessing the bytes, running inference, and returning formatted results. It can leverage anything in the context dict to do so: def process ( input_bytes , context ): inputs = np . array ( json . loads ( input_bytes )) / 2 inference_results = context [ \"model\" ] . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results The process function can call other functions if needed. Next, we initialize our Chassis client, which we'll use to communicate with the Chassis service. Here, we assume our instance of Chassis is running locally on port 5000: chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) Now let's create a Chassis model with our context dict and process function, test it locally with a local input file, and then also test the creation of the environment and the execution of the model with a sample input file within that created environment (this will occur within the Chassis service): # create Chassis model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) # test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here): sample_filepath = './examples/modzy/input_sample.json' results = chassis_model . test ( sample_filepath ) print ( results ) # test environment and model within Chassis service, must pass filepath here: test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Build the image and publish to Modzy Now that we have our model in Chassis format we need to make a request against the Chassis service to build the Docker image that exposes it. You can optionally define your desired conda environment and pass it to publish() , but if you don't the dependencies will be automatically inferred for you. We'll let Chassis handle inferring dependencies in this case. We just need to provide a model name and semantic version, dockerhub credentials, and we can optionally provide a sample input file and Modzy API key if we'd like to publish the model to Modzy: response = chassis_model . publish ( model_name = \"Sklearn Digits\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , modzy_sample_input_path = sample_filepath , modzy_api_key = modzy_api_key ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) The block_until_complete call will terminate once the Chassis job completes. Pull the image Now that the process has completely finished we can pull and see our built image. The image name will be the model_name specified in the publish() call, but lowercase and with dashes instead of spaces. The image tag will be the model_version . docker pull <user>/sklearn-digits:0.0.1 docker images <user>/sklearn-digits:0.0.1 If everything has gone as expected we will see something similar to this. REPOSITORY TAG IMAGE ID CREATED SIZE <user>/sklearn-digits latest 0e5c5815f2ec 3 minutes ago 2 .19GB Run an inference job in Modzy If you provided the required arguments to publish() to publish the model to Modzy, you can use the Modzy SDK to submit an inference job to your newly-published model. from modzy import ApiClient client = ApiClient ( base_url = 'https://your.modzy.com/api' , api_key = modzy_api_key ) input_name = final_status [ 'result' ][ 'inputs' ][ 0 ][ 'name' ] model_id = final_status [ 'result' ] . get ( \"model\" ) . get ( \"modelId\" ) model_version = final_status [ 'result' ] . get ( \"version\" ) inference_job = client . jobs . submit_file ( model_id , model_version , { input_name : sample_filepath }) inference_job_result = client . results . block_until_complete ( inference_job , timeout = None ) inference_job_results_json = inference_job_result . get_first_outputs ()[ 'results.json' ] print ( inference_job_results_json )","title":"Build a Model (Data Scientists)"},{"location":"tutorials/ds-connect/#build-a-model-data-scientists","text":"Note If you just want to try Chassis, you can use the test drive, which will deploy it for you so that you can go straight to building a MLflow model into a Docker container and pushing it to your Docker Hub account in the svc_demo.ipynb sample notebook: Launch Test Drive In order to connect to Chassis service we are going to use the SDK. We will transform our model into MLFlow format and we will upload it by making a request. After that, the image that have been created will be uploaded to Docker Hub and we will be able to use it. Please note that in addition to the tutorial on this page, there are example Jupyter notebooks available in the Chassis repo here . Instructions to run those notebooks are provided in the README in that repo subdirectory.","title":"Build a Model (Data Scientists)"},{"location":"tutorials/ds-connect/#install-the-sdk","text":"First step is to install the SDK and additional packages required for this tutorial using pip . pip install chassisml scikit-learn mlflow joblib requests","title":"Install the SDK"},{"location":"tutorials/ds-connect/#build-or-import-the-model","text":"We can start from an existing model or create a new one. After that, we will need to transform it to MLFlow format so Chassis service will be able to manage it.","title":"Build or import the model"},{"location":"tutorials/ds-connect/#import-required-libraries","text":"Since we are going to train our own model as an example, we need to import all the libraries that we will need to do that. import chassisml import sklearn from joblib import dump , load","title":"Import required libraries"},{"location":"tutorials/ds-connect/#create-the-model","text":"Just as an example we are going to create and train a simple SKLearn model. from sklearn import datasets , svm from sklearn.model_selection import train_test_split digits = datasets . load_digits () data = digits . images . reshape (( len ( digits . images ), - 1 )) # Create a classifier: a support vector classifier clf = svm . SVC ( gamma = 0.001 ) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) # Learn the digits on the train subset clf . fit ( X_train , y_train )","title":"Create the model"},{"location":"tutorials/ds-connect/#transform-the-model-to-chassis-format","text":"Once that we have our model we transform it to Chassis format. First we prepare the context dict, initializing anything here that should persist across inference runs. In this case, just the model: context = { \"model\" : clf } Notice that the SKLearn model that we created before is loaded into memory so that it will be packaged inside the MLFlow model. Next, we prepare the process function, which must take input file bytes and the context dict we prepared as input. It is responsible for preprocessing the bytes, running inference, and returning formatted results. It can leverage anything in the context dict to do so: def process ( input_bytes , context ): inputs = np . array ( json . loads ( input_bytes )) / 2 inference_results = context [ \"model\" ] . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results The process function can call other functions if needed. Next, we initialize our Chassis client, which we'll use to communicate with the Chassis service. Here, we assume our instance of Chassis is running locally on port 5000: chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) Now let's create a Chassis model with our context dict and process function, test it locally with a local input file, and then also test the creation of the environment and the execution of the model with a sample input file within that created environment (this will occur within the Chassis service): # create Chassis model chassis_model = chassis_client . create_model ( context = context , process_fn = process ) # test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here): sample_filepath = './examples/modzy/input_sample.json' results = chassis_model . test ( sample_filepath ) print ( results ) # test environment and model within Chassis service, must pass filepath here: test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result )","title":"Transform the model to Chassis format"},{"location":"tutorials/ds-connect/#build-the-image-and-publish-to-modzy","text":"Now that we have our model in Chassis format we need to make a request against the Chassis service to build the Docker image that exposes it. You can optionally define your desired conda environment and pass it to publish() , but if you don't the dependencies will be automatically inferred for you. We'll let Chassis handle inferring dependencies in this case. We just need to provide a model name and semantic version, dockerhub credentials, and we can optionally provide a sample input file and Modzy API key if we'd like to publish the model to Modzy: response = chassis_model . publish ( model_name = \"Sklearn Digits\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , modzy_sample_input_path = sample_filepath , modzy_api_key = modzy_api_key ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) The block_until_complete call will terminate once the Chassis job completes.","title":"Build the image and publish to Modzy"},{"location":"tutorials/ds-connect/#pull-the-image","text":"Now that the process has completely finished we can pull and see our built image. The image name will be the model_name specified in the publish() call, but lowercase and with dashes instead of spaces. The image tag will be the model_version . docker pull <user>/sklearn-digits:0.0.1 docker images <user>/sklearn-digits:0.0.1 If everything has gone as expected we will see something similar to this. REPOSITORY TAG IMAGE ID CREATED SIZE <user>/sklearn-digits latest 0e5c5815f2ec 3 minutes ago 2 .19GB","title":"Pull the image"},{"location":"tutorials/ds-connect/#run-an-inference-job-in-modzy","text":"If you provided the required arguments to publish() to publish the model to Modzy, you can use the Modzy SDK to submit an inference job to your newly-published model. from modzy import ApiClient client = ApiClient ( base_url = 'https://your.modzy.com/api' , api_key = modzy_api_key ) input_name = final_status [ 'result' ][ 'inputs' ][ 0 ][ 'name' ] model_id = final_status [ 'result' ] . get ( \"model\" ) . get ( \"modelId\" ) model_version = final_status [ 'result' ] . get ( \"version\" ) inference_job = client . jobs . submit_file ( model_id , model_version , { input_name : sample_filepath }) inference_job_result = client . results . block_until_complete ( inference_job , timeout = None ) inference_job_results_json = inference_job_result . get_first_outputs ()[ 'results.json' ] print ( inference_job_results_json )","title":"Run an inference job in Modzy"},{"location":"tutorials/ds-deploy-modzy/","text":"Deploy Model to Modzy Note If you just want to try Chassis, you can use the test drive, which will deploy Chassis and KFServing for you so you can use Chassis to containerize an MLflow model, push it to your Docker Hub account, and then publish it to the KFServing instance running inside the test drive, in the kfserving.ipynb sample notebook: Launch Test Drive Create the model The first steps we must follow here are the same ones that we can read in the Build a Model tutorial. We should follow exactly the same steps in case we don't have already our MLFlow model. In other case we can go directly to the step where we build the image. Basically we should: Install the SDK Import required libraries Create the model Transform the model to MLFlow Build the image Once we have finished the previous steps, we need to define not only the data that Chassis needs to build the image, but also the data that Modzy will need to deploy our model. So, we are going to assume that we have already generated our registry_auth for uploading the image to Docker Hub and we just have the data required for Chassis. image_data = { 'name' : '<user>/chassisml-sklearn-demo:latest' , 'model_name' : 'digits' , 'model_path' : './mlflow_custom_pyfunc_svm' , 'registry_auth' : 'XxXxXxXx' } Define Modzy data At this point, we are going to define the data that we need to deploy the model to Modzy. These are the fields that we need to take into account. metadata_path : this is the path to the model.yaml file that is needed to define all information about the model. Chassis has a default one, but you should define your own based on this example sample_input_path : this is the path to the sample input that is needed when deploying the model. An example that fits the model we built in this tutorial can be found here deploy : if it is True , then Chassis will manage to deploy the model into Modzy platform. Otherwise you can do this manually through the Modzy UI api_key : you should have your own api key from Modzy in order to let Chassis deploy the model for you modzy_data = { 'metadata_path' : './modzy/model.yaml' , 'sample_input_path' : './modzy/input_sample.json' , 'deploy' : True , 'api_key' : 'XxXxXxXx.XxXxXxXx' } Notice that if deploy is False this means that you can avoid defining the rest of the fields. Anyway, metadata_path should be defined in case you will eventually deploy the model to Modzy. This is important because the model will use this information when being deployed to Modzy, so it needs to be updated. Make the request Now that we have defined both the data for Chassis and the data for Modzy, we are able to make the request against Chassis service to build the image and deploy it to Modzy. Take into account that base_url should point to the address of the cluster where Chassis is running. In this case, that we want the built image to be deployed to Modzy, the cluster where Chassis is running needs to have access (VPN) to Modzy server. Otherwise, Modzy server will be unreachable by Chassis and it will not be possible to deploy the model. res = chassisml . publish ( image_data = image_data , modzy_data = modzy_data , upload = True , # ^ Needed because Chassis will deploy the model # to Modzy based on the Docker Hub uploaded image base_url = 'http://localhost:5000' ) error = res . get ( 'error' ) job_id = res . get ( 'job_id' ) if error : print ( 'Error:' , error ) else : print ( 'Job ID:' , job_id ) If everything has gone well we should see something similar to this. Publishing container... Ok! Job ID: chassis-builder-job-a3864869-a509-4658-986b-25cb4ddd604d Check the job status Notice that unlike in the KFServing tutorial , here we don't need to define the INTERFACE environment variable since Modzy is the default one. We must wait until it has finished, which may take several minutes. We can use the job id to check the status of the job. Once it has finished we should see something similar to this. { 'result' : { 'containerImage' : { 'containerImageSize' : 0 , ( ... ) 'uploadStatus' : 'IN_PROGRESS' }, 'container_url' : 'https://integration.modzy.engineering/models/<model_id>/<model_version>' , ( ... ) 'inputs' : [{ 'acceptedMediaTypes' : 'application/json' , 'description' : 'numpy array 2d' , 'maximumSize' : 1024000 , 'name' : 'input.json' }], 'isActive' : False , 'isAvailable' : True , 'longDescription' : 'It classifies digits.' , 'model' : { 'author' : 'Integration' , 'modelId' : '<model_id>' 'createdByEmail' : 'cmillan@sciling.com' , ( ... ) 'visibility' : { 'scope' : 'ALL' }}, 'outputs' : [{ 'description' : 'numpy 2d array' , 'maximumSize' : 1024000 , 'mediaType' : 'application/json' , 'name' : 'results.json' }], ( ... ) 'timeout' : { 'run' : 60000 , 'status' : 60000 }, 'version' : '0.1.0' }, 'status' : { 'active' : None , 'completion_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'conditions' : [{ 'last_probe_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'last_transition_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'message' : None , 'reason' : None , 'status' : 'True' , 'type' : 'Complete' }], 'failed' : None , 'start_time' : 'Thu, 05 Aug 2021 16:14:38 GMT' , 'succeeded' : 1 }} Two top keys can be seen: result : in case deploy was True this will contain some information related to the model deployed in Modzy. In particular we can see the full url to the model if we access the container_url key. status : this contains the information about the Kubernetes job that has built the image (and uploaded it to Modzy in case deploy was True as explained above) Query the model Now we are ready to query our model deployed in Modzy. Again, we will need VPN access to Modzy server to make a request against our model and to get the results. If we already have access, then we can use this json as a request to the model . Make the request This should be the contents of our request_body.json file. { \"model\" : { \"identifier\" : \"<model_id>\" , \"version\" : \"<model_version>\" }, \"input\" : { \"type\" : \"text\" , \"sources\" : { \"input\" : { \"<input_name>\" : \"<data_that_we_want_to_predict>\" } } } } Here there are some fields that we need to define. model_id : we can get this from the result obtained above model_version : we can get this from the result obtained above data_that_we_want_to_predict : this is the data to predict, that in our example can be the data sample So we can make the request like this. curl -X POST \\ -H 'Authorization: ApiKey <api_key>' \\ -H 'Content-Type: application/json' \\ -d@request_body.json \\ \"https://integration.modzy.engineering/api/jobs\" Which should output some information about the job that will run. Inside this information we can see the job_id , which we will use to get the results. { \"model\" : { \"identifier\" : \"<model_id>\" , \"version\" : \"<model_version>\" , (...) }, \"status\" : \"SUBMITTED\" , \"totalInputs\" : 1 , \"jobIdentifier\" : \"<job_identifier>\" , (...) \"team\" :{ (...) }, \"user\" :{ (...) }, \"jobInputs\" :{ \"identifier\" :[ \"input\" ] }, (...) } Get the results Once the job has finished, we can make a request agains Modzy again to see the results. curl -X GET \\ -H 'Authorization: ApiKey <api_key>' \\ \"https://integration.modzy.engineering/api/results/<job_id>\" And we will see the inference output. { \"jobIdentifier\" : \"<job_id>\" , \"total\" : 1 , \"completed\" : 1 , \"failed\" : 0 , \"finished\" : true , (...) \"results\" : { \"input\" : { \"status\" : \"SUCCESSFUL\" , (...) \"results.json\" : [{ \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"4\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"4\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }], \"voting\" : { (...) } } } }","title":"Deploy Model to Modzy"},{"location":"tutorials/ds-deploy-modzy/#deploy-model-to-modzy","text":"Note If you just want to try Chassis, you can use the test drive, which will deploy Chassis and KFServing for you so you can use Chassis to containerize an MLflow model, push it to your Docker Hub account, and then publish it to the KFServing instance running inside the test drive, in the kfserving.ipynb sample notebook: Launch Test Drive","title":"Deploy Model to Modzy"},{"location":"tutorials/ds-deploy-modzy/#create-the-model","text":"The first steps we must follow here are the same ones that we can read in the Build a Model tutorial. We should follow exactly the same steps in case we don't have already our MLFlow model. In other case we can go directly to the step where we build the image. Basically we should: Install the SDK Import required libraries Create the model Transform the model to MLFlow","title":"Create the model"},{"location":"tutorials/ds-deploy-modzy/#build-the-image","text":"Once we have finished the previous steps, we need to define not only the data that Chassis needs to build the image, but also the data that Modzy will need to deploy our model. So, we are going to assume that we have already generated our registry_auth for uploading the image to Docker Hub and we just have the data required for Chassis. image_data = { 'name' : '<user>/chassisml-sklearn-demo:latest' , 'model_name' : 'digits' , 'model_path' : './mlflow_custom_pyfunc_svm' , 'registry_auth' : 'XxXxXxXx' }","title":"Build the image"},{"location":"tutorials/ds-deploy-modzy/#define-modzy-data","text":"At this point, we are going to define the data that we need to deploy the model to Modzy. These are the fields that we need to take into account. metadata_path : this is the path to the model.yaml file that is needed to define all information about the model. Chassis has a default one, but you should define your own based on this example sample_input_path : this is the path to the sample input that is needed when deploying the model. An example that fits the model we built in this tutorial can be found here deploy : if it is True , then Chassis will manage to deploy the model into Modzy platform. Otherwise you can do this manually through the Modzy UI api_key : you should have your own api key from Modzy in order to let Chassis deploy the model for you modzy_data = { 'metadata_path' : './modzy/model.yaml' , 'sample_input_path' : './modzy/input_sample.json' , 'deploy' : True , 'api_key' : 'XxXxXxXx.XxXxXxXx' } Notice that if deploy is False this means that you can avoid defining the rest of the fields. Anyway, metadata_path should be defined in case you will eventually deploy the model to Modzy. This is important because the model will use this information when being deployed to Modzy, so it needs to be updated.","title":"Define Modzy data"},{"location":"tutorials/ds-deploy-modzy/#make-the-request","text":"Now that we have defined both the data for Chassis and the data for Modzy, we are able to make the request against Chassis service to build the image and deploy it to Modzy. Take into account that base_url should point to the address of the cluster where Chassis is running. In this case, that we want the built image to be deployed to Modzy, the cluster where Chassis is running needs to have access (VPN) to Modzy server. Otherwise, Modzy server will be unreachable by Chassis and it will not be possible to deploy the model. res = chassisml . publish ( image_data = image_data , modzy_data = modzy_data , upload = True , # ^ Needed because Chassis will deploy the model # to Modzy based on the Docker Hub uploaded image base_url = 'http://localhost:5000' ) error = res . get ( 'error' ) job_id = res . get ( 'job_id' ) if error : print ( 'Error:' , error ) else : print ( 'Job ID:' , job_id ) If everything has gone well we should see something similar to this. Publishing container... Ok! Job ID: chassis-builder-job-a3864869-a509-4658-986b-25cb4ddd604d","title":"Make the request"},{"location":"tutorials/ds-deploy-modzy/#check-the-job-status","text":"Notice that unlike in the KFServing tutorial , here we don't need to define the INTERFACE environment variable since Modzy is the default one. We must wait until it has finished, which may take several minutes. We can use the job id to check the status of the job. Once it has finished we should see something similar to this. { 'result' : { 'containerImage' : { 'containerImageSize' : 0 , ( ... ) 'uploadStatus' : 'IN_PROGRESS' }, 'container_url' : 'https://integration.modzy.engineering/models/<model_id>/<model_version>' , ( ... ) 'inputs' : [{ 'acceptedMediaTypes' : 'application/json' , 'description' : 'numpy array 2d' , 'maximumSize' : 1024000 , 'name' : 'input.json' }], 'isActive' : False , 'isAvailable' : True , 'longDescription' : 'It classifies digits.' , 'model' : { 'author' : 'Integration' , 'modelId' : '<model_id>' 'createdByEmail' : 'cmillan@sciling.com' , ( ... ) 'visibility' : { 'scope' : 'ALL' }}, 'outputs' : [{ 'description' : 'numpy 2d array' , 'maximumSize' : 1024000 , 'mediaType' : 'application/json' , 'name' : 'results.json' }], ( ... ) 'timeout' : { 'run' : 60000 , 'status' : 60000 }, 'version' : '0.1.0' }, 'status' : { 'active' : None , 'completion_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'conditions' : [{ 'last_probe_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'last_transition_time' : 'Thu, 05 Aug 2021 16:29:36 GMT' , 'message' : None , 'reason' : None , 'status' : 'True' , 'type' : 'Complete' }], 'failed' : None , 'start_time' : 'Thu, 05 Aug 2021 16:14:38 GMT' , 'succeeded' : 1 }} Two top keys can be seen: result : in case deploy was True this will contain some information related to the model deployed in Modzy. In particular we can see the full url to the model if we access the container_url key. status : this contains the information about the Kubernetes job that has built the image (and uploaded it to Modzy in case deploy was True as explained above)","title":"Check the job status"},{"location":"tutorials/ds-deploy-modzy/#query-the-model","text":"Now we are ready to query our model deployed in Modzy. Again, we will need VPN access to Modzy server to make a request against our model and to get the results. If we already have access, then we can use this json as a request to the model .","title":"Query the model"},{"location":"tutorials/ds-deploy-modzy/#make-the-request_1","text":"This should be the contents of our request_body.json file. { \"model\" : { \"identifier\" : \"<model_id>\" , \"version\" : \"<model_version>\" }, \"input\" : { \"type\" : \"text\" , \"sources\" : { \"input\" : { \"<input_name>\" : \"<data_that_we_want_to_predict>\" } } } } Here there are some fields that we need to define. model_id : we can get this from the result obtained above model_version : we can get this from the result obtained above data_that_we_want_to_predict : this is the data to predict, that in our example can be the data sample So we can make the request like this. curl -X POST \\ -H 'Authorization: ApiKey <api_key>' \\ -H 'Content-Type: application/json' \\ -d@request_body.json \\ \"https://integration.modzy.engineering/api/jobs\" Which should output some information about the job that will run. Inside this information we can see the job_id , which we will use to get the results. { \"model\" : { \"identifier\" : \"<model_id>\" , \"version\" : \"<model_version>\" , (...) }, \"status\" : \"SUBMITTED\" , \"totalInputs\" : 1 , \"jobIdentifier\" : \"<job_identifier>\" , (...) \"team\" :{ (...) }, \"user\" :{ (...) }, \"jobInputs\" :{ \"identifier\" :[ \"input\" ] }, (...) }","title":"Make the request"},{"location":"tutorials/ds-deploy-modzy/#get-the-results","text":"Once the job has finished, we can make a request agains Modzy again to see the results. curl -X GET \\ -H 'Authorization: ApiKey <api_key>' \\ \"https://integration.modzy.engineering/api/results/<job_id>\" And we will see the inference output. { \"jobIdentifier\" : \"<job_id>\" , \"total\" : 1 , \"completed\" : 1 , \"failed\" : 0 , \"finished\" : true , (...) \"results\" : { \"input\" : { \"status\" : \"SUCCESSFUL\" , (...) \"results.json\" : [{ \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"4\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"4\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }, { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : \"8\" , \"score\" : \"1\" }] }, \"explanation\" : null , \"drift\" : null } }], \"voting\" : { (...) } } } }","title":"Get the results"},{"location":"tutorials/ds-deploy/","text":"Deploy Model to KFServing Note If you just want to try Chassis, you can use the test drive, which will deploy Chassis and KFServing for you so you can use Chassis to containerize an MLflow model, push it to your Docker Hub account, and then publish it to the KFServing instance running inside the test drive, in the kfserving.ipynb sample notebook: Launch Test Drive Install KFServing in minikube You just need to clone the KFServing repository and run the quick_install.sh script. # Install KFServing. Versions above 0.5.1 doesn't work, so force it. git clone --single-branch --branch v0.5.1 git@github.com:kubeflow/kfserving.git ./kfserving/hack/quick_install.sh Required variables There are some environment variables that must be defined for KFServing to work: INTERFACE: kfserving HTTP_PORT: port where kfserving will be running PROTOCOL: it can be v1 or v2 MODEL_NAME: a name for the model must be defined Deploy the model Assuming the image generated by ChassisML has been uploaded to a repository called carmilso/chassisml-sklearn-demo:latest , just deploy the file that defines the InferenceService for the protocol v1 of KFServing apiVersion : \"serving.kubeflow.org/v1beta1\" kind : \"InferenceService\" metadata : name : chassisml-sklearn-demo spec : predictor : containers : - image : carmilso/chassisml-sklearn-demo:latest name : chassisml-sklearn-demo-container env : - name : INTERFACE value : kfserving - name : HTTP_PORT value : \"8080\" - name : PROTOCOL value : v1 - name : MODEL_NAME value : digits ports : - containerPort : 8080 protocol : TCP In this case, the variable MODEL_NAME should not be necessary since it's defined when creating the image. kubectl apply -f custom_v1.yaml This should output a success message. Define required variables to query the pod This is needed in order to be able to communicate with the deployed image. The SERVICE_NAME must match the name defined in the metadata.name of the InferenceService created above. The MODEL_NAME must match the name of your model. It can be defined by the data scientist when making the request against Chassis service or overwritten in the InferenceService as defined above. export INGRESS_HOST = $( minikube ip ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) export SERVICE_NAME = chassisml-sklearn-demo export MODEL_NAME = digits export SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ SERVICE_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) Query the model Please note that you must base64 encode each input instance. For example: import json import base64 as b64 instances = [[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ]] input_dict = { 'instances' : [ b64 . b64encode ( str ( entry ) . encode ()) . decode () for entry in instances ]} json . dump ( input_dict , open ( 'kserve_input.json' , 'w' )) Now you can just make a request to predict some data. Take into account that you must download inputsv1.json before making the request. curl -H \"Host: ${ SERVICE_HOSTNAME } \" \"http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict\" -d@inputsv1.json | jq The output should be similar to this: { \"predictions\" : [ { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } } ] } In this case, the data was prepared for the protocol v1, but we can deploy the image using the protocol v2 and make the request using the data for v2 . Deploy the model locally The model can also be deployed locally: docker run --rm -p 8080 :8080 \\ -e INTERFACE = kfserving \\ -e HTTP_PORT = 8080 \\ -e PROTOCOL = v2 \\ -e MODEL_NAME = digits \\ carmilso/chassisml-sklearn-demo:latest So we can query it this way. Take into account that you must download inputsv2.json before making the request: curl localhost:8080/v2/models/digits/infer -d@inputsv2.json","title":"Deploy Model to KFServing"},{"location":"tutorials/ds-deploy/#deploy-model-to-kfserving","text":"Note If you just want to try Chassis, you can use the test drive, which will deploy Chassis and KFServing for you so you can use Chassis to containerize an MLflow model, push it to your Docker Hub account, and then publish it to the KFServing instance running inside the test drive, in the kfserving.ipynb sample notebook: Launch Test Drive","title":"Deploy Model to KFServing"},{"location":"tutorials/ds-deploy/#install-kfserving-in-minikube","text":"You just need to clone the KFServing repository and run the quick_install.sh script. # Install KFServing. Versions above 0.5.1 doesn't work, so force it. git clone --single-branch --branch v0.5.1 git@github.com:kubeflow/kfserving.git ./kfserving/hack/quick_install.sh","title":"Install KFServing in minikube"},{"location":"tutorials/ds-deploy/#required-variables","text":"There are some environment variables that must be defined for KFServing to work: INTERFACE: kfserving HTTP_PORT: port where kfserving will be running PROTOCOL: it can be v1 or v2 MODEL_NAME: a name for the model must be defined","title":"Required variables"},{"location":"tutorials/ds-deploy/#deploy-the-model","text":"Assuming the image generated by ChassisML has been uploaded to a repository called carmilso/chassisml-sklearn-demo:latest , just deploy the file that defines the InferenceService for the protocol v1 of KFServing apiVersion : \"serving.kubeflow.org/v1beta1\" kind : \"InferenceService\" metadata : name : chassisml-sklearn-demo spec : predictor : containers : - image : carmilso/chassisml-sklearn-demo:latest name : chassisml-sklearn-demo-container env : - name : INTERFACE value : kfserving - name : HTTP_PORT value : \"8080\" - name : PROTOCOL value : v1 - name : MODEL_NAME value : digits ports : - containerPort : 8080 protocol : TCP In this case, the variable MODEL_NAME should not be necessary since it's defined when creating the image. kubectl apply -f custom_v1.yaml This should output a success message.","title":"Deploy the model"},{"location":"tutorials/ds-deploy/#define-required-variables-to-query-the-pod","text":"This is needed in order to be able to communicate with the deployed image. The SERVICE_NAME must match the name defined in the metadata.name of the InferenceService created above. The MODEL_NAME must match the name of your model. It can be defined by the data scientist when making the request against Chassis service or overwritten in the InferenceService as defined above. export INGRESS_HOST = $( minikube ip ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) export SERVICE_NAME = chassisml-sklearn-demo export MODEL_NAME = digits export SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ SERVICE_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 )","title":"Define required variables to query the pod"},{"location":"tutorials/ds-deploy/#query-the-model","text":"Please note that you must base64 encode each input instance. For example: import json import base64 as b64 instances = [[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ]] input_dict = { 'instances' : [ b64 . b64encode ( str ( entry ) . encode ()) . decode () for entry in instances ]} json . dump ( input_dict , open ( 'kserve_input.json' , 'w' )) Now you can just make a request to predict some data. Take into account that you must download inputsv1.json before making the request. curl -H \"Host: ${ SERVICE_HOSTNAME } \" \"http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict\" -d@inputsv1.json | jq The output should be similar to this: { \"predictions\" : [ { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } } ] } In this case, the data was prepared for the protocol v1, but we can deploy the image using the protocol v2 and make the request using the data for v2 .","title":"Query the model"},{"location":"tutorials/ds-deploy/#deploy-the-model-locally","text":"The model can also be deployed locally: docker run --rm -p 8080 :8080 \\ -e INTERFACE = kfserving \\ -e HTTP_PORT = 8080 \\ -e PROTOCOL = v2 \\ -e MODEL_NAME = digits \\ carmilso/chassisml-sklearn-demo:latest So we can query it this way. Take into account that you must download inputsv2.json before making the request: curl localhost:8080/v2/models/digits/infer -d@inputsv2.json","title":"Deploy the model locally"}]}