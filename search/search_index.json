{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Build ML Model Containers. Automatically. Turn your machine learning models into portable container images that can run just about anywhere using Chassis. For a deeper dive and better understanding of what Chassis is, learn more here . Easy to Connect Requires Installation Quickest and easiest way to start using Chassis. No DevOps experience required. Preferred Connect to Chassis Service More involved installation. Requires a moderate understanding of Kubernetes, Helm, and Docker. Install Chassis on Your Machine How it Works After connecting to the Chassis service, your workflow will involve a few simple steps: Set Up Environment Create your workspace environment, open a Jupyter Notebook or other Python editor, and install the Chassisml SDK. pip install chassisml Load Your Model Train your model or load your pre-trained model into memory ( .pth , .pkl , .h5 , .joblib , or other file format - all model types and formats are supported!). model = framework . load ( \"path/to/model.file\" ) Write Process Function The process function will use your model to perform any required preprocessing and inference execution on the incoming input_bytes data. def process ( input_bytes ): # preprocess data = preprocess ( input_bytes ) # run inference predictions = model . predict ( data ) # post process predictions formatted_results = postprocess ( predictions ) return formatted_results Initialize Client and Create Chassis Model NOTE : Depending on how you connect to the service, you will need to identify the URL on which the service is running and can be accessed. If you are connecting to the publicly-hosted version, make sure to sign up to access this URL. Otherwise if you are deploying manually and connecting to a locally running instance, your URL will look something like http://localhost:5000 . Once you have this URL, replace <chassis-instance-url> in the below line with your URL. chassis_client = chassisml . ChassisClient ( \"<chassis-instance-url>\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Publish Chassis Model response = chassis_model . publish ( model_name = \"Sample ML Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) Run and Query Model Run your model locally or on your preferred serving platform and begin making inference calls right away.","title":"Home"},{"location":"#build-ml-model-containers-automatically","text":"Turn your machine learning models into portable container images that can run just about anywhere using Chassis. For a deeper dive and better understanding of what Chassis is, learn more here . Easy to Connect Requires Installation Quickest and easiest way to start using Chassis. No DevOps experience required. Preferred Connect to Chassis Service More involved installation. Requires a moderate understanding of Kubernetes, Helm, and Docker. Install Chassis on Your Machine","title":"Build ML Model Containers. Automatically."},{"location":"#how-it-works","text":"After connecting to the Chassis service, your workflow will involve a few simple steps:","title":"How it Works"},{"location":"#set-up-environment","text":"Create your workspace environment, open a Jupyter Notebook or other Python editor, and install the Chassisml SDK. pip install chassisml","title":"Set Up Environment"},{"location":"#load-your-model","text":"Train your model or load your pre-trained model into memory ( .pth , .pkl , .h5 , .joblib , or other file format - all model types and formats are supported!). model = framework . load ( \"path/to/model.file\" )","title":"Load Your Model"},{"location":"#write-process-function","text":"The process function will use your model to perform any required preprocessing and inference execution on the incoming input_bytes data. def process ( input_bytes ): # preprocess data = preprocess ( input_bytes ) # run inference predictions = model . predict ( data ) # post process predictions formatted_results = postprocess ( predictions ) return formatted_results","title":"Write Process Function"},{"location":"#initialize-client-and-create-chassis-model","text":"NOTE : Depending on how you connect to the service, you will need to identify the URL on which the service is running and can be accessed. If you are connecting to the publicly-hosted version, make sure to sign up to access this URL. Otherwise if you are deploying manually and connecting to a locally running instance, your URL will look something like http://localhost:5000 . Once you have this URL, replace <chassis-instance-url> in the below line with your URL. chassis_client = chassisml . ChassisClient ( \"<chassis-instance-url>\" ) chassis_model = chassis_client . create_model ( process_fn = process )","title":"Initialize Client and Create Chassis Model"},{"location":"#publish-chassis-model","text":"response = chassis_model . publish ( model_name = \"Sample ML Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , )","title":"Publish Chassis Model"},{"location":"#run-and-query-model","text":"Run your model locally or on your preferred serving platform and begin making inference calls right away.","title":"Run and Query Model"},{"location":"chassisml_sdk-reference/","text":"Chassisml Python SDK Introduction The Chassisml Python SDK offers convenience functions that interact with the Chassisml service to automate the containerization and deployment of models to your preferred model serving platform. It is organized into two classes: ChassisClient : interacts with the Chassis routes that build a user's container from their model artifacts ChasissModel : creates a chassis compliant model out of a few lines of Python code from user supplied model artifacts First, install the Chassisml SDK to get started: pip install chassisml To import the library into your editor: import chassisml Usage ChassisClient The Chassis Client object. This class is used to interact with the Kaniko service. Attributes: Name Type Description base_url str The base url for the API. Source code in chassisml_sdk/chassisml/chassisml.py class ChassisClient : \"\"\"The Chassis Client object. This class is used to interact with the Kaniko service. Attributes: base_url (str): The base url for the API. \"\"\" def __init__ ( self , base_url = 'http://localhost:5000' ): self . base_url = base_url def get_job_status ( self , job_id ): ''' Checks the status of a chassis job Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method Returns: Dict: JSON Chassis job status Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') job_status = chassis_client.get_job_status(job_id) ``` ''' route = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } ' res = requests . get ( route ) data = res . json () return data def block_until_complete ( self , job_id , timeout = None , poll_interval = 5 ): ''' Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method timeout (int): Timeout threshold in seconds poll_intervall (int): Amount of time to wait in between API polls to check status of job Returns: Dict: final job status returned by `ChassisClient.block_until_complete` method Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') final_status = chassis_client.block_until_complete(job_id) ``` ''' endby = time . time () + timeout if ( timeout is not None ) else None while True : status = self . get_job_status ( job_id ) if status [ 'status' ][ 'succeeded' ] or status [ 'status' ][ 'failed' ]: return status if ( endby is not None ) and ( time . time () > endby - poll_interval ): print ( 'Timed out before completion.' ) return False time . sleep ( poll_interval ) def download_tar ( self , job_id , output_filename ): ''' Downloads container image as tar archive **NOTE**: This method is not available in the publicly-hosted service. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method output_filename (str): Local output filepath to save container image Returns: None: This method does not return an object Examples: ```python # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id) chassis_client.download_tar(job_id, \"./chassis-model.tar\") ``` ''' url = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } /download-tar' r = requests . get ( url ) if r . status_code == 200 : with open ( output_filename , 'wb' ) as f : f . write ( r . content ) else : print ( f 'Error download tar: { r . text } ' ) def test_OMI_compliance ( self , image_id = None ): ''' Tests a local image for compliance with the [Open Model Interface Specification](https://openmodel.ml/spec/) Args: image_id (str): image id of a local docker container. e.g. `dockerusername/repositoryname:tag` Returns: tuple(bool, str): Tuple containing compliance boolean (`True` if compliant, `False` if not) and corresponding string containing concatenation of any logs. Examples: ```python # test a local docker image OMI_test, logs = chassis_client.test_OMI_compliance(image_id) if OMI_test: print(\"OMI compliance test passed\") else: print(\"OMI compliance test failed\",logs) ``` ''' rValue = ( False , \"Nothing Initialized\" ) try : checkObject = OMI_check ( image_id = image_id ) if checkObject . client is None : raise TypeError ( \"The Docker Client couldn't be initialized. Is Docker installed?\" ) image_check = checkObject . validate_image () if \"Failure\" in image_check : raise ValueError ( image_check ) container_start = checkObject . start_container () if \"Failure\" in container_start : raise ValueError ( container_start ) gRPC_check = checkObject . validate_gRPC () if \"Failure\" in gRPC_check : raise ValueError ( gRPC_check ) clean_up = checkObject . clean_up () if \"Failure\" in clean_up : raise ValueError ( clean_up ) rValue = ( True , \" \\n \" + image_check + \" \\n \" + container_start + \" \\n \" + gRPC_check + \" \\n \" + clean_up ) except Exception as e : rValue = ( False , e ) return rValue def create_model ( self , process_fn = None , batch_process_fn = None , batch_size = None ): ''' Builds chassis model locally Args: process_fn (function): Python function that must accept a single piece of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_process_fn (function): Python function that must accept a batch of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_size (int): Maximum batch size if `batch_process_fn` is defined Returns: ChassisModel: Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this [example](https://docs.modzy.com/docs/chassis-ml). ```python # Import and normalize data X_digits, y_digits = datasets.load_digits(return_X_y=True) X_digits = X_digits / X_digits.max() n_samples = len(X_digits) # Split data into training and test sets X_train = X_digits[: int(0.9 * n_samples)] y_train = y_digits[: int(0.9 * n_samples)] X_test = X_digits[int(0.9 * n_samples) :] y_test = y_digits[int(0.9 * n_samples) :] # Train Model logistic = LogisticRegression(max_iter=1000) print( \"LogisticRegression mean accuracy score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test) ) # Save small sample input to use for testing later sample = X_test[:5].tolist() with open(\"digits_sample.json\", 'w') as out: json.dump(sample, out) # Define Process function def process(input_bytes): inputs = np.array(json.loads(input_bytes)) inference_results = logistic.predict(inputs) structured_results = [] for inference_result in inference_results: structured_output = { \"data\": { \"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]} } } structured_results.append(structured_output) return structured_results # create Chassis model chassis_model = chassis_client.create_model(process_fn=process) ``` ''' if not ( process_fn or batch_process_fn ): raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) if ( batch_process_fn and not batch_size ) or ( batch_size and not batch_process_fn ): raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) return ChassisModel ( process_fn , batch_process_fn , batch_size , self . base_url ) def run_inference ( self , input_data , container_url = \"localhost\" , host_port = 45000 ): ''' This is the method you use to submit data to a container chassis has built for inference. It assumes the container has been downloaded from dockerhub and is running somewhere you have access to. Args: input_data (json): dictionary of the form {\"input\": <binary respresentaion of your data>} container_url (str): URL where container is running host_port (int): host port that forwards to container's grpc server port Returns: return_value (str): Success -> results from model processing as specified in the process function. Failure -> Error codes from processing errors. All errors should container the word \"Error.\" Examples: ```python # assume that the container is running locally, and that it was started with this docker command # docker run -it -p 5001:45000 <docker_uname>/<container_name>:<tag_id> from chassisml_sdk.chassisml import chassisml client = chassisml.ChassisClient() input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"} input_list = [input_data for _ in range(30)] print(\"single input\") print(client.run_inference(input_data, container_url=\"localhost\", host_port=5001)) print(\"multi inputs\") results = client.run_inference(input_list, container_url=\"localhost\", host_port=5001) for x in results: print(x) ``` ''' model_client . override_server_URL ( container_url , host_port ) return model_client . run ( input_data ) def docker_infer ( self , image_id , input_data , container_url = \"localhost\" , host_port = 5001 , container_port = None , timeout = 20 , clean_up = True , pull_container = False ): ''' Runs inference on an OMI compliant container. This method checks to see if a container is running and if not starts it. The method then runs inference against the input_data with the model in the container, and optionally shuts down the container. Args: image_id (string): the name of an OMI container image usually of the form <docker_uname>/<container_name>:<tag_id> input_data (json): dictionary of the form {\"input\": <binary respresentaion of your data>} container_url (str): URL where container is running host_port (int): host port that forwards to container's grpc server port container_port (str): container port the grpc server listens to timeout (int): number of seconds to wait for gRPC server to spin up clean_up (bool): whether or not to stop and remove the container after inference pull_container (bool): if True pulls missing container from repo Returns: return_value (str): Success -> model output as defined in the process function Failure -> Error message if any success criteria is missing. Example: ```python host_port = 5002 client = chassisml.ChassisClient() input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"} input_list = [input_data for _ in range(30)] print(\"single input\") print(client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_data, container_url=\"localhost\", host_port=host_port, clean_up=False, pull_container=True)) print(\"multi inputs\") results = client.run_inference(input_list, container_url=\"localhost\", host_port=host_port) results = client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_list, container_url=\"localhost\", host_port=host_port) for x in results: print(x) ``` ''' try : container_id = docker_start ( image_id , host_port = host_port , container_port = container_port , timeout = timeout , pull_container = pull_container ) if \"Error\" in container_id : raise ValueError ( \"container_id wrong\" ) return_value = self . run_inference ( input_data , container_url = container_url , host_port = host_port ) if clean_up : docker_clean_up ( container_id ) except Exception as e : return_value = { \"results\" : [ \"Error \" + str ( e )]} return return_value block_until_complete ( self , job_id , timeout = None , poll_interval = 5 ) Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required timeout int Timeout threshold in seconds None poll_intervall int Amount of time to wait in between API polls to check status of job required Returns: Type Description Dict final job status returned by ChassisClient.block_until_complete method Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) Source code in chassisml_sdk/chassisml/chassisml.py def block_until_complete ( self , job_id , timeout = None , poll_interval = 5 ): ''' Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method timeout (int): Timeout threshold in seconds poll_intervall (int): Amount of time to wait in between API polls to check status of job Returns: Dict: final job status returned by `ChassisClient.block_until_complete` method Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') final_status = chassis_client.block_until_complete(job_id) ``` ''' endby = time . time () + timeout if ( timeout is not None ) else None while True : status = self . get_job_status ( job_id ) if status [ 'status' ][ 'succeeded' ] or status [ 'status' ][ 'failed' ]: return status if ( endby is not None ) and ( time . time () > endby - poll_interval ): print ( 'Timed out before completion.' ) return False time . sleep ( poll_interval ) create_model ( self , process_fn = None , batch_process_fn = None , batch_size = None ) Builds chassis model locally Parameters: Name Type Description Default process_fn function Python function that must accept a single piece of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the process method None batch_process_fn function Python function that must accept a batch of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the process method None batch_size int Maximum batch size if batch_process_fn is defined None Returns: Type Description ChassisModel Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this example . # Import and normalize data X_digits , y_digits = datasets . load_digits ( return_X_y = True ) X_digits = X_digits / X_digits . max () n_samples = len ( X_digits ) # Split data into training and test sets X_train = X_digits [: int ( 0.9 * n_samples )] y_train = y_digits [: int ( 0.9 * n_samples )] X_test = X_digits [ int ( 0.9 * n_samples ) :] y_test = y_digits [ int ( 0.9 * n_samples ) :] # Train Model logistic = LogisticRegression ( max_iter = 1000 ) print ( \"LogisticRegression mean accuracy score: %f \" % logistic . fit ( X_train , y_train ) . score ( X_test , y_test ) ) # Save small sample input to use for testing later sample = X_test [: 5 ] . tolist () with open ( \"digits_sample.json\" , 'w' ) as out : json . dump ( sample , out ) # Define Process function def process ( input_bytes ): inputs = np . array ( json . loads ( input_bytes )) inference_results = logistic . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results # create Chassis model chassis_model = chassis_client . create_model ( process_fn = process ) Source code in chassisml_sdk/chassisml/chassisml.py def create_model ( self , process_fn = None , batch_process_fn = None , batch_size = None ): ''' Builds chassis model locally Args: process_fn (function): Python function that must accept a single piece of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_process_fn (function): Python function that must accept a batch of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_size (int): Maximum batch size if `batch_process_fn` is defined Returns: ChassisModel: Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this [example](https://docs.modzy.com/docs/chassis-ml). ```python # Import and normalize data X_digits, y_digits = datasets.load_digits(return_X_y=True) X_digits = X_digits / X_digits.max() n_samples = len(X_digits) # Split data into training and test sets X_train = X_digits[: int(0.9 * n_samples)] y_train = y_digits[: int(0.9 * n_samples)] X_test = X_digits[int(0.9 * n_samples) :] y_test = y_digits[int(0.9 * n_samples) :] # Train Model logistic = LogisticRegression(max_iter=1000) print( \"LogisticRegression mean accuracy score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test) ) # Save small sample input to use for testing later sample = X_test[:5].tolist() with open(\"digits_sample.json\", 'w') as out: json.dump(sample, out) # Define Process function def process(input_bytes): inputs = np.array(json.loads(input_bytes)) inference_results = logistic.predict(inputs) structured_results = [] for inference_result in inference_results: structured_output = { \"data\": { \"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]} } } structured_results.append(structured_output) return structured_results # create Chassis model chassis_model = chassis_client.create_model(process_fn=process) ``` ''' if not ( process_fn or batch_process_fn ): raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) if ( batch_process_fn and not batch_size ) or ( batch_size and not batch_process_fn ): raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) return ChassisModel ( process_fn , batch_process_fn , batch_size , self . base_url ) docker_infer ( self , image_id , input_data , container_url = 'localhost' , host_port = 5001 , container_port = None , timeout = 20 , clean_up = True , pull_container = False ) Runs inference on an OMI compliant container. This method checks to see if a container is running and if not starts it. The method then runs inference against the input_data with the model in the container, and optionally shuts down the container. Parameters: Name Type Description Default image_id string the name of an OMI container image usually of the form / : required input_data json dictionary of the form {\"input\": } required container_url str URL where container is running 'localhost' host_port int host port that forwards to container's grpc server port 5001 container_port str container port the grpc server listens to None timeout int number of seconds to wait for gRPC server to spin up 20 clean_up bool whether or not to stop and remove the container after inference True pull_container bool if True pulls missing container from repo False Returns: Type Description return_value (str) Success -> model output as defined in the process function Failure -> Error message if any success criteria is missing. Examples: host_port = 5002 client = chassisml . ChassisClient () input_data = { \"input\" : b \"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\" } input_list = [ input_data for _ in range ( 30 )] print ( \"single input\" ) print ( client . docker_infer ( image_id = \"claytondavisms/sklearn-digits-docker-test:0.0.7\" , input_data = input_data , container_url = \"localhost\" , host_port = host_port , clean_up = False , pull_container = True )) print ( \"multi inputs\" ) results = client . run_inference ( input_list , container_url = \"localhost\" , host_port = host_port ) results = client . docker_infer ( image_id = \"claytondavisms/sklearn-digits-docker-test:0.0.7\" , input_data = input_list , container_url = \"localhost\" , host_port = host_port ) for x in results : print ( x ) Source code in chassisml_sdk/chassisml/chassisml.py def docker_infer ( self , image_id , input_data , container_url = \"localhost\" , host_port = 5001 , container_port = None , timeout = 20 , clean_up = True , pull_container = False ): ''' Runs inference on an OMI compliant container. This method checks to see if a container is running and if not starts it. The method then runs inference against the input_data with the model in the container, and optionally shuts down the container. Args: image_id (string): the name of an OMI container image usually of the form <docker_uname>/<container_name>:<tag_id> input_data (json): dictionary of the form {\"input\": <binary respresentaion of your data>} container_url (str): URL where container is running host_port (int): host port that forwards to container's grpc server port container_port (str): container port the grpc server listens to timeout (int): number of seconds to wait for gRPC server to spin up clean_up (bool): whether or not to stop and remove the container after inference pull_container (bool): if True pulls missing container from repo Returns: return_value (str): Success -> model output as defined in the process function Failure -> Error message if any success criteria is missing. Example: ```python host_port = 5002 client = chassisml.ChassisClient() input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"} input_list = [input_data for _ in range(30)] print(\"single input\") print(client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_data, container_url=\"localhost\", host_port=host_port, clean_up=False, pull_container=True)) print(\"multi inputs\") results = client.run_inference(input_list, container_url=\"localhost\", host_port=host_port) results = client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_list, container_url=\"localhost\", host_port=host_port) for x in results: print(x) ``` ''' try : container_id = docker_start ( image_id , host_port = host_port , container_port = container_port , timeout = timeout , pull_container = pull_container ) if \"Error\" in container_id : raise ValueError ( \"container_id wrong\" ) return_value = self . run_inference ( input_data , container_url = container_url , host_port = host_port ) if clean_up : docker_clean_up ( container_id ) except Exception as e : return_value = { \"results\" : [ \"Error \" + str ( e )]} return return_value download_tar ( self , job_id , output_filename ) Downloads container image as tar archive NOTE : This method is not available in the publicly-hosted service. Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required output_filename str Local output filepath to save container image required Returns: Type Description None This method does not return an object Examples: # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id) chassis_client . download_tar ( job_id , \"./chassis-model.tar\" ) Source code in chassisml_sdk/chassisml/chassisml.py def download_tar ( self , job_id , output_filename ): ''' Downloads container image as tar archive **NOTE**: This method is not available in the publicly-hosted service. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method output_filename (str): Local output filepath to save container image Returns: None: This method does not return an object Examples: ```python # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id) chassis_client.download_tar(job_id, \"./chassis-model.tar\") ``` ''' url = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } /download-tar' r = requests . get ( url ) if r . status_code == 200 : with open ( output_filename , 'wb' ) as f : f . write ( r . content ) else : print ( f 'Error download tar: { r . text } ' ) get_job_status ( self , job_id ) Checks the status of a chassis job Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required Returns: Type Description Dict JSON Chassis job status Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) job_status = chassis_client . get_job_status ( job_id ) Source code in chassisml_sdk/chassisml/chassisml.py def get_job_status ( self , job_id ): ''' Checks the status of a chassis job Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method Returns: Dict: JSON Chassis job status Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') job_status = chassis_client.get_job_status(job_id) ``` ''' route = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } ' res = requests . get ( route ) data = res . json () return data run_inference ( self , input_data , container_url = 'localhost' , host_port = 45000 ) This is the method you use to submit data to a container chassis has built for inference. It assumes the container has been downloaded from dockerhub and is running somewhere you have access to. Parameters: Name Type Description Default input_data json dictionary of the form {\"input\": } required container_url str URL where container is running 'localhost' host_port int host port that forwards to container's grpc server port 45000 Returns: Type Description return_value (str) Success -> results from model processing as specified in the process function. Failure -> Error codes from processing errors. All errors should container the word \"Error.\" Examples: # assume that the container is running locally, and that it was started with this docker command # docker run -it -p 5001:45000 <docker_uname>/<container_name>:<tag_id> from chassisml_sdk.chassisml import chassisml client = chassisml . ChassisClient () input_data = { \"input\" : b \"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\" } input_list = [ input_data for _ in range ( 30 )] print ( \"single input\" ) print ( client . run_inference ( input_data , container_url = \"localhost\" , host_port = 5001 )) print ( \"multi inputs\" ) results = client . run_inference ( input_list , container_url = \"localhost\" , host_port = 5001 ) for x in results : print ( x ) Source code in chassisml_sdk/chassisml/chassisml.py def run_inference ( self , input_data , container_url = \"localhost\" , host_port = 45000 ): ''' This is the method you use to submit data to a container chassis has built for inference. It assumes the container has been downloaded from dockerhub and is running somewhere you have access to. Args: input_data (json): dictionary of the form {\"input\": <binary respresentaion of your data>} container_url (str): URL where container is running host_port (int): host port that forwards to container's grpc server port Returns: return_value (str): Success -> results from model processing as specified in the process function. Failure -> Error codes from processing errors. All errors should container the word \"Error.\" Examples: ```python # assume that the container is running locally, and that it was started with this docker command # docker run -it -p 5001:45000 <docker_uname>/<container_name>:<tag_id> from chassisml_sdk.chassisml import chassisml client = chassisml.ChassisClient() input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"} input_list = [input_data for _ in range(30)] print(\"single input\") print(client.run_inference(input_data, container_url=\"localhost\", host_port=5001)) print(\"multi inputs\") results = client.run_inference(input_list, container_url=\"localhost\", host_port=5001) for x in results: print(x) ``` ''' model_client . override_server_URL ( container_url , host_port ) return model_client . run ( input_data ) test_OMI_compliance ( self , image_id = None ) Tests a local image for compliance with the Open Model Interface Specification Parameters: Name Type Description Default image_id str image id of a local docker container. e.g. dockerusername/repositoryname:tag None Returns: Type Description tuple(bool, str) Tuple containing compliance boolean ( True if compliant, False if not) and corresponding string containing concatenation of any logs. Examples: # test a local docker image OMI_test , logs = chassis_client . test_OMI_compliance ( image_id ) if OMI_test : print ( \"OMI compliance test passed\" ) else : print ( \"OMI compliance test failed\" , logs ) Source code in chassisml_sdk/chassisml/chassisml.py def test_OMI_compliance ( self , image_id = None ): ''' Tests a local image for compliance with the [Open Model Interface Specification](https://openmodel.ml/spec/) Args: image_id (str): image id of a local docker container. e.g. `dockerusername/repositoryname:tag` Returns: tuple(bool, str): Tuple containing compliance boolean (`True` if compliant, `False` if not) and corresponding string containing concatenation of any logs. Examples: ```python # test a local docker image OMI_test, logs = chassis_client.test_OMI_compliance(image_id) if OMI_test: print(\"OMI compliance test passed\") else: print(\"OMI compliance test failed\",logs) ``` ''' rValue = ( False , \"Nothing Initialized\" ) try : checkObject = OMI_check ( image_id = image_id ) if checkObject . client is None : raise TypeError ( \"The Docker Client couldn't be initialized. Is Docker installed?\" ) image_check = checkObject . validate_image () if \"Failure\" in image_check : raise ValueError ( image_check ) container_start = checkObject . start_container () if \"Failure\" in container_start : raise ValueError ( container_start ) gRPC_check = checkObject . validate_gRPC () if \"Failure\" in gRPC_check : raise ValueError ( gRPC_check ) clean_up = checkObject . clean_up () if \"Failure\" in clean_up : raise ValueError ( clean_up ) rValue = ( True , \" \\n \" + image_check + \" \\n \" + container_start + \" \\n \" + gRPC_check + \" \\n \" + clean_up ) except Exception as e : rValue = ( False , e ) return rValue ChassisModel ( PythonModel ) The Chassis Model object. This class inherits from mlflow.pyfunc.PythonModel and adds Chassis functionality. Attributes: Name Type Description predict function MLflow pyfunc compatible predict function. Will wrap user-provided function which takes two arguments: model_input (bytes) and model_context (dict). chassis_build_url str The build url for the Chassis API. Source code in chassisml_sdk/chassisml/chassisml.py class ChassisModel ( mlflow . pyfunc . PythonModel ): \"\"\"The Chassis Model object. This class inherits from `mlflow.pyfunc.PythonModel` and adds Chassis functionality. Attributes: predict (function): MLflow pyfunc compatible predict function. Will wrap user-provided function which takes two arguments: model_input (bytes) and model_context (dict). chassis_build_url (str): The build url for the Chassis API. \"\"\" def __init__ ( self , process_fn , batch_process_fn , batch_size , chassis_base_url ): if process_fn and batch_process_fn : if not batch_size : raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) self . predict = self . _gen_predict_method ( process_fn ) self . batch_predict = self . _gen_predict_method ( batch_process_fn , batch = True ) self . batch_input = True self . batch_size = batch_size elif process_fn and not batch_process_fn : self . predict = self . _gen_predict_method ( process_fn ) self . batch_input = False self . batch_size = None elif batch_process_fn and not process_fn : if not batch_size : raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) self . predict = self . _gen_predict_method ( batch_process_fn , batch_to_single = True ) self . batch_predict = self . _gen_predict_method ( batch_process_fn , batch = True ) self . batch_input = True self . batch_size = batch_size else : raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) self . chassis_build_url = urllib . parse . urljoin ( chassis_base_url , routes [ 'build' ]) self . chassis_test_url = urllib . parse . urljoin ( chassis_base_url , routes [ 'test' ]) def _gen_predict_method ( self , process_fn , batch = False , batch_to_single = False ): def predict ( _ , model_input ): if batch_to_single : output = process_fn ([ model_input ])[ 0 ] else : output = process_fn ( model_input ) if batch : return [ json . dumps ( out , separators = ( \",\" , \":\" ), cls = NumpyEncoder ) . encode () for out in output ] else : return json . dumps ( output , separators = ( \",\" , \":\" ), cls = NumpyEncoder ) . encode () return predict def test ( self , test_input ): ''' Runs a sample inference test on a single input on chassis model locally Args: test_input (Union[str, bytes, BufferedReader]): Single sample input data to test model Returns: bytes: raw model predictions returned by `process_fn` method Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test(sample_filepath) ``` ''' if isinstance ( test_input , _io . BufferedReader ): result = self . predict ( None , test_input . read ()) elif isinstance ( test_input , bytes ): result = self . predict ( None , test_input ) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): result = self . predict ( None , open ( test_input , 'rb' ) . read ()) else : result = self . predict ( None , bytes ( test_input , encoding = 'utf8' )) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return result def test_batch ( self , test_input ): ''' Takes a single input file, creates a batch of size `batch_size` defined in `ChassisModel.create_model`, and runs a batch job against chassis model locally if `batch_process_fn` is defined. Args: test_input (Union[str, bytes, BufferedReader]): Batch of sample input data to test model Returns: bytes: raw model predictions returned by `batch_process_fn` method Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_input = sample_filepath = './sample_data.json' results = chassis_model.test_batch(sample_filepath) ``` ''' if not self . batch_input : raise NotImplementedError ( \"Batch inference not implemented.\" ) if hasattr ( self , 'batch_predict' ): batch_method = self . batch_predict else : batch_method = self . predict if isinstance ( test_input , _io . BufferedReader ): results = batch_method ( None ,[ test_input . read () for _ in range ( self . batch_size )]) elif isinstance ( test_input , bytes ): results = batch_method ( None ,[ test_input for _ in range ( self . batch_size )]) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): results = batch_method ( None ,[ open ( test_input , 'rb' ) . read () for _ in range ( self . batch_size )]) else : results = batch_method ( None ,[ bytes ( test_input , encoding = 'utf8' ) for _ in range ( self . batch_size )]) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return results def test_env ( self , test_input_path , conda_env = None , fix_env = True ): ''' Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. **NOTE**: This method is not available in the publicly-hosted service. Args: test_input_path (str): Filepath to sample input data conda_env (str): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build Returns: Dict: raw model predictions returned by `process_fn` or `batch_process_fn` run from within chassis service Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test_env(sample_filepath) ``` ''' model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as model_f , \\ open ( test_input_path , 'rb' ) as test_input_f : files = [ ( 'sample_input' , test_input_f ), ( 'model' , model_f ) ] print ( 'Starting test job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_test_url , files = files ) res . raise_for_status () print ( 'Ok!' ) shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () def save ( self , path , conda_env = None , overwrite = False , fix_env = True , gpu = False , arm64 = False ): ''' Saves a copy of ChassisModel to local filepath Args: path (str): Filepath to save chassis model as local MLflow model conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment overwrite (bool): If True, overwrites existing contents of `path` parameter gpu (bool): If True and `arm64` is True, modifies dependencies as needed by chassis for ARM64+GPU support arm64 (bool): If True and `gpu` is True, modifies dependencies as needed by chassis for ARM64+GPU support Returns: None: This method does not return an object Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) chassis_model.save(\"local_model_directory\") ``` ''' if overwrite and os . path . exists ( path ): shutil . rmtree ( path ) mlflow . pyfunc . save_model ( path = path , python_model = self , conda_env = conda_env ) if fix_env : fix_dependencies ( path ) if arm64 and gpu : fix_dependencies_arm_gpu ( path ) print ( \"Chassis model saved.\" ) def publish ( self , model_name , model_version , registry_user , registry_pass , conda_env = None , fix_env = True , gpu = False , arm64 = False , modzy_sample_input_path = None , modzy_api_key = None , modzy_url = None , modzy_model_id = None ): ''' Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Args: model_name (str): Model name that serves as model's name in Modzy and docker registry repository name. **Note**: this string cannot include punctuation model_version (str): Version of model registry_user (str): Docker registry username registry_pass (str): Docker registry password conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build gpu (bool): If True, builds container image that runs on GPU hardware arm64 (bool): If True, builds container image that runs on ARM64 architecture modzy_sample_input_path (str): Filepath to sample input data. Required to deploy model to Modzy modzy_api_key (str): Valid Modzy API Key modzy_url (str): Valid Modzy instance URL, example: https://my.modzy.com modzy_model_id (str): Existing Modzy model identifier, if requesting new version of existing model instead of new model Returns: Dict: Response to Chassis `/build` endpoint Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) ``` ''' if ( modzy_sample_input_path or modzy_api_key or modzy_url ) and not \\ ( modzy_sample_input_path and modzy_api_key and modzy_url ): raise ValueError ( '\"modzy_sample_input_path\", \"modzy_api_key\" and \"modzy_url\" must all be provided to publish to Modzy.' ) try : model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) if arm64 : warnings . warn ( \"ARM64 support is experimental, KServe currently not supported and builds may take a while or fail depending on your required dependencies\" ) if gpu : warnings . warn ( \"ARM64+GPU support tested on Nvidia Jetson Nano\" ) fix_dependencies_arm_gpu ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) image_name = \"-\" . join ( model_name . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () . split ()) image_data = { 'name' : \" {} / {} \" . format ( registry_user , \" {} : {} \" . format ( image_name , model_version )), 'model_name' : model_name , 'model_path' : tmppath , 'registry_auth' : base64 . b64encode ( \" {} : {} \" . format ( registry_user , registry_pass ) . encode ( \"utf-8\" )) . decode ( \"utf-8\" ), 'publish' : True , 'gpu' : gpu , 'arm64' : arm64 } if modzy_sample_input_path and modzy_api_key and check_modzy_url ( modzy_url ): modzy_metadata_path = os . path . join ( tmppath , MODZY_YAML_NAME ) modzy_data = { 'metadata_path' : modzy_metadata_path , 'sample_input_path' : modzy_sample_input_path , 'deploy' : True , 'api_key' : modzy_api_key , 'modzy_model_id' : modzy_model_id , 'modzy_url' : modzy_url if not modzy_url . endswith ( '/api' ) else modzy_url [: - 4 ] } write_modzy_yaml ( model_name , model_version , modzy_metadata_path , batch_size = self . batch_size , gpu = gpu ) else : modzy_data = {} with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as f : files = [ ( 'image_data' , json . dumps ( image_data )), ( 'modzy_data' , json . dumps ( modzy_data )), ( 'model' , f ) ] file_pointers = [] for key , file_key in [( 'metadata_path' , 'modzy_metadata_data' ), ( 'sample_input_path' , 'modzy_sample_input_data' )]: value = modzy_data . get ( key ) if value : fp = open ( value , 'rb' ) file_pointers . append ( fp ) files . append (( file_key , fp )) print ( 'Starting build job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_build_url , files = files ) res . raise_for_status () print ( 'Ok!' ) for fp in file_pointers : fp . close () shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () except Exception as e : if os . path . exists ( tmppath ): shutil . rmtree ( tmppath ) if os . path . exists ( model_directory ): shutil . rmtree ( model_directory ) raise ( e ) publish ( self , model_name , model_version , registry_user , registry_pass , conda_env = None , fix_env = True , gpu = False , arm64 = False , modzy_sample_input_path = None , modzy_api_key = None , modzy_url = None , modzy_model_id = None ) Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Parameters: Name Type Description Default model_name str Model name that serves as model's name in Modzy and docker registry repository name. Note : this string cannot include punctuation required model_version str Version of model required registry_user str Docker registry username required registry_pass str Docker registry password required conda_env Union[str, dict] Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None fix_env bool Modifies conda or pip-installable packages into list of dependencies to be installed during the container build True gpu bool If True, builds container image that runs on GPU hardware False arm64 bool If True, builds container image that runs on ARM64 architecture False modzy_sample_input_path str Filepath to sample input data. Required to deploy model to Modzy None modzy_api_key str Valid Modzy API Key None modzy_url str Valid Modzy instance URL, example: https://my.modzy.com None modzy_model_id str Existing Modzy model identifier, if requesting new version of existing model instead of new model None Returns: Type Description Dict Response to Chassis /build endpoint Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) Source code in chassisml_sdk/chassisml/chassisml.py def publish ( self , model_name , model_version , registry_user , registry_pass , conda_env = None , fix_env = True , gpu = False , arm64 = False , modzy_sample_input_path = None , modzy_api_key = None , modzy_url = None , modzy_model_id = None ): ''' Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Args: model_name (str): Model name that serves as model's name in Modzy and docker registry repository name. **Note**: this string cannot include punctuation model_version (str): Version of model registry_user (str): Docker registry username registry_pass (str): Docker registry password conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build gpu (bool): If True, builds container image that runs on GPU hardware arm64 (bool): If True, builds container image that runs on ARM64 architecture modzy_sample_input_path (str): Filepath to sample input data. Required to deploy model to Modzy modzy_api_key (str): Valid Modzy API Key modzy_url (str): Valid Modzy instance URL, example: https://my.modzy.com modzy_model_id (str): Existing Modzy model identifier, if requesting new version of existing model instead of new model Returns: Dict: Response to Chassis `/build` endpoint Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) ``` ''' if ( modzy_sample_input_path or modzy_api_key or modzy_url ) and not \\ ( modzy_sample_input_path and modzy_api_key and modzy_url ): raise ValueError ( '\"modzy_sample_input_path\", \"modzy_api_key\" and \"modzy_url\" must all be provided to publish to Modzy.' ) try : model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) if arm64 : warnings . warn ( \"ARM64 support is experimental, KServe currently not supported and builds may take a while or fail depending on your required dependencies\" ) if gpu : warnings . warn ( \"ARM64+GPU support tested on Nvidia Jetson Nano\" ) fix_dependencies_arm_gpu ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) image_name = \"-\" . join ( model_name . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () . split ()) image_data = { 'name' : \" {} / {} \" . format ( registry_user , \" {} : {} \" . format ( image_name , model_version )), 'model_name' : model_name , 'model_path' : tmppath , 'registry_auth' : base64 . b64encode ( \" {} : {} \" . format ( registry_user , registry_pass ) . encode ( \"utf-8\" )) . decode ( \"utf-8\" ), 'publish' : True , 'gpu' : gpu , 'arm64' : arm64 } if modzy_sample_input_path and modzy_api_key and check_modzy_url ( modzy_url ): modzy_metadata_path = os . path . join ( tmppath , MODZY_YAML_NAME ) modzy_data = { 'metadata_path' : modzy_metadata_path , 'sample_input_path' : modzy_sample_input_path , 'deploy' : True , 'api_key' : modzy_api_key , 'modzy_model_id' : modzy_model_id , 'modzy_url' : modzy_url if not modzy_url . endswith ( '/api' ) else modzy_url [: - 4 ] } write_modzy_yaml ( model_name , model_version , modzy_metadata_path , batch_size = self . batch_size , gpu = gpu ) else : modzy_data = {} with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as f : files = [ ( 'image_data' , json . dumps ( image_data )), ( 'modzy_data' , json . dumps ( modzy_data )), ( 'model' , f ) ] file_pointers = [] for key , file_key in [( 'metadata_path' , 'modzy_metadata_data' ), ( 'sample_input_path' , 'modzy_sample_input_data' )]: value = modzy_data . get ( key ) if value : fp = open ( value , 'rb' ) file_pointers . append ( fp ) files . append (( file_key , fp )) print ( 'Starting build job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_build_url , files = files ) res . raise_for_status () print ( 'Ok!' ) for fp in file_pointers : fp . close () shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () except Exception as e : if os . path . exists ( tmppath ): shutil . rmtree ( tmppath ) if os . path . exists ( model_directory ): shutil . rmtree ( model_directory ) raise ( e ) save ( self , path , conda_env = None , overwrite = False , fix_env = True , gpu = False , arm64 = False ) Saves a copy of ChassisModel to local filepath Parameters: Name Type Description Default path str Filepath to save chassis model as local MLflow model required conda_env Union[str, dict] Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None overwrite bool If True, overwrites existing contents of path parameter False gpu bool If True and arm64 is True, modifies dependencies as needed by chassis for ARM64+GPU support False arm64 bool If True and gpu is True, modifies dependencies as needed by chassis for ARM64+GPU support False Returns: Type Description None This method does not return an object Examples: chassis_model = chassis_client . create_model ( process_fn = process ) chassis_model . save ( \"local_model_directory\" ) Source code in chassisml_sdk/chassisml/chassisml.py def save ( self , path , conda_env = None , overwrite = False , fix_env = True , gpu = False , arm64 = False ): ''' Saves a copy of ChassisModel to local filepath Args: path (str): Filepath to save chassis model as local MLflow model conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment overwrite (bool): If True, overwrites existing contents of `path` parameter gpu (bool): If True and `arm64` is True, modifies dependencies as needed by chassis for ARM64+GPU support arm64 (bool): If True and `gpu` is True, modifies dependencies as needed by chassis for ARM64+GPU support Returns: None: This method does not return an object Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) chassis_model.save(\"local_model_directory\") ``` ''' if overwrite and os . path . exists ( path ): shutil . rmtree ( path ) mlflow . pyfunc . save_model ( path = path , python_model = self , conda_env = conda_env ) if fix_env : fix_dependencies ( path ) if arm64 and gpu : fix_dependencies_arm_gpu ( path ) print ( \"Chassis model saved.\" ) test ( self , test_input ) Runs a sample inference test on a single input on chassis model locally Parameters: Name Type Description Default test_input Union[str, bytes, BufferedReader] Single sample input data to test model required Returns: Type Description bytes raw model predictions returned by process_fn method Examples: chassis_model = chassis_client . create_model ( process_fn = process ) sample_filepath = './sample_data.json' results = chassis_model . test ( sample_filepath ) Source code in chassisml_sdk/chassisml/chassisml.py def test ( self , test_input ): ''' Runs a sample inference test on a single input on chassis model locally Args: test_input (Union[str, bytes, BufferedReader]): Single sample input data to test model Returns: bytes: raw model predictions returned by `process_fn` method Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test(sample_filepath) ``` ''' if isinstance ( test_input , _io . BufferedReader ): result = self . predict ( None , test_input . read ()) elif isinstance ( test_input , bytes ): result = self . predict ( None , test_input ) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): result = self . predict ( None , open ( test_input , 'rb' ) . read ()) else : result = self . predict ( None , bytes ( test_input , encoding = 'utf8' )) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return result test_batch ( self , test_input ) Takes a single input file, creates a batch of size batch_size defined in ChassisModel.create_model , and runs a batch job against chassis model locally if batch_process_fn is defined. Parameters: Name Type Description Default test_input Union[str, bytes, BufferedReader] Batch of sample input data to test model required Returns: Type Description bytes raw model predictions returned by batch_process_fn method Examples: chassis_model = chassis_client . create_model ( process_fn = process ) sample_input = sample_filepath = './sample_data.json' results = chassis_model . test_batch ( sample_filepath ) Source code in chassisml_sdk/chassisml/chassisml.py def test_batch ( self , test_input ): ''' Takes a single input file, creates a batch of size `batch_size` defined in `ChassisModel.create_model`, and runs a batch job against chassis model locally if `batch_process_fn` is defined. Args: test_input (Union[str, bytes, BufferedReader]): Batch of sample input data to test model Returns: bytes: raw model predictions returned by `batch_process_fn` method Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_input = sample_filepath = './sample_data.json' results = chassis_model.test_batch(sample_filepath) ``` ''' if not self . batch_input : raise NotImplementedError ( \"Batch inference not implemented.\" ) if hasattr ( self , 'batch_predict' ): batch_method = self . batch_predict else : batch_method = self . predict if isinstance ( test_input , _io . BufferedReader ): results = batch_method ( None ,[ test_input . read () for _ in range ( self . batch_size )]) elif isinstance ( test_input , bytes ): results = batch_method ( None ,[ test_input for _ in range ( self . batch_size )]) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): results = batch_method ( None ,[ open ( test_input , 'rb' ) . read () for _ in range ( self . batch_size )]) else : results = batch_method ( None ,[ bytes ( test_input , encoding = 'utf8' ) for _ in range ( self . batch_size )]) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return results test_env ( self , test_input_path , conda_env = None , fix_env = True ) Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. NOTE : This method is not available in the publicly-hosted service. Parameters: Name Type Description Default test_input_path str Filepath to sample input data required conda_env str Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None fix_env bool Modifies conda or pip-installable packages into list of dependencies to be installed during the container build True Returns: Type Description Dict raw model predictions returned by process_fn or batch_process_fn run from within chassis service Examples: chassis_model = chassis_client . create_model ( process_fn = process ) sample_filepath = './sample_data.json' results = chassis_model . test_env ( sample_filepath ) Source code in chassisml_sdk/chassisml/chassisml.py def test_env ( self , test_input_path , conda_env = None , fix_env = True ): ''' Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. **NOTE**: This method is not available in the publicly-hosted service. Args: test_input_path (str): Filepath to sample input data conda_env (str): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build Returns: Dict: raw model predictions returned by `process_fn` or `batch_process_fn` run from within chassis service Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test_env(sample_filepath) ``` ''' model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as model_f , \\ open ( test_input_path , 'rb' ) as test_input_f : files = [ ( 'sample_input' , test_input_f ), ( 'model' , model_f ) ] print ( 'Starting test job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_test_url , files = files ) res . raise_for_status () print ( 'Ok!' ) shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json ()","title":"Python SDK"},{"location":"chassisml_sdk-reference/#chassisml-python-sdk","text":"","title":"Chassisml Python SDK"},{"location":"chassisml_sdk-reference/#introduction","text":"The Chassisml Python SDK offers convenience functions that interact with the Chassisml service to automate the containerization and deployment of models to your preferred model serving platform. It is organized into two classes: ChassisClient : interacts with the Chassis routes that build a user's container from their model artifacts ChasissModel : creates a chassis compliant model out of a few lines of Python code from user supplied model artifacts First, install the Chassisml SDK to get started: pip install chassisml To import the library into your editor: import chassisml","title":"Introduction"},{"location":"chassisml_sdk-reference/#usage","text":"","title":"Usage"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient","text":"The Chassis Client object. This class is used to interact with the Kaniko service. Attributes: Name Type Description base_url str The base url for the API. Source code in chassisml_sdk/chassisml/chassisml.py class ChassisClient : \"\"\"The Chassis Client object. This class is used to interact with the Kaniko service. Attributes: base_url (str): The base url for the API. \"\"\" def __init__ ( self , base_url = 'http://localhost:5000' ): self . base_url = base_url def get_job_status ( self , job_id ): ''' Checks the status of a chassis job Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method Returns: Dict: JSON Chassis job status Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') job_status = chassis_client.get_job_status(job_id) ``` ''' route = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } ' res = requests . get ( route ) data = res . json () return data def block_until_complete ( self , job_id , timeout = None , poll_interval = 5 ): ''' Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method timeout (int): Timeout threshold in seconds poll_intervall (int): Amount of time to wait in between API polls to check status of job Returns: Dict: final job status returned by `ChassisClient.block_until_complete` method Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') final_status = chassis_client.block_until_complete(job_id) ``` ''' endby = time . time () + timeout if ( timeout is not None ) else None while True : status = self . get_job_status ( job_id ) if status [ 'status' ][ 'succeeded' ] or status [ 'status' ][ 'failed' ]: return status if ( endby is not None ) and ( time . time () > endby - poll_interval ): print ( 'Timed out before completion.' ) return False time . sleep ( poll_interval ) def download_tar ( self , job_id , output_filename ): ''' Downloads container image as tar archive **NOTE**: This method is not available in the publicly-hosted service. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method output_filename (str): Local output filepath to save container image Returns: None: This method does not return an object Examples: ```python # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id) chassis_client.download_tar(job_id, \"./chassis-model.tar\") ``` ''' url = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } /download-tar' r = requests . get ( url ) if r . status_code == 200 : with open ( output_filename , 'wb' ) as f : f . write ( r . content ) else : print ( f 'Error download tar: { r . text } ' ) def test_OMI_compliance ( self , image_id = None ): ''' Tests a local image for compliance with the [Open Model Interface Specification](https://openmodel.ml/spec/) Args: image_id (str): image id of a local docker container. e.g. `dockerusername/repositoryname:tag` Returns: tuple(bool, str): Tuple containing compliance boolean (`True` if compliant, `False` if not) and corresponding string containing concatenation of any logs. Examples: ```python # test a local docker image OMI_test, logs = chassis_client.test_OMI_compliance(image_id) if OMI_test: print(\"OMI compliance test passed\") else: print(\"OMI compliance test failed\",logs) ``` ''' rValue = ( False , \"Nothing Initialized\" ) try : checkObject = OMI_check ( image_id = image_id ) if checkObject . client is None : raise TypeError ( \"The Docker Client couldn't be initialized. Is Docker installed?\" ) image_check = checkObject . validate_image () if \"Failure\" in image_check : raise ValueError ( image_check ) container_start = checkObject . start_container () if \"Failure\" in container_start : raise ValueError ( container_start ) gRPC_check = checkObject . validate_gRPC () if \"Failure\" in gRPC_check : raise ValueError ( gRPC_check ) clean_up = checkObject . clean_up () if \"Failure\" in clean_up : raise ValueError ( clean_up ) rValue = ( True , \" \\n \" + image_check + \" \\n \" + container_start + \" \\n \" + gRPC_check + \" \\n \" + clean_up ) except Exception as e : rValue = ( False , e ) return rValue def create_model ( self , process_fn = None , batch_process_fn = None , batch_size = None ): ''' Builds chassis model locally Args: process_fn (function): Python function that must accept a single piece of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_process_fn (function): Python function that must accept a batch of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_size (int): Maximum batch size if `batch_process_fn` is defined Returns: ChassisModel: Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this [example](https://docs.modzy.com/docs/chassis-ml). ```python # Import and normalize data X_digits, y_digits = datasets.load_digits(return_X_y=True) X_digits = X_digits / X_digits.max() n_samples = len(X_digits) # Split data into training and test sets X_train = X_digits[: int(0.9 * n_samples)] y_train = y_digits[: int(0.9 * n_samples)] X_test = X_digits[int(0.9 * n_samples) :] y_test = y_digits[int(0.9 * n_samples) :] # Train Model logistic = LogisticRegression(max_iter=1000) print( \"LogisticRegression mean accuracy score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test) ) # Save small sample input to use for testing later sample = X_test[:5].tolist() with open(\"digits_sample.json\", 'w') as out: json.dump(sample, out) # Define Process function def process(input_bytes): inputs = np.array(json.loads(input_bytes)) inference_results = logistic.predict(inputs) structured_results = [] for inference_result in inference_results: structured_output = { \"data\": { \"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]} } } structured_results.append(structured_output) return structured_results # create Chassis model chassis_model = chassis_client.create_model(process_fn=process) ``` ''' if not ( process_fn or batch_process_fn ): raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) if ( batch_process_fn and not batch_size ) or ( batch_size and not batch_process_fn ): raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) return ChassisModel ( process_fn , batch_process_fn , batch_size , self . base_url ) def run_inference ( self , input_data , container_url = \"localhost\" , host_port = 45000 ): ''' This is the method you use to submit data to a container chassis has built for inference. It assumes the container has been downloaded from dockerhub and is running somewhere you have access to. Args: input_data (json): dictionary of the form {\"input\": <binary respresentaion of your data>} container_url (str): URL where container is running host_port (int): host port that forwards to container's grpc server port Returns: return_value (str): Success -> results from model processing as specified in the process function. Failure -> Error codes from processing errors. All errors should container the word \"Error.\" Examples: ```python # assume that the container is running locally, and that it was started with this docker command # docker run -it -p 5001:45000 <docker_uname>/<container_name>:<tag_id> from chassisml_sdk.chassisml import chassisml client = chassisml.ChassisClient() input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"} input_list = [input_data for _ in range(30)] print(\"single input\") print(client.run_inference(input_data, container_url=\"localhost\", host_port=5001)) print(\"multi inputs\") results = client.run_inference(input_list, container_url=\"localhost\", host_port=5001) for x in results: print(x) ``` ''' model_client . override_server_URL ( container_url , host_port ) return model_client . run ( input_data ) def docker_infer ( self , image_id , input_data , container_url = \"localhost\" , host_port = 5001 , container_port = None , timeout = 20 , clean_up = True , pull_container = False ): ''' Runs inference on an OMI compliant container. This method checks to see if a container is running and if not starts it. The method then runs inference against the input_data with the model in the container, and optionally shuts down the container. Args: image_id (string): the name of an OMI container image usually of the form <docker_uname>/<container_name>:<tag_id> input_data (json): dictionary of the form {\"input\": <binary respresentaion of your data>} container_url (str): URL where container is running host_port (int): host port that forwards to container's grpc server port container_port (str): container port the grpc server listens to timeout (int): number of seconds to wait for gRPC server to spin up clean_up (bool): whether or not to stop and remove the container after inference pull_container (bool): if True pulls missing container from repo Returns: return_value (str): Success -> model output as defined in the process function Failure -> Error message if any success criteria is missing. Example: ```python host_port = 5002 client = chassisml.ChassisClient() input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"} input_list = [input_data for _ in range(30)] print(\"single input\") print(client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_data, container_url=\"localhost\", host_port=host_port, clean_up=False, pull_container=True)) print(\"multi inputs\") results = client.run_inference(input_list, container_url=\"localhost\", host_port=host_port) results = client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_list, container_url=\"localhost\", host_port=host_port) for x in results: print(x) ``` ''' try : container_id = docker_start ( image_id , host_port = host_port , container_port = container_port , timeout = timeout , pull_container = pull_container ) if \"Error\" in container_id : raise ValueError ( \"container_id wrong\" ) return_value = self . run_inference ( input_data , container_url = container_url , host_port = host_port ) if clean_up : docker_clean_up ( container_id ) except Exception as e : return_value = { \"results\" : [ \"Error \" + str ( e )]} return return_value","title":"ChassisClient"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.block_until_complete","text":"Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required timeout int Timeout threshold in seconds None poll_intervall int Amount of time to wait in between API polls to check status of job required Returns: Type Description Dict final job status returned by ChassisClient.block_until_complete method Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) Source code in chassisml_sdk/chassisml/chassisml.py def block_until_complete ( self , job_id , timeout = None , poll_interval = 5 ): ''' Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method timeout (int): Timeout threshold in seconds poll_intervall (int): Amount of time to wait in between API polls to check status of job Returns: Dict: final job status returned by `ChassisClient.block_until_complete` method Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') final_status = chassis_client.block_until_complete(job_id) ``` ''' endby = time . time () + timeout if ( timeout is not None ) else None while True : status = self . get_job_status ( job_id ) if status [ 'status' ][ 'succeeded' ] or status [ 'status' ][ 'failed' ]: return status if ( endby is not None ) and ( time . time () > endby - poll_interval ): print ( 'Timed out before completion.' ) return False time . sleep ( poll_interval )","title":"block_until_complete()"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.create_model","text":"Builds chassis model locally Parameters: Name Type Description Default process_fn function Python function that must accept a single piece of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the process method None batch_process_fn function Python function that must accept a batch of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the process method None batch_size int Maximum batch size if batch_process_fn is defined None Returns: Type Description ChassisModel Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this example . # Import and normalize data X_digits , y_digits = datasets . load_digits ( return_X_y = True ) X_digits = X_digits / X_digits . max () n_samples = len ( X_digits ) # Split data into training and test sets X_train = X_digits [: int ( 0.9 * n_samples )] y_train = y_digits [: int ( 0.9 * n_samples )] X_test = X_digits [ int ( 0.9 * n_samples ) :] y_test = y_digits [ int ( 0.9 * n_samples ) :] # Train Model logistic = LogisticRegression ( max_iter = 1000 ) print ( \"LogisticRegression mean accuracy score: %f \" % logistic . fit ( X_train , y_train ) . score ( X_test , y_test ) ) # Save small sample input to use for testing later sample = X_test [: 5 ] . tolist () with open ( \"digits_sample.json\" , 'w' ) as out : json . dump ( sample , out ) # Define Process function def process ( input_bytes ): inputs = np . array ( json . loads ( input_bytes )) inference_results = logistic . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results # create Chassis model chassis_model = chassis_client . create_model ( process_fn = process ) Source code in chassisml_sdk/chassisml/chassisml.py def create_model ( self , process_fn = None , batch_process_fn = None , batch_size = None ): ''' Builds chassis model locally Args: process_fn (function): Python function that must accept a single piece of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_process_fn (function): Python function that must accept a batch of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the `process` method batch_size (int): Maximum batch size if `batch_process_fn` is defined Returns: ChassisModel: Chassis Model object that can be tested locally and published to a Docker Registry Examples: The following snippet was taken from this [example](https://docs.modzy.com/docs/chassis-ml). ```python # Import and normalize data X_digits, y_digits = datasets.load_digits(return_X_y=True) X_digits = X_digits / X_digits.max() n_samples = len(X_digits) # Split data into training and test sets X_train = X_digits[: int(0.9 * n_samples)] y_train = y_digits[: int(0.9 * n_samples)] X_test = X_digits[int(0.9 * n_samples) :] y_test = y_digits[int(0.9 * n_samples) :] # Train Model logistic = LogisticRegression(max_iter=1000) print( \"LogisticRegression mean accuracy score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test) ) # Save small sample input to use for testing later sample = X_test[:5].tolist() with open(\"digits_sample.json\", 'w') as out: json.dump(sample, out) # Define Process function def process(input_bytes): inputs = np.array(json.loads(input_bytes)) inference_results = logistic.predict(inputs) structured_results = [] for inference_result in inference_results: structured_output = { \"data\": { \"result\": {\"classPredictions\": [{\"class\": str(inference_result), \"score\": str(1)}]} } } structured_results.append(structured_output) return structured_results # create Chassis model chassis_model = chassis_client.create_model(process_fn=process) ``` ''' if not ( process_fn or batch_process_fn ): raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) if ( batch_process_fn and not batch_size ) or ( batch_size and not batch_process_fn ): raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) return ChassisModel ( process_fn , batch_process_fn , batch_size , self . base_url )","title":"create_model()"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.docker_infer","text":"Runs inference on an OMI compliant container. This method checks to see if a container is running and if not starts it. The method then runs inference against the input_data with the model in the container, and optionally shuts down the container. Parameters: Name Type Description Default image_id string the name of an OMI container image usually of the form / : required input_data json dictionary of the form {\"input\": } required container_url str URL where container is running 'localhost' host_port int host port that forwards to container's grpc server port 5001 container_port str container port the grpc server listens to None timeout int number of seconds to wait for gRPC server to spin up 20 clean_up bool whether or not to stop and remove the container after inference True pull_container bool if True pulls missing container from repo False Returns: Type Description return_value (str) Success -> model output as defined in the process function Failure -> Error message if any success criteria is missing. Examples: host_port = 5002 client = chassisml . ChassisClient () input_data = { \"input\" : b \"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\" } input_list = [ input_data for _ in range ( 30 )] print ( \"single input\" ) print ( client . docker_infer ( image_id = \"claytondavisms/sklearn-digits-docker-test:0.0.7\" , input_data = input_data , container_url = \"localhost\" , host_port = host_port , clean_up = False , pull_container = True )) print ( \"multi inputs\" ) results = client . run_inference ( input_list , container_url = \"localhost\" , host_port = host_port ) results = client . docker_infer ( image_id = \"claytondavisms/sklearn-digits-docker-test:0.0.7\" , input_data = input_list , container_url = \"localhost\" , host_port = host_port ) for x in results : print ( x ) Source code in chassisml_sdk/chassisml/chassisml.py def docker_infer ( self , image_id , input_data , container_url = \"localhost\" , host_port = 5001 , container_port = None , timeout = 20 , clean_up = True , pull_container = False ): ''' Runs inference on an OMI compliant container. This method checks to see if a container is running and if not starts it. The method then runs inference against the input_data with the model in the container, and optionally shuts down the container. Args: image_id (string): the name of an OMI container image usually of the form <docker_uname>/<container_name>:<tag_id> input_data (json): dictionary of the form {\"input\": <binary respresentaion of your data>} container_url (str): URL where container is running host_port (int): host port that forwards to container's grpc server port container_port (str): container port the grpc server listens to timeout (int): number of seconds to wait for gRPC server to spin up clean_up (bool): whether or not to stop and remove the container after inference pull_container (bool): if True pulls missing container from repo Returns: return_value (str): Success -> model output as defined in the process function Failure -> Error message if any success criteria is missing. Example: ```python host_port = 5002 client = chassisml.ChassisClient() input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"} input_list = [input_data for _ in range(30)] print(\"single input\") print(client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_data, container_url=\"localhost\", host_port=host_port, clean_up=False, pull_container=True)) print(\"multi inputs\") results = client.run_inference(input_list, container_url=\"localhost\", host_port=host_port) results = client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_list, container_url=\"localhost\", host_port=host_port) for x in results: print(x) ``` ''' try : container_id = docker_start ( image_id , host_port = host_port , container_port = container_port , timeout = timeout , pull_container = pull_container ) if \"Error\" in container_id : raise ValueError ( \"container_id wrong\" ) return_value = self . run_inference ( input_data , container_url = container_url , host_port = host_port ) if clean_up : docker_clean_up ( container_id ) except Exception as e : return_value = { \"results\" : [ \"Error \" + str ( e )]} return return_value","title":"docker_infer()"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.download_tar","text":"Downloads container image as tar archive NOTE : This method is not available in the publicly-hosted service. Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required output_filename str Local output filepath to save container image required Returns: Type Description None This method does not return an object Examples: # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id) chassis_client . download_tar ( job_id , \"./chassis-model.tar\" ) Source code in chassisml_sdk/chassisml/chassisml.py def download_tar ( self , job_id , output_filename ): ''' Downloads container image as tar archive **NOTE**: This method is not available in the publicly-hosted service. Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method output_filename (str): Local output filepath to save container image Returns: None: This method does not return an object Examples: ```python # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id) chassis_client.download_tar(job_id, \"./chassis-model.tar\") ``` ''' url = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } /download-tar' r = requests . get ( url ) if r . status_code == 200 : with open ( output_filename , 'wb' ) as f : f . write ( r . content ) else : print ( f 'Error download tar: { r . text } ' )","title":"download_tar()"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.get_job_status","text":"Checks the status of a chassis job Parameters: Name Type Description Default job_id str Chassis job identifier generated from ChassisModel.publish method required Returns: Type Description Dict JSON Chassis job status Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) job_status = chassis_client . get_job_status ( job_id ) Source code in chassisml_sdk/chassisml/chassisml.py def get_job_status ( self , job_id ): ''' Checks the status of a chassis job Args: job_id (str): Chassis job identifier generated from `ChassisModel.publish` method Returns: Dict: JSON Chassis job status Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) job_id = response.get('job_id') job_status = chassis_client.get_job_status(job_id) ``` ''' route = f ' { urllib . parse . urljoin ( self . base_url , routes [ \"job\" ]) } / { job_id } ' res = requests . get ( route ) data = res . json () return data","title":"get_job_status()"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.run_inference","text":"This is the method you use to submit data to a container chassis has built for inference. It assumes the container has been downloaded from dockerhub and is running somewhere you have access to. Parameters: Name Type Description Default input_data json dictionary of the form {\"input\": } required container_url str URL where container is running 'localhost' host_port int host port that forwards to container's grpc server port 45000 Returns: Type Description return_value (str) Success -> results from model processing as specified in the process function. Failure -> Error codes from processing errors. All errors should container the word \"Error.\" Examples: # assume that the container is running locally, and that it was started with this docker command # docker run -it -p 5001:45000 <docker_uname>/<container_name>:<tag_id> from chassisml_sdk.chassisml import chassisml client = chassisml . ChassisClient () input_data = { \"input\" : b \"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\" } input_list = [ input_data for _ in range ( 30 )] print ( \"single input\" ) print ( client . run_inference ( input_data , container_url = \"localhost\" , host_port = 5001 )) print ( \"multi inputs\" ) results = client . run_inference ( input_list , container_url = \"localhost\" , host_port = 5001 ) for x in results : print ( x ) Source code in chassisml_sdk/chassisml/chassisml.py def run_inference ( self , input_data , container_url = \"localhost\" , host_port = 45000 ): ''' This is the method you use to submit data to a container chassis has built for inference. It assumes the container has been downloaded from dockerhub and is running somewhere you have access to. Args: input_data (json): dictionary of the form {\"input\": <binary respresentaion of your data>} container_url (str): URL where container is running host_port (int): host port that forwards to container's grpc server port Returns: return_value (str): Success -> results from model processing as specified in the process function. Failure -> Error codes from processing errors. All errors should container the word \"Error.\" Examples: ```python # assume that the container is running locally, and that it was started with this docker command # docker run -it -p 5001:45000 <docker_uname>/<container_name>:<tag_id> from chassisml_sdk.chassisml import chassisml client = chassisml.ChassisClient() input_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"} input_list = [input_data for _ in range(30)] print(\"single input\") print(client.run_inference(input_data, container_url=\"localhost\", host_port=5001)) print(\"multi inputs\") results = client.run_inference(input_list, container_url=\"localhost\", host_port=5001) for x in results: print(x) ``` ''' model_client . override_server_URL ( container_url , host_port ) return model_client . run ( input_data )","title":"run_inference()"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisClient.test_OMI_compliance","text":"Tests a local image for compliance with the Open Model Interface Specification Parameters: Name Type Description Default image_id str image id of a local docker container. e.g. dockerusername/repositoryname:tag None Returns: Type Description tuple(bool, str) Tuple containing compliance boolean ( True if compliant, False if not) and corresponding string containing concatenation of any logs. Examples: # test a local docker image OMI_test , logs = chassis_client . test_OMI_compliance ( image_id ) if OMI_test : print ( \"OMI compliance test passed\" ) else : print ( \"OMI compliance test failed\" , logs ) Source code in chassisml_sdk/chassisml/chassisml.py def test_OMI_compliance ( self , image_id = None ): ''' Tests a local image for compliance with the [Open Model Interface Specification](https://openmodel.ml/spec/) Args: image_id (str): image id of a local docker container. e.g. `dockerusername/repositoryname:tag` Returns: tuple(bool, str): Tuple containing compliance boolean (`True` if compliant, `False` if not) and corresponding string containing concatenation of any logs. Examples: ```python # test a local docker image OMI_test, logs = chassis_client.test_OMI_compliance(image_id) if OMI_test: print(\"OMI compliance test passed\") else: print(\"OMI compliance test failed\",logs) ``` ''' rValue = ( False , \"Nothing Initialized\" ) try : checkObject = OMI_check ( image_id = image_id ) if checkObject . client is None : raise TypeError ( \"The Docker Client couldn't be initialized. Is Docker installed?\" ) image_check = checkObject . validate_image () if \"Failure\" in image_check : raise ValueError ( image_check ) container_start = checkObject . start_container () if \"Failure\" in container_start : raise ValueError ( container_start ) gRPC_check = checkObject . validate_gRPC () if \"Failure\" in gRPC_check : raise ValueError ( gRPC_check ) clean_up = checkObject . clean_up () if \"Failure\" in clean_up : raise ValueError ( clean_up ) rValue = ( True , \" \\n \" + image_check + \" \\n \" + container_start + \" \\n \" + gRPC_check + \" \\n \" + clean_up ) except Exception as e : rValue = ( False , e ) return rValue","title":"test_OMI_compliance()"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel","text":"The Chassis Model object. This class inherits from mlflow.pyfunc.PythonModel and adds Chassis functionality. Attributes: Name Type Description predict function MLflow pyfunc compatible predict function. Will wrap user-provided function which takes two arguments: model_input (bytes) and model_context (dict). chassis_build_url str The build url for the Chassis API. Source code in chassisml_sdk/chassisml/chassisml.py class ChassisModel ( mlflow . pyfunc . PythonModel ): \"\"\"The Chassis Model object. This class inherits from `mlflow.pyfunc.PythonModel` and adds Chassis functionality. Attributes: predict (function): MLflow pyfunc compatible predict function. Will wrap user-provided function which takes two arguments: model_input (bytes) and model_context (dict). chassis_build_url (str): The build url for the Chassis API. \"\"\" def __init__ ( self , process_fn , batch_process_fn , batch_size , chassis_base_url ): if process_fn and batch_process_fn : if not batch_size : raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) self . predict = self . _gen_predict_method ( process_fn ) self . batch_predict = self . _gen_predict_method ( batch_process_fn , batch = True ) self . batch_input = True self . batch_size = batch_size elif process_fn and not batch_process_fn : self . predict = self . _gen_predict_method ( process_fn ) self . batch_input = False self . batch_size = None elif batch_process_fn and not process_fn : if not batch_size : raise ValueError ( \"Both batch_process_fn and batch_size must be provided for batch support.\" ) self . predict = self . _gen_predict_method ( batch_process_fn , batch_to_single = True ) self . batch_predict = self . _gen_predict_method ( batch_process_fn , batch = True ) self . batch_input = True self . batch_size = batch_size else : raise ValueError ( \"At least one of process_fn or batch_process_fn must be provided.\" ) self . chassis_build_url = urllib . parse . urljoin ( chassis_base_url , routes [ 'build' ]) self . chassis_test_url = urllib . parse . urljoin ( chassis_base_url , routes [ 'test' ]) def _gen_predict_method ( self , process_fn , batch = False , batch_to_single = False ): def predict ( _ , model_input ): if batch_to_single : output = process_fn ([ model_input ])[ 0 ] else : output = process_fn ( model_input ) if batch : return [ json . dumps ( out , separators = ( \",\" , \":\" ), cls = NumpyEncoder ) . encode () for out in output ] else : return json . dumps ( output , separators = ( \",\" , \":\" ), cls = NumpyEncoder ) . encode () return predict def test ( self , test_input ): ''' Runs a sample inference test on a single input on chassis model locally Args: test_input (Union[str, bytes, BufferedReader]): Single sample input data to test model Returns: bytes: raw model predictions returned by `process_fn` method Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test(sample_filepath) ``` ''' if isinstance ( test_input , _io . BufferedReader ): result = self . predict ( None , test_input . read ()) elif isinstance ( test_input , bytes ): result = self . predict ( None , test_input ) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): result = self . predict ( None , open ( test_input , 'rb' ) . read ()) else : result = self . predict ( None , bytes ( test_input , encoding = 'utf8' )) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return result def test_batch ( self , test_input ): ''' Takes a single input file, creates a batch of size `batch_size` defined in `ChassisModel.create_model`, and runs a batch job against chassis model locally if `batch_process_fn` is defined. Args: test_input (Union[str, bytes, BufferedReader]): Batch of sample input data to test model Returns: bytes: raw model predictions returned by `batch_process_fn` method Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_input = sample_filepath = './sample_data.json' results = chassis_model.test_batch(sample_filepath) ``` ''' if not self . batch_input : raise NotImplementedError ( \"Batch inference not implemented.\" ) if hasattr ( self , 'batch_predict' ): batch_method = self . batch_predict else : batch_method = self . predict if isinstance ( test_input , _io . BufferedReader ): results = batch_method ( None ,[ test_input . read () for _ in range ( self . batch_size )]) elif isinstance ( test_input , bytes ): results = batch_method ( None ,[ test_input for _ in range ( self . batch_size )]) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): results = batch_method ( None ,[ open ( test_input , 'rb' ) . read () for _ in range ( self . batch_size )]) else : results = batch_method ( None ,[ bytes ( test_input , encoding = 'utf8' ) for _ in range ( self . batch_size )]) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return results def test_env ( self , test_input_path , conda_env = None , fix_env = True ): ''' Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. **NOTE**: This method is not available in the publicly-hosted service. Args: test_input_path (str): Filepath to sample input data conda_env (str): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build Returns: Dict: raw model predictions returned by `process_fn` or `batch_process_fn` run from within chassis service Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test_env(sample_filepath) ``` ''' model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as model_f , \\ open ( test_input_path , 'rb' ) as test_input_f : files = [ ( 'sample_input' , test_input_f ), ( 'model' , model_f ) ] print ( 'Starting test job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_test_url , files = files ) res . raise_for_status () print ( 'Ok!' ) shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () def save ( self , path , conda_env = None , overwrite = False , fix_env = True , gpu = False , arm64 = False ): ''' Saves a copy of ChassisModel to local filepath Args: path (str): Filepath to save chassis model as local MLflow model conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment overwrite (bool): If True, overwrites existing contents of `path` parameter gpu (bool): If True and `arm64` is True, modifies dependencies as needed by chassis for ARM64+GPU support arm64 (bool): If True and `gpu` is True, modifies dependencies as needed by chassis for ARM64+GPU support Returns: None: This method does not return an object Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) chassis_model.save(\"local_model_directory\") ``` ''' if overwrite and os . path . exists ( path ): shutil . rmtree ( path ) mlflow . pyfunc . save_model ( path = path , python_model = self , conda_env = conda_env ) if fix_env : fix_dependencies ( path ) if arm64 and gpu : fix_dependencies_arm_gpu ( path ) print ( \"Chassis model saved.\" ) def publish ( self , model_name , model_version , registry_user , registry_pass , conda_env = None , fix_env = True , gpu = False , arm64 = False , modzy_sample_input_path = None , modzy_api_key = None , modzy_url = None , modzy_model_id = None ): ''' Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Args: model_name (str): Model name that serves as model's name in Modzy and docker registry repository name. **Note**: this string cannot include punctuation model_version (str): Version of model registry_user (str): Docker registry username registry_pass (str): Docker registry password conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build gpu (bool): If True, builds container image that runs on GPU hardware arm64 (bool): If True, builds container image that runs on ARM64 architecture modzy_sample_input_path (str): Filepath to sample input data. Required to deploy model to Modzy modzy_api_key (str): Valid Modzy API Key modzy_url (str): Valid Modzy instance URL, example: https://my.modzy.com modzy_model_id (str): Existing Modzy model identifier, if requesting new version of existing model instead of new model Returns: Dict: Response to Chassis `/build` endpoint Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) ``` ''' if ( modzy_sample_input_path or modzy_api_key or modzy_url ) and not \\ ( modzy_sample_input_path and modzy_api_key and modzy_url ): raise ValueError ( '\"modzy_sample_input_path\", \"modzy_api_key\" and \"modzy_url\" must all be provided to publish to Modzy.' ) try : model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) if arm64 : warnings . warn ( \"ARM64 support is experimental, KServe currently not supported and builds may take a while or fail depending on your required dependencies\" ) if gpu : warnings . warn ( \"ARM64+GPU support tested on Nvidia Jetson Nano\" ) fix_dependencies_arm_gpu ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) image_name = \"-\" . join ( model_name . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () . split ()) image_data = { 'name' : \" {} / {} \" . format ( registry_user , \" {} : {} \" . format ( image_name , model_version )), 'model_name' : model_name , 'model_path' : tmppath , 'registry_auth' : base64 . b64encode ( \" {} : {} \" . format ( registry_user , registry_pass ) . encode ( \"utf-8\" )) . decode ( \"utf-8\" ), 'publish' : True , 'gpu' : gpu , 'arm64' : arm64 } if modzy_sample_input_path and modzy_api_key and check_modzy_url ( modzy_url ): modzy_metadata_path = os . path . join ( tmppath , MODZY_YAML_NAME ) modzy_data = { 'metadata_path' : modzy_metadata_path , 'sample_input_path' : modzy_sample_input_path , 'deploy' : True , 'api_key' : modzy_api_key , 'modzy_model_id' : modzy_model_id , 'modzy_url' : modzy_url if not modzy_url . endswith ( '/api' ) else modzy_url [: - 4 ] } write_modzy_yaml ( model_name , model_version , modzy_metadata_path , batch_size = self . batch_size , gpu = gpu ) else : modzy_data = {} with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as f : files = [ ( 'image_data' , json . dumps ( image_data )), ( 'modzy_data' , json . dumps ( modzy_data )), ( 'model' , f ) ] file_pointers = [] for key , file_key in [( 'metadata_path' , 'modzy_metadata_data' ), ( 'sample_input_path' , 'modzy_sample_input_data' )]: value = modzy_data . get ( key ) if value : fp = open ( value , 'rb' ) file_pointers . append ( fp ) files . append (( file_key , fp )) print ( 'Starting build job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_build_url , files = files ) res . raise_for_status () print ( 'Ok!' ) for fp in file_pointers : fp . close () shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () except Exception as e : if os . path . exists ( tmppath ): shutil . rmtree ( tmppath ) if os . path . exists ( model_directory ): shutil . rmtree ( model_directory ) raise ( e )","title":"ChassisModel"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel.publish","text":"Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Parameters: Name Type Description Default model_name str Model name that serves as model's name in Modzy and docker registry repository name. Note : this string cannot include punctuation required model_version str Version of model required registry_user str Docker registry username required registry_pass str Docker registry password required conda_env Union[str, dict] Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None fix_env bool Modifies conda or pip-installable packages into list of dependencies to be installed during the container build True gpu bool If True, builds container image that runs on GPU hardware False arm64 bool If True, builds container image that runs on ARM64 architecture False modzy_sample_input_path str Filepath to sample input data. Required to deploy model to Modzy None modzy_api_key str Valid Modzy API Key None modzy_url str Valid Modzy instance URL, example: https://my.modzy.com None modzy_model_id str Existing Modzy model identifier, if requesting new version of existing model instead of new model None Returns: Type Description Dict Response to Chassis /build endpoint Examples: # Create Chassisml model chassis_model = chassis_client . create_model ( process_fn = process ) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model . publish ( model_name = \"Chassisml Regression Model\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) Source code in chassisml_sdk/chassisml/chassisml.py def publish ( self , model_name , model_version , registry_user , registry_pass , conda_env = None , fix_env = True , gpu = False , arm64 = False , modzy_sample_input_path = None , modzy_api_key = None , modzy_url = None , modzy_model_id = None ): ''' Executes chassis job, which containerizes model, pushes container image to Docker registry, and optionally deploys model to Modzy Args: model_name (str): Model name that serves as model's name in Modzy and docker registry repository name. **Note**: this string cannot include punctuation model_version (str): Version of model registry_user (str): Docker registry username registry_pass (str): Docker registry password conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build gpu (bool): If True, builds container image that runs on GPU hardware arm64 (bool): If True, builds container image that runs on ARM64 architecture modzy_sample_input_path (str): Filepath to sample input data. Required to deploy model to Modzy modzy_api_key (str): Valid Modzy API Key modzy_url (str): Valid Modzy instance URL, example: https://my.modzy.com modzy_model_id (str): Existing Modzy model identifier, if requesting new version of existing model instead of new model Returns: Dict: Response to Chassis `/build` endpoint Examples: ```python # Create Chassisml model chassis_model = chassis_client.create_model(process_fn=process) # Define Dockerhub credentials dockerhub_user = \"user\" dockerhub_pass = \"password\" # Publish model to Docker registry response = chassis_model.publish( model_name=\"Chassisml Regression Model\", model_version=\"0.0.1\", registry_user=dockerhub_user, registry_pass=dockerhub_pass, ) ``` ''' if ( modzy_sample_input_path or modzy_api_key or modzy_url ) and not \\ ( modzy_sample_input_path and modzy_api_key and modzy_url ): raise ValueError ( '\"modzy_sample_input_path\", \"modzy_api_key\" and \"modzy_url\" must all be provided to publish to Modzy.' ) try : model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) if arm64 : warnings . warn ( \"ARM64 support is experimental, KServe currently not supported and builds may take a while or fail depending on your required dependencies\" ) if gpu : warnings . warn ( \"ARM64+GPU support tested on Nvidia Jetson Nano\" ) fix_dependencies_arm_gpu ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) image_name = \"-\" . join ( model_name . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () . split ()) image_data = { 'name' : \" {} / {} \" . format ( registry_user , \" {} : {} \" . format ( image_name , model_version )), 'model_name' : model_name , 'model_path' : tmppath , 'registry_auth' : base64 . b64encode ( \" {} : {} \" . format ( registry_user , registry_pass ) . encode ( \"utf-8\" )) . decode ( \"utf-8\" ), 'publish' : True , 'gpu' : gpu , 'arm64' : arm64 } if modzy_sample_input_path and modzy_api_key and check_modzy_url ( modzy_url ): modzy_metadata_path = os . path . join ( tmppath , MODZY_YAML_NAME ) modzy_data = { 'metadata_path' : modzy_metadata_path , 'sample_input_path' : modzy_sample_input_path , 'deploy' : True , 'api_key' : modzy_api_key , 'modzy_model_id' : modzy_model_id , 'modzy_url' : modzy_url if not modzy_url . endswith ( '/api' ) else modzy_url [: - 4 ] } write_modzy_yaml ( model_name , model_version , modzy_metadata_path , batch_size = self . batch_size , gpu = gpu ) else : modzy_data = {} with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as f : files = [ ( 'image_data' , json . dumps ( image_data )), ( 'modzy_data' , json . dumps ( modzy_data )), ( 'model' , f ) ] file_pointers = [] for key , file_key in [( 'metadata_path' , 'modzy_metadata_data' ), ( 'sample_input_path' , 'modzy_sample_input_data' )]: value = modzy_data . get ( key ) if value : fp = open ( value , 'rb' ) file_pointers . append ( fp ) files . append (( file_key , fp )) print ( 'Starting build job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_build_url , files = files ) res . raise_for_status () print ( 'Ok!' ) for fp in file_pointers : fp . close () shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json () except Exception as e : if os . path . exists ( tmppath ): shutil . rmtree ( tmppath ) if os . path . exists ( model_directory ): shutil . rmtree ( model_directory ) raise ( e )","title":"publish()"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel.save","text":"Saves a copy of ChassisModel to local filepath Parameters: Name Type Description Default path str Filepath to save chassis model as local MLflow model required conda_env Union[str, dict] Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None overwrite bool If True, overwrites existing contents of path parameter False gpu bool If True and arm64 is True, modifies dependencies as needed by chassis for ARM64+GPU support False arm64 bool If True and gpu is True, modifies dependencies as needed by chassis for ARM64+GPU support False Returns: Type Description None This method does not return an object Examples: chassis_model = chassis_client . create_model ( process_fn = process ) chassis_model . save ( \"local_model_directory\" ) Source code in chassisml_sdk/chassisml/chassisml.py def save ( self , path , conda_env = None , overwrite = False , fix_env = True , gpu = False , arm64 = False ): ''' Saves a copy of ChassisModel to local filepath Args: path (str): Filepath to save chassis model as local MLflow model conda_env (Union[str, dict]): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment overwrite (bool): If True, overwrites existing contents of `path` parameter gpu (bool): If True and `arm64` is True, modifies dependencies as needed by chassis for ARM64+GPU support arm64 (bool): If True and `gpu` is True, modifies dependencies as needed by chassis for ARM64+GPU support Returns: None: This method does not return an object Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) chassis_model.save(\"local_model_directory\") ``` ''' if overwrite and os . path . exists ( path ): shutil . rmtree ( path ) mlflow . pyfunc . save_model ( path = path , python_model = self , conda_env = conda_env ) if fix_env : fix_dependencies ( path ) if arm64 and gpu : fix_dependencies_arm_gpu ( path ) print ( \"Chassis model saved.\" )","title":"save()"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel.test","text":"Runs a sample inference test on a single input on chassis model locally Parameters: Name Type Description Default test_input Union[str, bytes, BufferedReader] Single sample input data to test model required Returns: Type Description bytes raw model predictions returned by process_fn method Examples: chassis_model = chassis_client . create_model ( process_fn = process ) sample_filepath = './sample_data.json' results = chassis_model . test ( sample_filepath ) Source code in chassisml_sdk/chassisml/chassisml.py def test ( self , test_input ): ''' Runs a sample inference test on a single input on chassis model locally Args: test_input (Union[str, bytes, BufferedReader]): Single sample input data to test model Returns: bytes: raw model predictions returned by `process_fn` method Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test(sample_filepath) ``` ''' if isinstance ( test_input , _io . BufferedReader ): result = self . predict ( None , test_input . read ()) elif isinstance ( test_input , bytes ): result = self . predict ( None , test_input ) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): result = self . predict ( None , open ( test_input , 'rb' ) . read ()) else : result = self . predict ( None , bytes ( test_input , encoding = 'utf8' )) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return result","title":"test()"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel.test_batch","text":"Takes a single input file, creates a batch of size batch_size defined in ChassisModel.create_model , and runs a batch job against chassis model locally if batch_process_fn is defined. Parameters: Name Type Description Default test_input Union[str, bytes, BufferedReader] Batch of sample input data to test model required Returns: Type Description bytes raw model predictions returned by batch_process_fn method Examples: chassis_model = chassis_client . create_model ( process_fn = process ) sample_input = sample_filepath = './sample_data.json' results = chassis_model . test_batch ( sample_filepath ) Source code in chassisml_sdk/chassisml/chassisml.py def test_batch ( self , test_input ): ''' Takes a single input file, creates a batch of size `batch_size` defined in `ChassisModel.create_model`, and runs a batch job against chassis model locally if `batch_process_fn` is defined. Args: test_input (Union[str, bytes, BufferedReader]): Batch of sample input data to test model Returns: bytes: raw model predictions returned by `batch_process_fn` method Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_input = sample_filepath = './sample_data.json' results = chassis_model.test_batch(sample_filepath) ``` ''' if not self . batch_input : raise NotImplementedError ( \"Batch inference not implemented.\" ) if hasattr ( self , 'batch_predict' ): batch_method = self . batch_predict else : batch_method = self . predict if isinstance ( test_input , _io . BufferedReader ): results = batch_method ( None ,[ test_input . read () for _ in range ( self . batch_size )]) elif isinstance ( test_input , bytes ): results = batch_method ( None ,[ test_input for _ in range ( self . batch_size )]) elif isinstance ( test_input , str ): if os . path . exists ( test_input ): results = batch_method ( None ,[ open ( test_input , 'rb' ) . read () for _ in range ( self . batch_size )]) else : results = batch_method ( None ,[ bytes ( test_input , encoding = 'utf8' ) for _ in range ( self . batch_size )]) else : print ( \"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\" ) return False return results","title":"test_batch()"},{"location":"chassisml_sdk-reference/#chassisml_sdk.chassisml.chassisml.ChassisModel.test_env","text":"Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. NOTE : This method is not available in the publicly-hosted service. Parameters: Name Type Description Default test_input_path str Filepath to sample input data required conda_env str Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment None fix_env bool Modifies conda or pip-installable packages into list of dependencies to be installed during the container build True Returns: Type Description Dict raw model predictions returned by process_fn or batch_process_fn run from within chassis service Examples: chassis_model = chassis_client . create_model ( process_fn = process ) sample_filepath = './sample_data.json' results = chassis_model . test_env ( sample_filepath ) Source code in chassisml_sdk/chassisml/chassisml.py def test_env ( self , test_input_path , conda_env = None , fix_env = True ): ''' Runs a sample inference test in new conda environment created on the chassis service side. In other words, a \"dry run\" of a true chassis job to ensure model code runs within the chassis service. **NOTE**: This method is not available in the publicly-hosted service. Args: test_input_path (str): Filepath to sample input data conda_env (str): Either filepath to conda.yaml file or dictionary with environment requirements. If not provided, chassis will infer dependency requirements from local environment fix_env (bool): Modifies conda or pip-installable packages into list of dependencies to be installed during the container build Returns: Dict: raw model predictions returned by `process_fn` or `batch_process_fn` run from within chassis service Examples: ```python chassis_model = chassis_client.create_model(process_fn=process) sample_filepath = './sample_data.json' results = chassis_model.test_env(sample_filepath) ``` ''' model_directory = os . path . join ( tempfile . mkdtemp (), CHASSIS_TMP_DIRNAME ) mlflow . pyfunc . save_model ( path = model_directory , python_model = self , conda_env = conda_env , extra_pip_requirements = None if conda_env else [ \"chassisml== {} \" . format ( __version__ )]) if fix_env : fix_dependencies ( model_directory ) # Compress all files in model directory to send them as a zip. tmppath = tempfile . mkdtemp () zipdir ( model_directory , tmppath , MODEL_ZIP_NAME ) with open ( ' {} / {} ' . format ( tmppath , MODEL_ZIP_NAME ), 'rb' ) as model_f , \\ open ( test_input_path , 'rb' ) as test_input_f : files = [ ( 'sample_input' , test_input_f ), ( 'model' , model_f ) ] print ( 'Starting test job... ' , end = '' , flush = True ) res = requests . post ( self . chassis_test_url , files = files ) res . raise_for_status () print ( 'Ok!' ) shutil . rmtree ( tmppath ) shutil . rmtree ( model_directory ) return res . json ()","title":"test_env()"},{"location":"faqs/","text":"FAQs Why the name Chassis? If your model is the engine, we provide a chassis for your model to drive!","title":"FAQs"},{"location":"faqs/#faqs","text":"","title":"FAQs"},{"location":"faqs/#why-the-name-chassis","text":"If your model is the engine, we provide a chassis for your model to drive!","title":"Why the name Chassis?"},{"location":"get-involved/","text":"Get Involved Contributors We are actively looking for new contributors to join us! Feel free to fork and open pull requests to contribute to modzy/chassis . A full list of contributors can be found here . Community Join the #chassisml channel on Modzy's Discord Server where our maintainers meet to plan changes and improvements. We also have a #chassis-model-builder Slack channel on the MLOps.community Slack !","title":"Get Involved"},{"location":"get-involved/#get-involved","text":"","title":"Get Involved"},{"location":"get-involved/#contributors","text":"We are actively looking for new contributors to join us! Feel free to fork and open pull requests to contribute to modzy/chassis . A full list of contributors can be found here .","title":"Contributors"},{"location":"get-involved/#community","text":"Join the #chassisml channel on Modzy's Discord Server where our maintainers meet to plan changes and improvements. We also have a #chassis-model-builder Slack channel on the MLOps.community Slack !","title":"Community"},{"location":"release-notes/","text":"Release Notes v1.0.0 March 2022 v1.0.0 release of Chassis includes: GPU support Batch processing support Arm64 compatibility Open Model Interface compliance check v0.0.1 July 2021 First working release of Chassis.","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/#v100","text":"March 2022 v1.0.0 release of Chassis includes: GPU support Batch processing support Arm64 compatibility Open Model Interface compliance check","title":"v1.0.0"},{"location":"release-notes/#v001","text":"July 2021 First working release of Chassis.","title":"v0.0.1"},{"location":"service-reference/","text":"Chassisml API Reference Welcome to the Chassisml API Reference documentation homepage. The API is designed using the principles of REST services using standard HTTP verbs and status codes, implemented with Flask . On this page, you will find documentation for: Available REST endpoints in API Service Methods implemented within the each endpoint Endpoints /health (GET) Confirms Chassis service is up and running /build (POST) Kicks off the container image build process /job/{job_id} (GET) Retrieves the status of a chassis /build job /job/{job_id}/download-tar (GET) Retrieves docker image tar archive from a volume attached to the Kubernetes cluster hosting chassis and downloads it to a local filepath /test (POST) Creates a conda environment as specified by the user's model artifacts and runs the ChassisModel to ensure the model code can run within the provided conda environment Functions build_image () This method is run by the /build endpoint. It generates a model image based upon a POST request. The request.files structure can be seen in the Python SDK docs. Parameters: Name Type Description Default None None This method does not take any parameters required Returns: Type Description Dict information about whether or not the image build resulted in an error Source code in service/app.py def build_image (): ''' This method is run by the `/build` endpoint. It generates a model image based upon a POST request. The `request.files` structure can be seen in the Python SDK docs. Args: None (None): This method does not take any parameters Returns: Dict: information about whether or not the image build resulted in an error ''' if not ( 'image_data' in request . files and 'model' in request . files ): return 'Both model and image_data are required' , 500 # retrieve image_data and populate variables accordingly image_data = json . load ( request . files . get ( 'image_data' )) model_name = image_data . get ( 'model_name' ) image_name = image_data . get ( 'name' ) gpu = image_data . get ( 'gpu' ) arm64 = image_data . get ( 'arm64' ) publish = image_data . get ( 'publish' , False ) publish = True if publish else '' registry_auth = image_data . get ( 'registry_auth' ) # retrieve binary representations for all three variables model = request . files . get ( 'model' ) modzy_metadata_data = request . files . get ( 'modzy_metadata_data' ) modzy_sample_input_data = request . files . get ( 'modzy_sample_input_data' ) # json string loaded into variable modzy_data = json . load ( request . files . get ( 'modzy_data' ) or {}) modzy_model_id = modzy_data . get ( 'modzy_model_id' ) # This is a future proofing variable in case we encounter a model that cannot be converted into mlflow. # It will remain hardcoded for now. module_name = 'mlflow' # This name is a random id used to ensure that all jobs are uniquely named and traceable. random_name = str ( uuid . uuid4 ()) # Unzip model archive if PV_MODE : unzip_model ( model , module_name , random_name ) context_uri = None else : dockerfile = choose_dockerfile ( gpu , arm64 ) context_uri = upload_context ( model , module_name , random_name , modzy_metadata_data , dockerfile ) # User can build the image but not deploy it to Modzy. So no input_sample is mandatory. # On the other hand, the model.yaml is needed to build the image so proceed with it. # save the sample input to the modzy_sample_input_path directory if modzy_data : try : # attempt modzy connection modzy_url = modzy_data . get ( 'modzy_url' ) api_key = modzy_data . get ( 'api_key' ) modzy_client = ApiClient ( modzy_url , api_key ) # attempt to create new version of existing modzy model if requested if modzy_model_id : requested_version = yaml . safe_load ( modzy_metadata_data )[ 'version' ] modzy_metadata_data . seek ( 0 ) modzy_utils . create_version ( modzy_client , modzy_model_id , requested_version ) except Exception as e : return { 'error' : str ( e ), 'job_id' : None } if PV_MODE : modzy_sample_input_path = extract_modzy_sample_input ( modzy_sample_input_data , module_name , random_name ) modzy_data [ 'modzy_sample_input_path' ] = modzy_sample_input_path modzy_uri = None else : modzy_uri = upload_modzy_files ( random_name , modzy_metadata_data , modzy_sample_input_data ) modzy_metadata_path = extract_modzy_metadata ( modzy_metadata_data , module_name , random_name ) modzy_data [ 'modzy_metadata_path' ] = modzy_metadata_path else : modzy_uri = None # this path is the local location that kaniko will store the image it creates path_to_tar_file = f ' { DATA_DIR if PV_MODE else \"/tar\" } /kaniko_image- { random_name } .tar' logger . debug ( f 'Request data: { image_name } , { module_name } , { model_name } , { path_to_tar_file } ' ) error = run_kaniko ( image_name , module_name , model_name , path_to_tar_file , random_name , modzy_data , publish , registry_auth , gpu , arm64 , context_uri , modzy_uri , modzy_model_id ) if error : return { 'error' : error , 'job_id' : None } return { 'error' : False , 'job_id' : f ' { K_JOB_NAME } - { random_name } ' } create_job_object ( image_name , module_name , model_name , path_to_tar_file , random_name , modzy_data , publish , registry_auth , gpu = False , arm64 = False , context_uri = None , modzy_uri = None , modzy_model_id = None ) This utility method sets up all the required objects needed to create a model image and is run within the run_kaniko method. Parameters: Name Type Description Default image_name str container image name required module_name str reference module to locate location within service input is saved required model_name str name of model to package required path_to_tar_file str filepath destination to save docker image tar file required random_name str random id generated during build process that is used to ensure that all jobs are uniquely named and traceable required modzy_data str modzy_metadata_path returned from extract_modzy_metadata method required publish bool determines if image will be published to Docker registry required registry_auth dict Docker registry authorization credentials required gpu bool If True , will build container image that runs on GPU False arm64 bool If True , will build container image that runs on ARM64 architecture False context_uri str Location of build context in S3 (S3 mode only) None modzy_uri str Location of modzy data in S3 (S3 mode only) None modzy_model_id str existing modzy model id if user requested new version None Returns: Type Description Job Chassis job object Source code in service/app.py def create_job_object ( image_name , module_name , model_name , path_to_tar_file , random_name , modzy_data , publish , registry_auth , gpu = False , arm64 = False , context_uri = None , modzy_uri = None , modzy_model_id = None ): ''' This utility method sets up all the required objects needed to create a model image and is run within the `run_kaniko` method. Args: image_name (str): container image name module_name (str): reference module to locate location within service input is saved model_name (str): name of model to package path_to_tar_file (str): filepath destination to save docker image tar file random_name (str): random id generated during build process that is used to ensure that all jobs are uniquely named and traceable modzy_data (str): modzy_metadata_path returned from `extract_modzy_metadata` method publish (bool): determines if image will be published to Docker registry registry_auth (dict): Docker registry authorization credentials gpu (bool): If `True`, will build container image that runs on GPU arm64 (bool): If `True`, will build container image that runs on ARM64 architecture context_uri (str): Location of build context in S3 (S3 mode only) modzy_uri (str): Location of modzy data in S3 (S3 mode only) modzy_model_id (str): existing modzy model id if user requested new version Returns: Job: Chassis job object ''' job_name = f ' { K_JOB_NAME } - { random_name } ' # credential setup for Docker Hub. # json for holding registry credentials that will access docker hub. # reference: https://github.com/GoogleContainerTools/kaniko#pushing-to-docker-hub registry_credentials = f ' {{ \"auths\": {{ \"https://index.docker.io/v1/\": {{ \"auth\":\" { registry_auth } \" }}}}}} ' b64_registry_credentials = base64 . b64encode ( registry_credentials . encode ( \"utf-8\" )) . decode ( \"utf-8\" ) # mount path leads to /data # this is a mount point. NOT the volume itself. # name aligns with a volume defined below. if PV_MODE : data_volume_mount = client . V1VolumeMount ( mount_path = MOUNT_PATH_DIR , name = \"local-volume-code\" ) if CHASSIS_DEV else client . V1VolumeMount ( mount_path = MOUNT_PATH_DIR , name = K_DATA_VOLUME_NAME ) # This volume will be used by kaniko container to get registry credentials. # mount path leads to /kaniko/.docker per kaniko reference documentation # this is a mount point. NOT the volume itself. # name aligns with a volume defined below. kaniko_credentials_volume_mount = client . V1VolumeMount ( mount_path = K_KANIKO_EMPTY_DIR_PATH , name = K_EMPTY_DIR_NAME ) # create secret for registry credentials registry_creds_secret_name = f ' { random_name } -creds' metadata = { 'name' : registry_creds_secret_name , 'namespace' : ENVIRONMENT } data = { 'config.json' : b64_registry_credentials } api_version = 'v1' kind = 'Secret' secret = client . V1Secret ( api_version , data , kind , metadata ) client . CoreV1Api () . create_namespaced_secret ( ENVIRONMENT , secret ) # volume holding credentials kaniko_credentials_volume = client . V1Volume ( name = K_EMPTY_DIR_NAME , secret = client . V1SecretVolumeSource ( secret_name = registry_creds_secret_name ) ) # This is the kaniko container used to build the final image. kaniko_args = [ '' if publish else '--no-push' , f '--destination= { image_name }{ \"\" if \":\" in image_name else \":latest\" } ' , '--snapshotMode=redo' , '--use-new-run' , f '--build-arg=MODEL_DIR=model- { random_name } ' , f '--build-arg=MODZY_METADATA_PATH= { modzy_data . get ( \"modzy_metadata_path\" ) if modzy_data . get ( \"modzy_metadata_path\" ) is not None else \"flavours/mlflow/interfaces/modzy/asset_bundle/0.1.0/model.yaml\" } ' , f '--build-arg=MODEL_NAME= { model_name } ' , f '--build-arg=MODEL_CLASS= { module_name } ' , # Modzy is the default interface. '--build-arg=INTERFACE=modzy' , ] modzy_uploader_args = [ f '--api_key= { modzy_data . get ( \"api_key\" ) } ' , f '--deploy= { True if modzy_data . get ( \"deploy\" ) else \"\" } ' , f '--sample_input_path= { modzy_data . get ( \"modzy_sample_input_path\" ) } ' , f '--image_tag= { image_name }{ \"\" if \":\" in image_name else \":latest\" } ' , f '--modzy_url= { modzy_data . get ( \"modzy_url\" ) } ' ] if modzy_model_id : modzy_uploader_args . append ( f '--model_id= { modzy_model_id } ' ) volumes = [ kaniko_credentials_volume ] modzy_uploader_volume_mounts = [] kaniko_volume_mounts = [ kaniko_credentials_volume_mount ] base_resources = { \"memory\" : \"8Gi\" , \"cpu\" : \"2\" } slim_reqs = { \"memory\" : \"2Gi\" , \"cpu\" : \"1\" } kaniko_reqs = client . V1ResourceRequirements ( limits = base_resources , requests = base_resources ) modzy_uploader_reqs = client . V1ResourceRequirements ( limits = slim_reqs , requests = slim_reqs ) if PV_MODE : dockerfile = choose_dockerfile ( gpu , arm64 ) kaniko_args . extend ([ f '--dockerfile= { DATA_DIR } /flavours/ { module_name } / { dockerfile } ' , f '--context= { DATA_DIR } ' , f '--tarPath= { path_to_tar_file } ' ,]) kaniko_volume_mounts . append ( data_volume_mount ) init_container_kaniko = client . V1Container ( name = 'kaniko' , image = 'gcr.io/kaniko-project/executor:latest' , volume_mounts = kaniko_volume_mounts , resources = kaniko_reqs , args = kaniko_args ) # volume claim data_pv_claim = client . V1PersistentVolumeClaimVolumeSource ( claim_name = \"dir-claim-chassis\" ) if CHASSIS_DEV else client . V1PersistentVolumeClaimVolumeSource ( claim_name = K_DATA_VOLUME_NAME ) # volume holding data data_volume = client . V1Volume ( name = \"local-volume-code\" , persistent_volume_claim = data_pv_claim ) if CHASSIS_DEV else client . V1Volume ( name = K_DATA_VOLUME_NAME , persistent_volume_claim = data_pv_claim ) modzy_uploader_args . extend ([ f '--sample_input_path= { modzy_data . get ( \"modzy_sample_input_path\" ) } ' , f '--metadata_path= { DATA_DIR } / { modzy_data . get ( \"modzy_metadata_path\" ) } ' ]) modzy_uploader_volume_mounts . append ( data_volume_mount ) volumes . append ( data_volume ) else : kaniko_args . append ( f '--context= { context_uri } ' ) if MODE == \"s3\" : kaniko_s3_volume_mount = client . V1VolumeMount ( mount_path = '/root/.aws' , name = 'storage-key' ) kaniko_storage_key_volume = client . V1Volume ( name = 'storage-key' , secret = client . V1SecretVolumeSource ( secret_name = 'storage-key' ) ) kaniko_volume_mounts . append ( kaniko_s3_volume_mount ) init_container_kaniko = client . V1Container ( name = 'kaniko' , image = 'gcr.io/kaniko-project/executor:latest' , volume_mounts = kaniko_volume_mounts , env = [ client . V1EnvVar ( name = 'AWS_REGION' , value = AWS_REGION )], resources = kaniko_reqs , args = kaniko_args ) elif MODE == \"gs\" : kaniko_gs_volume_mount = client . V1VolumeMount ( mount_path = '/secret' , name = 'storage-key' ) kaniko_storage_key_volume = client . V1Volume ( name = 'storage-key' , secret = client . V1SecretVolumeSource ( secret_name = 'storage-key' ) ) kaniko_volume_mounts . append ( kaniko_gs_volume_mount ) init_container_kaniko = client . V1Container ( name = 'kaniko' , image = 'gcr.io/kaniko-project/executor:latest' , volume_mounts = kaniko_volume_mounts , env = [ client . V1EnvVar ( name = 'GOOGLE_APPLICATION_CREDENTIALS' , value = '/secret/storage-key.json' )], resources = kaniko_reqs , args = kaniko_args ) else : raise ValueError ( \"Only allowed modes are: 'pv', 'gs', 's3'\" ) modzy_uploader_args . append ( f '--modzy_uri= { modzy_uri } ' ) volumes . append ( kaniko_storage_key_volume ) modzy_uploader_container = client . V1Container ( name = 'modzy-uploader' , image = MODZY_UPLOADER_REPOSITORY , volume_mounts = modzy_uploader_volume_mounts , env = [ client . V1EnvVar ( name = 'JOB_NAME' , value = job_name ), client . V1EnvVar ( name = 'ENVIRONMENT' , value = ENVIRONMENT ) ], resources = modzy_uploader_reqs , args = modzy_uploader_args ) # Pod spec for the image build process if modzy_data . get ( \"modzy_metadata_path\" ) is None : # No specific Modzy Data provided init_container_list = [] containers_list = [ init_container_kaniko ] else : init_container_list = [ init_container_kaniko ] containers_list = [ modzy_uploader_container ] pod_spec = client . V1PodSpec ( service_account_name = K_SERVICE_ACCOUNT_NAME , restart_policy = 'Never' , init_containers = init_container_list , containers = containers_list , volumes = volumes ) # setup and initiate model image build template = client . V1PodTemplateSpec ( metadata = client . V1ObjectMeta ( name = job_name ), spec = pod_spec ) spec = client . V1JobSpec ( backoff_limit = 0 , template = template ) job = client . V1Job ( api_version = 'batch/v1' , kind = 'Job' , metadata = client . V1ObjectMeta ( name = job_name , ), spec = spec ) return job download_tar ( job_id ) This method is run by the /job/{job_id}/download-tar endpoint. It downloads the container image from kaniko, built during the chassis job with the name job_id Parameters: Name Type Description Default job_id str valid Chassis job identifier, generated by create_job method required Returns: Type Description Dict response from download_tar endpoint Source code in service/app.py def download_tar ( job_id ): ''' This method is run by the `/job/{job_id}/download-tar` endpoint. It downloads the container image from kaniko, built during the chassis job with the name `job_id` Args: job_id (str): valid Chassis job identifier, generated by `create_job` method Returns: Dict: response from `download_tar` endpoint ''' uid = job_id . split ( f ' { K_JOB_NAME } -' )[ 1 ] if PV_MODE : return send_from_directory ( DATA_DIR , path = f 'kaniko_image- { uid } .tar' , as_attachment = False ) else : return Response ( f \"400 Bad Request: Tar download not available in production mode, please use 'docker pull ...'\" , 400 ) get_job_status ( job_id ) This method is run by the /job/{job_id} endpoint. Based on a GET request, it retrieves the status of the Kaniko job and the results if the job has completed. Parameters: Name Type Description Default job_id str valid Chassis job identifier, generated by create_job method required Returns: Type Description Dict Dictionary containing corresponding job data of job job_id Source code in service/app.py def get_job_status ( job_id ): ''' This method is run by the `/job/{job_id}` endpoint. Based on a GET request, it retrieves the status of the Kaniko job and the results if the job has completed. Args: job_id (str): valid Chassis job identifier, generated by `create_job` method Returns: Dict: Dictionary containing corresponding job data of job `job_id` ''' if CHASSIS_DEV : # if you are doing local dev you need to point at the local kubernetes cluster with your config file kubefile = os . getenv ( \"CHASSIS_KUBECONFIG\" ) config . load_kube_config ( kubefile ) else : # if the service is running inside a cluster during production then the config can be inherited config . load_incluster_config () batch_v1 = client . BatchV1Api () try : job = batch_v1 . read_namespaced_job ( job_id , ENVIRONMENT ) annotations = job . metadata . annotations or {} result = annotations . get ( 'result' ) result = json . loads ( result ) if result else None status = job . status job_data = { 'result' : result , 'status' : status . to_dict () } return job_data except ApiException as e : logger . error ( f 'Exception when getting job status: { e } ' ) return e . body test_model () This method is run by the /test endpoint. It creates a new conda environment from the provided conda.yaml file and then tests the provided model in that conda environment with provided test input file. Parameters: Name Type Description Default None None This method does not take any parameters required Returns: Type Description Dict model response to /test endpoint. Should contain either successful predictions or error message Source code in service/app.py def test_model (): ''' This method is run by the `/test` endpoint. It creates a new conda environment from the provided `conda.yaml` file and then tests the provided model in that conda environment with provided test input file. Args: None (None): This method does not take any parameters Returns: Dict: model response to `/test` endpoint. Should contain either successful predictions or error message ''' if not ( 'sample_input' in request . files and 'model' in request . files ): return 'Both sample input and model are required' , 500 output_dict = {} # retrieve binary representations for both variables model = request . files . get ( 'model' ) sample_input = request . files . get ( 'sample_input' ) # This is a future proofing variable in case we encounter a model that cannot be converted into mlflow. # It will remain hardcoded for now. module_name = 'mlflow' # This name is a random id used to ensure that all jobs are uniquely named and traceable. random_name = str ( uuid . uuid4 ()) # Unzip model archive unzipped_path = unzip_model ( model , module_name , random_name ) # get sample input path sample_input_path = extract_modzy_sample_input ( sample_input , module_name , random_name ) # create conda env, return error if fails try : tmp_env_name = str ( time . time ()) rm_env_cmd = \"conda env remove --name {} \" . format ( tmp_env_name ) yaml_path = os . path . join ( unzipped_path , \"conda.yaml\" ) create_env_cmd = \"conda env create -f {} -n {} \" . format ( yaml_path , tmp_env_name ) subprocess . run ( create_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' , check = True ) except subprocess . CalledProcessError as e : print ( e ) subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) output_dict [ \"env_error\" ] = e . stderr . decode () return output_dict # test model in env with sample input file, return error if fails try : test_model_cmd = \"\"\" source activate {} ; python test_chassis_model.py {} {} \"\"\" . format ( tmp_env_name , unzipped_path , sample_input_path ) test_ret = subprocess . run ( test_model_cmd , capture_output = True , shell = True , executable = '/bin/bash' , check = True ) output_dict [ \"model_output\" ] = test_ret . stdout . decode () except subprocess . CalledProcessError as e : subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) output_dict [ \"model_error\" ] = e . stderr . decode () return output_dict # if we make it here, test was successful, remove env and return output subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) return output_dict","title":"Chassisml Service"},{"location":"service-reference/#chassisml-api-reference","text":"Welcome to the Chassisml API Reference documentation homepage. The API is designed using the principles of REST services using standard HTTP verbs and status codes, implemented with Flask . On this page, you will find documentation for: Available REST endpoints in API Service Methods implemented within the each endpoint","title":"Chassisml API Reference"},{"location":"service-reference/#endpoints","text":"/health (GET) Confirms Chassis service is up and running /build (POST) Kicks off the container image build process /job/{job_id} (GET) Retrieves the status of a chassis /build job /job/{job_id}/download-tar (GET) Retrieves docker image tar archive from a volume attached to the Kubernetes cluster hosting chassis and downloads it to a local filepath /test (POST) Creates a conda environment as specified by the user's model artifacts and runs the ChassisModel to ensure the model code can run within the provided conda environment","title":"Endpoints"},{"location":"service-reference/#service.app-functions","text":"","title":"Functions"},{"location":"service-reference/#service.app.build_image","text":"This method is run by the /build endpoint. It generates a model image based upon a POST request. The request.files structure can be seen in the Python SDK docs. Parameters: Name Type Description Default None None This method does not take any parameters required Returns: Type Description Dict information about whether or not the image build resulted in an error Source code in service/app.py def build_image (): ''' This method is run by the `/build` endpoint. It generates a model image based upon a POST request. The `request.files` structure can be seen in the Python SDK docs. Args: None (None): This method does not take any parameters Returns: Dict: information about whether or not the image build resulted in an error ''' if not ( 'image_data' in request . files and 'model' in request . files ): return 'Both model and image_data are required' , 500 # retrieve image_data and populate variables accordingly image_data = json . load ( request . files . get ( 'image_data' )) model_name = image_data . get ( 'model_name' ) image_name = image_data . get ( 'name' ) gpu = image_data . get ( 'gpu' ) arm64 = image_data . get ( 'arm64' ) publish = image_data . get ( 'publish' , False ) publish = True if publish else '' registry_auth = image_data . get ( 'registry_auth' ) # retrieve binary representations for all three variables model = request . files . get ( 'model' ) modzy_metadata_data = request . files . get ( 'modzy_metadata_data' ) modzy_sample_input_data = request . files . get ( 'modzy_sample_input_data' ) # json string loaded into variable modzy_data = json . load ( request . files . get ( 'modzy_data' ) or {}) modzy_model_id = modzy_data . get ( 'modzy_model_id' ) # This is a future proofing variable in case we encounter a model that cannot be converted into mlflow. # It will remain hardcoded for now. module_name = 'mlflow' # This name is a random id used to ensure that all jobs are uniquely named and traceable. random_name = str ( uuid . uuid4 ()) # Unzip model archive if PV_MODE : unzip_model ( model , module_name , random_name ) context_uri = None else : dockerfile = choose_dockerfile ( gpu , arm64 ) context_uri = upload_context ( model , module_name , random_name , modzy_metadata_data , dockerfile ) # User can build the image but not deploy it to Modzy. So no input_sample is mandatory. # On the other hand, the model.yaml is needed to build the image so proceed with it. # save the sample input to the modzy_sample_input_path directory if modzy_data : try : # attempt modzy connection modzy_url = modzy_data . get ( 'modzy_url' ) api_key = modzy_data . get ( 'api_key' ) modzy_client = ApiClient ( modzy_url , api_key ) # attempt to create new version of existing modzy model if requested if modzy_model_id : requested_version = yaml . safe_load ( modzy_metadata_data )[ 'version' ] modzy_metadata_data . seek ( 0 ) modzy_utils . create_version ( modzy_client , modzy_model_id , requested_version ) except Exception as e : return { 'error' : str ( e ), 'job_id' : None } if PV_MODE : modzy_sample_input_path = extract_modzy_sample_input ( modzy_sample_input_data , module_name , random_name ) modzy_data [ 'modzy_sample_input_path' ] = modzy_sample_input_path modzy_uri = None else : modzy_uri = upload_modzy_files ( random_name , modzy_metadata_data , modzy_sample_input_data ) modzy_metadata_path = extract_modzy_metadata ( modzy_metadata_data , module_name , random_name ) modzy_data [ 'modzy_metadata_path' ] = modzy_metadata_path else : modzy_uri = None # this path is the local location that kaniko will store the image it creates path_to_tar_file = f ' { DATA_DIR if PV_MODE else \"/tar\" } /kaniko_image- { random_name } .tar' logger . debug ( f 'Request data: { image_name } , { module_name } , { model_name } , { path_to_tar_file } ' ) error = run_kaniko ( image_name , module_name , model_name , path_to_tar_file , random_name , modzy_data , publish , registry_auth , gpu , arm64 , context_uri , modzy_uri , modzy_model_id ) if error : return { 'error' : error , 'job_id' : None } return { 'error' : False , 'job_id' : f ' { K_JOB_NAME } - { random_name } ' }","title":"build_image()"},{"location":"service-reference/#service.app.create_job_object","text":"This utility method sets up all the required objects needed to create a model image and is run within the run_kaniko method. Parameters: Name Type Description Default image_name str container image name required module_name str reference module to locate location within service input is saved required model_name str name of model to package required path_to_tar_file str filepath destination to save docker image tar file required random_name str random id generated during build process that is used to ensure that all jobs are uniquely named and traceable required modzy_data str modzy_metadata_path returned from extract_modzy_metadata method required publish bool determines if image will be published to Docker registry required registry_auth dict Docker registry authorization credentials required gpu bool If True , will build container image that runs on GPU False arm64 bool If True , will build container image that runs on ARM64 architecture False context_uri str Location of build context in S3 (S3 mode only) None modzy_uri str Location of modzy data in S3 (S3 mode only) None modzy_model_id str existing modzy model id if user requested new version None Returns: Type Description Job Chassis job object Source code in service/app.py def create_job_object ( image_name , module_name , model_name , path_to_tar_file , random_name , modzy_data , publish , registry_auth , gpu = False , arm64 = False , context_uri = None , modzy_uri = None , modzy_model_id = None ): ''' This utility method sets up all the required objects needed to create a model image and is run within the `run_kaniko` method. Args: image_name (str): container image name module_name (str): reference module to locate location within service input is saved model_name (str): name of model to package path_to_tar_file (str): filepath destination to save docker image tar file random_name (str): random id generated during build process that is used to ensure that all jobs are uniquely named and traceable modzy_data (str): modzy_metadata_path returned from `extract_modzy_metadata` method publish (bool): determines if image will be published to Docker registry registry_auth (dict): Docker registry authorization credentials gpu (bool): If `True`, will build container image that runs on GPU arm64 (bool): If `True`, will build container image that runs on ARM64 architecture context_uri (str): Location of build context in S3 (S3 mode only) modzy_uri (str): Location of modzy data in S3 (S3 mode only) modzy_model_id (str): existing modzy model id if user requested new version Returns: Job: Chassis job object ''' job_name = f ' { K_JOB_NAME } - { random_name } ' # credential setup for Docker Hub. # json for holding registry credentials that will access docker hub. # reference: https://github.com/GoogleContainerTools/kaniko#pushing-to-docker-hub registry_credentials = f ' {{ \"auths\": {{ \"https://index.docker.io/v1/\": {{ \"auth\":\" { registry_auth } \" }}}}}} ' b64_registry_credentials = base64 . b64encode ( registry_credentials . encode ( \"utf-8\" )) . decode ( \"utf-8\" ) # mount path leads to /data # this is a mount point. NOT the volume itself. # name aligns with a volume defined below. if PV_MODE : data_volume_mount = client . V1VolumeMount ( mount_path = MOUNT_PATH_DIR , name = \"local-volume-code\" ) if CHASSIS_DEV else client . V1VolumeMount ( mount_path = MOUNT_PATH_DIR , name = K_DATA_VOLUME_NAME ) # This volume will be used by kaniko container to get registry credentials. # mount path leads to /kaniko/.docker per kaniko reference documentation # this is a mount point. NOT the volume itself. # name aligns with a volume defined below. kaniko_credentials_volume_mount = client . V1VolumeMount ( mount_path = K_KANIKO_EMPTY_DIR_PATH , name = K_EMPTY_DIR_NAME ) # create secret for registry credentials registry_creds_secret_name = f ' { random_name } -creds' metadata = { 'name' : registry_creds_secret_name , 'namespace' : ENVIRONMENT } data = { 'config.json' : b64_registry_credentials } api_version = 'v1' kind = 'Secret' secret = client . V1Secret ( api_version , data , kind , metadata ) client . CoreV1Api () . create_namespaced_secret ( ENVIRONMENT , secret ) # volume holding credentials kaniko_credentials_volume = client . V1Volume ( name = K_EMPTY_DIR_NAME , secret = client . V1SecretVolumeSource ( secret_name = registry_creds_secret_name ) ) # This is the kaniko container used to build the final image. kaniko_args = [ '' if publish else '--no-push' , f '--destination= { image_name }{ \"\" if \":\" in image_name else \":latest\" } ' , '--snapshotMode=redo' , '--use-new-run' , f '--build-arg=MODEL_DIR=model- { random_name } ' , f '--build-arg=MODZY_METADATA_PATH= { modzy_data . get ( \"modzy_metadata_path\" ) if modzy_data . get ( \"modzy_metadata_path\" ) is not None else \"flavours/mlflow/interfaces/modzy/asset_bundle/0.1.0/model.yaml\" } ' , f '--build-arg=MODEL_NAME= { model_name } ' , f '--build-arg=MODEL_CLASS= { module_name } ' , # Modzy is the default interface. '--build-arg=INTERFACE=modzy' , ] modzy_uploader_args = [ f '--api_key= { modzy_data . get ( \"api_key\" ) } ' , f '--deploy= { True if modzy_data . get ( \"deploy\" ) else \"\" } ' , f '--sample_input_path= { modzy_data . get ( \"modzy_sample_input_path\" ) } ' , f '--image_tag= { image_name }{ \"\" if \":\" in image_name else \":latest\" } ' , f '--modzy_url= { modzy_data . get ( \"modzy_url\" ) } ' ] if modzy_model_id : modzy_uploader_args . append ( f '--model_id= { modzy_model_id } ' ) volumes = [ kaniko_credentials_volume ] modzy_uploader_volume_mounts = [] kaniko_volume_mounts = [ kaniko_credentials_volume_mount ] base_resources = { \"memory\" : \"8Gi\" , \"cpu\" : \"2\" } slim_reqs = { \"memory\" : \"2Gi\" , \"cpu\" : \"1\" } kaniko_reqs = client . V1ResourceRequirements ( limits = base_resources , requests = base_resources ) modzy_uploader_reqs = client . V1ResourceRequirements ( limits = slim_reqs , requests = slim_reqs ) if PV_MODE : dockerfile = choose_dockerfile ( gpu , arm64 ) kaniko_args . extend ([ f '--dockerfile= { DATA_DIR } /flavours/ { module_name } / { dockerfile } ' , f '--context= { DATA_DIR } ' , f '--tarPath= { path_to_tar_file } ' ,]) kaniko_volume_mounts . append ( data_volume_mount ) init_container_kaniko = client . V1Container ( name = 'kaniko' , image = 'gcr.io/kaniko-project/executor:latest' , volume_mounts = kaniko_volume_mounts , resources = kaniko_reqs , args = kaniko_args ) # volume claim data_pv_claim = client . V1PersistentVolumeClaimVolumeSource ( claim_name = \"dir-claim-chassis\" ) if CHASSIS_DEV else client . V1PersistentVolumeClaimVolumeSource ( claim_name = K_DATA_VOLUME_NAME ) # volume holding data data_volume = client . V1Volume ( name = \"local-volume-code\" , persistent_volume_claim = data_pv_claim ) if CHASSIS_DEV else client . V1Volume ( name = K_DATA_VOLUME_NAME , persistent_volume_claim = data_pv_claim ) modzy_uploader_args . extend ([ f '--sample_input_path= { modzy_data . get ( \"modzy_sample_input_path\" ) } ' , f '--metadata_path= { DATA_DIR } / { modzy_data . get ( \"modzy_metadata_path\" ) } ' ]) modzy_uploader_volume_mounts . append ( data_volume_mount ) volumes . append ( data_volume ) else : kaniko_args . append ( f '--context= { context_uri } ' ) if MODE == \"s3\" : kaniko_s3_volume_mount = client . V1VolumeMount ( mount_path = '/root/.aws' , name = 'storage-key' ) kaniko_storage_key_volume = client . V1Volume ( name = 'storage-key' , secret = client . V1SecretVolumeSource ( secret_name = 'storage-key' ) ) kaniko_volume_mounts . append ( kaniko_s3_volume_mount ) init_container_kaniko = client . V1Container ( name = 'kaniko' , image = 'gcr.io/kaniko-project/executor:latest' , volume_mounts = kaniko_volume_mounts , env = [ client . V1EnvVar ( name = 'AWS_REGION' , value = AWS_REGION )], resources = kaniko_reqs , args = kaniko_args ) elif MODE == \"gs\" : kaniko_gs_volume_mount = client . V1VolumeMount ( mount_path = '/secret' , name = 'storage-key' ) kaniko_storage_key_volume = client . V1Volume ( name = 'storage-key' , secret = client . V1SecretVolumeSource ( secret_name = 'storage-key' ) ) kaniko_volume_mounts . append ( kaniko_gs_volume_mount ) init_container_kaniko = client . V1Container ( name = 'kaniko' , image = 'gcr.io/kaniko-project/executor:latest' , volume_mounts = kaniko_volume_mounts , env = [ client . V1EnvVar ( name = 'GOOGLE_APPLICATION_CREDENTIALS' , value = '/secret/storage-key.json' )], resources = kaniko_reqs , args = kaniko_args ) else : raise ValueError ( \"Only allowed modes are: 'pv', 'gs', 's3'\" ) modzy_uploader_args . append ( f '--modzy_uri= { modzy_uri } ' ) volumes . append ( kaniko_storage_key_volume ) modzy_uploader_container = client . V1Container ( name = 'modzy-uploader' , image = MODZY_UPLOADER_REPOSITORY , volume_mounts = modzy_uploader_volume_mounts , env = [ client . V1EnvVar ( name = 'JOB_NAME' , value = job_name ), client . V1EnvVar ( name = 'ENVIRONMENT' , value = ENVIRONMENT ) ], resources = modzy_uploader_reqs , args = modzy_uploader_args ) # Pod spec for the image build process if modzy_data . get ( \"modzy_metadata_path\" ) is None : # No specific Modzy Data provided init_container_list = [] containers_list = [ init_container_kaniko ] else : init_container_list = [ init_container_kaniko ] containers_list = [ modzy_uploader_container ] pod_spec = client . V1PodSpec ( service_account_name = K_SERVICE_ACCOUNT_NAME , restart_policy = 'Never' , init_containers = init_container_list , containers = containers_list , volumes = volumes ) # setup and initiate model image build template = client . V1PodTemplateSpec ( metadata = client . V1ObjectMeta ( name = job_name ), spec = pod_spec ) spec = client . V1JobSpec ( backoff_limit = 0 , template = template ) job = client . V1Job ( api_version = 'batch/v1' , kind = 'Job' , metadata = client . V1ObjectMeta ( name = job_name , ), spec = spec ) return job","title":"create_job_object()"},{"location":"service-reference/#service.app.download_tar","text":"This method is run by the /job/{job_id}/download-tar endpoint. It downloads the container image from kaniko, built during the chassis job with the name job_id Parameters: Name Type Description Default job_id str valid Chassis job identifier, generated by create_job method required Returns: Type Description Dict response from download_tar endpoint Source code in service/app.py def download_tar ( job_id ): ''' This method is run by the `/job/{job_id}/download-tar` endpoint. It downloads the container image from kaniko, built during the chassis job with the name `job_id` Args: job_id (str): valid Chassis job identifier, generated by `create_job` method Returns: Dict: response from `download_tar` endpoint ''' uid = job_id . split ( f ' { K_JOB_NAME } -' )[ 1 ] if PV_MODE : return send_from_directory ( DATA_DIR , path = f 'kaniko_image- { uid } .tar' , as_attachment = False ) else : return Response ( f \"400 Bad Request: Tar download not available in production mode, please use 'docker pull ...'\" , 400 )","title":"download_tar()"},{"location":"service-reference/#service.app.get_job_status","text":"This method is run by the /job/{job_id} endpoint. Based on a GET request, it retrieves the status of the Kaniko job and the results if the job has completed. Parameters: Name Type Description Default job_id str valid Chassis job identifier, generated by create_job method required Returns: Type Description Dict Dictionary containing corresponding job data of job job_id Source code in service/app.py def get_job_status ( job_id ): ''' This method is run by the `/job/{job_id}` endpoint. Based on a GET request, it retrieves the status of the Kaniko job and the results if the job has completed. Args: job_id (str): valid Chassis job identifier, generated by `create_job` method Returns: Dict: Dictionary containing corresponding job data of job `job_id` ''' if CHASSIS_DEV : # if you are doing local dev you need to point at the local kubernetes cluster with your config file kubefile = os . getenv ( \"CHASSIS_KUBECONFIG\" ) config . load_kube_config ( kubefile ) else : # if the service is running inside a cluster during production then the config can be inherited config . load_incluster_config () batch_v1 = client . BatchV1Api () try : job = batch_v1 . read_namespaced_job ( job_id , ENVIRONMENT ) annotations = job . metadata . annotations or {} result = annotations . get ( 'result' ) result = json . loads ( result ) if result else None status = job . status job_data = { 'result' : result , 'status' : status . to_dict () } return job_data except ApiException as e : logger . error ( f 'Exception when getting job status: { e } ' ) return e . body","title":"get_job_status()"},{"location":"service-reference/#service.app.test_model","text":"This method is run by the /test endpoint. It creates a new conda environment from the provided conda.yaml file and then tests the provided model in that conda environment with provided test input file. Parameters: Name Type Description Default None None This method does not take any parameters required Returns: Type Description Dict model response to /test endpoint. Should contain either successful predictions or error message Source code in service/app.py def test_model (): ''' This method is run by the `/test` endpoint. It creates a new conda environment from the provided `conda.yaml` file and then tests the provided model in that conda environment with provided test input file. Args: None (None): This method does not take any parameters Returns: Dict: model response to `/test` endpoint. Should contain either successful predictions or error message ''' if not ( 'sample_input' in request . files and 'model' in request . files ): return 'Both sample input and model are required' , 500 output_dict = {} # retrieve binary representations for both variables model = request . files . get ( 'model' ) sample_input = request . files . get ( 'sample_input' ) # This is a future proofing variable in case we encounter a model that cannot be converted into mlflow. # It will remain hardcoded for now. module_name = 'mlflow' # This name is a random id used to ensure that all jobs are uniquely named and traceable. random_name = str ( uuid . uuid4 ()) # Unzip model archive unzipped_path = unzip_model ( model , module_name , random_name ) # get sample input path sample_input_path = extract_modzy_sample_input ( sample_input , module_name , random_name ) # create conda env, return error if fails try : tmp_env_name = str ( time . time ()) rm_env_cmd = \"conda env remove --name {} \" . format ( tmp_env_name ) yaml_path = os . path . join ( unzipped_path , \"conda.yaml\" ) create_env_cmd = \"conda env create -f {} -n {} \" . format ( yaml_path , tmp_env_name ) subprocess . run ( create_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' , check = True ) except subprocess . CalledProcessError as e : print ( e ) subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) output_dict [ \"env_error\" ] = e . stderr . decode () return output_dict # test model in env with sample input file, return error if fails try : test_model_cmd = \"\"\" source activate {} ; python test_chassis_model.py {} {} \"\"\" . format ( tmp_env_name , unzipped_path , sample_input_path ) test_ret = subprocess . run ( test_model_cmd , capture_output = True , shell = True , executable = '/bin/bash' , check = True ) output_dict [ \"model_output\" ] = test_ret . stdout . decode () except subprocess . CalledProcessError as e : subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) output_dict [ \"model_error\" ] = e . stderr . decode () return output_dict # if we make it here, test was successful, remove env and return output subprocess . run ( rm_env_cmd , capture_output = True , shell = True , executable = '/bin/bash' ) return output_dict","title":"test_model()"},{"location":"conceptual-guides/containers/","text":"Containers Overview Chassis packages your model into a \"container\" image which can be run in different environments. Let's understand what containers are and why they're used. Without containers, applications which work in one environment often fail when moved to another environment. This is the problem that containers solve. A container can be defined as a package of software containing everything required to run your application. This includes things such as the application code, dependencies, runtime, system libraries, system settings, etc... How to Use Containers The first step is to determine all of those components required for your application to run and build a container \"image\" which includes those components. This process can get complicated, luckily Chassis handles all of this for you! This \"image\" is a static, immutable file which can create a container when executed. It consists of layers which are added on to a \"parent\" or \"base\" image. This use of layers allows for efficient reuse of common components across images. In order to execute container images, a container \"engine\" such as Docker must be installed on the host machine to run and manage your containers. Containers share the host machine's operating system kernel allowing for multiple containers to run simultaneously on a single host. Containers can be stored and managed in \"registries\", which are collections of \"repositories\", which are collections of container images with the same name but different \"tags\", which are labels that convey information about specific versions of images. For machine learning models, we often use the model version number as the image tag. The Bottom Line Chassis makes it super easy to package your model into a container, which allows you to seamlessly deploy your model to a wide variety of environments.","title":"Containers"},{"location":"conceptual-guides/containers/#containers","text":"","title":"Containers"},{"location":"conceptual-guides/containers/#overview","text":"Chassis packages your model into a \"container\" image which can be run in different environments. Let's understand what containers are and why they're used. Without containers, applications which work in one environment often fail when moved to another environment. This is the problem that containers solve. A container can be defined as a package of software containing everything required to run your application. This includes things such as the application code, dependencies, runtime, system libraries, system settings, etc...","title":"Overview"},{"location":"conceptual-guides/containers/#how-to-use-containers","text":"The first step is to determine all of those components required for your application to run and build a container \"image\" which includes those components. This process can get complicated, luckily Chassis handles all of this for you! This \"image\" is a static, immutable file which can create a container when executed. It consists of layers which are added on to a \"parent\" or \"base\" image. This use of layers allows for efficient reuse of common components across images. In order to execute container images, a container \"engine\" such as Docker must be installed on the host machine to run and manage your containers. Containers share the host machine's operating system kernel allowing for multiple containers to run simultaneously on a single host. Containers can be stored and managed in \"registries\", which are collections of \"repositories\", which are collections of container images with the same name but different \"tags\", which are labels that convey information about specific versions of images. For machine learning models, we often use the model version number as the image tag.","title":"How to Use Containers"},{"location":"conceptual-guides/containers/#the-bottom-line","text":"Chassis makes it super easy to package your model into a container, which allows you to seamlessly deploy your model to a wide variety of environments.","title":"The Bottom Line"},{"location":"conceptual-guides/design/","text":"Design & Architecture Chassis makes it easy to create a deployable docker image from your trained ML model. The idea behind this project is to provide Data Scientists with a way to package their models into a Docker image. This image will manage to build the inference service compatible with several common platforms for free. Build models directly into DevOps-ready container images for inference (using MLflow under the hood) Supports parallel builds in Kubernetes jobs, using Kaniko , no Docker socket required! Generates Open Model Interface compatible images that are multi-purpose and portable, they work on multiple platforms: KServe and Modzy Try the test drive today, then deploy our Helm chart to your K8s cluster to use it for real At the moment, Chassis images are compatible with KServe and Modzy gRPC. This means you can deploy your built image into these platforms once it has been built. Deploy Chassis, send your model to it and start using the built container image to run inference on your data. Architecture This diagram shows the overall architecture of the Chassis system: This diagram zooms in on the generated container, showing the pluggable interface, configurable at runtime:","title":"Design & Architecture"},{"location":"conceptual-guides/design/#design-architecture","text":"Chassis makes it easy to create a deployable docker image from your trained ML model. The idea behind this project is to provide Data Scientists with a way to package their models into a Docker image. This image will manage to build the inference service compatible with several common platforms for free. Build models directly into DevOps-ready container images for inference (using MLflow under the hood) Supports parallel builds in Kubernetes jobs, using Kaniko , no Docker socket required! Generates Open Model Interface compatible images that are multi-purpose and portable, they work on multiple platforms: KServe and Modzy Try the test drive today, then deploy our Helm chart to your K8s cluster to use it for real At the moment, Chassis images are compatible with KServe and Modzy gRPC. This means you can deploy your built image into these platforms once it has been built. Deploy Chassis, send your model to it and start using the built container image to run inference on your data.","title":"Design &amp; Architecture"},{"location":"conceptual-guides/design/#architecture","text":"This diagram shows the overall architecture of the Chassis system: This diagram zooms in on the generated container, showing the pluggable interface, configurable at runtime:","title":"Architecture"},{"location":"conceptual-guides/grpc/","text":"gRPC Overview Chassis builds model containers which support gRPC. Let's understand what it is and why it's used. RPC stands for \"Remote Procedure Call\", and it is a newer model for API design. In this model, a client script is able to call remote procedures, which are actually executing server-side, as if they were executing locally. gRPC is an RPC framework built by Google that uses the HTTP 2.0 protocol under the hood. In gRPC, code \"stubs\" are auto-generated (in different languages) for the client and server, so the details of the RPC to HTTP mapping are abstracted away for developers. Benefits Versus Traditional REST Client-Server Interaction Traditional REST (REpresentational State Transfer) APIs are built using HTTP 1.1, which can support unary interactions (client sends single request, server sends back single response). Since gRPC uses HTTP 2.0, it can handle more complex client-server interactions in addition to those unary interactions, such as: client streaming (client sends stream of messages, server sends back single response), server streaming (client sends single message, server sends back stream of messages), bidirectional streaming (client and server send messages to each other in two independent streams). Data Transmission Format Another key difference has to do with data transmission formats. REST mainly uses JSON or XML whereas gRPC uses protocol buffers (protobuf). Protobuf is a structured data serialization/deserialization mechanism developed by Google, and it is much more lightweight than JSON and XML. The Bottom Line These key differences contribute to the significant speed increase typically observed using gRPC APIs as opposed to traditional REST APIs.","title":"gRPC"},{"location":"conceptual-guides/grpc/#grpc","text":"","title":"gRPC"},{"location":"conceptual-guides/grpc/#overview","text":"Chassis builds model containers which support gRPC. Let's understand what it is and why it's used. RPC stands for \"Remote Procedure Call\", and it is a newer model for API design. In this model, a client script is able to call remote procedures, which are actually executing server-side, as if they were executing locally. gRPC is an RPC framework built by Google that uses the HTTP 2.0 protocol under the hood. In gRPC, code \"stubs\" are auto-generated (in different languages) for the client and server, so the details of the RPC to HTTP mapping are abstracted away for developers.","title":"Overview"},{"location":"conceptual-guides/grpc/#benefits-versus-traditional-rest","text":"","title":"Benefits Versus Traditional REST"},{"location":"conceptual-guides/grpc/#client-server-interaction","text":"Traditional REST (REpresentational State Transfer) APIs are built using HTTP 1.1, which can support unary interactions (client sends single request, server sends back single response). Since gRPC uses HTTP 2.0, it can handle more complex client-server interactions in addition to those unary interactions, such as: client streaming (client sends stream of messages, server sends back single response), server streaming (client sends single message, server sends back stream of messages), bidirectional streaming (client and server send messages to each other in two independent streams).","title":"Client-Server Interaction"},{"location":"conceptual-guides/grpc/#data-transmission-format","text":"Another key difference has to do with data transmission formats. REST mainly uses JSON or XML whereas gRPC uses protocol buffers (protobuf). Protobuf is a structured data serialization/deserialization mechanism developed by Google, and it is much more lightweight than JSON and XML.","title":"Data Transmission Format"},{"location":"conceptual-guides/grpc/#the-bottom-line","text":"These key differences contribute to the significant speed increase typically observed using gRPC APIs as opposed to traditional REST APIs.","title":"The Bottom Line"},{"location":"conceptual-guides/overview/","text":"Overview Goals Chassis is a Kubernetes service that can be deployed in your preferred cluster using Helm. It works by creating jobs that can be run in parallel to create Docker images that package ML models. It provides integration with most common deployment platforms so your model will be ready to be deployed in a simple way. It also provides a python SDK that makes it very easy to communicate with Chassis service in order to build your image. Simple Just make a request to build your image using the python SDK Small set of dependencies: mlflow, flask Supports multiple deployment platforms No DevOps knowledge needed Fast Start building the image as soon as you make the request Automatically upload the image to Docker Hub Image ready to be deployed Secure Using Kaniko to securely build the image Non-goals Some non-goals of this project are: Deploy the built image - this is up to the platform that you deploy the container into, like KServe or Modzy Intro Video .video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Overview"},{"location":"conceptual-guides/overview/#overview","text":"","title":"Overview"},{"location":"conceptual-guides/overview/#goals","text":"Chassis is a Kubernetes service that can be deployed in your preferred cluster using Helm. It works by creating jobs that can be run in parallel to create Docker images that package ML models. It provides integration with most common deployment platforms so your model will be ready to be deployed in a simple way. It also provides a python SDK that makes it very easy to communicate with Chassis service in order to build your image. Simple Just make a request to build your image using the python SDK Small set of dependencies: mlflow, flask Supports multiple deployment platforms No DevOps knowledge needed Fast Start building the image as soon as you make the request Automatically upload the image to Docker Hub Image ready to be deployed Secure Using Kaniko to securely build the image","title":"Goals"},{"location":"conceptual-guides/overview/#non-goals","text":"Some non-goals of this project are: Deploy the built image - this is up to the platform that you deploy the container into, like KServe or Modzy","title":"Non-goals"},{"location":"conceptual-guides/overview/#intro-video","text":".video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Intro Video"},{"location":"getting-started/deploy-connect/","text":"Connect to Hosted Service Welcome! Connecting to this service eliminates the need for you to deploy and stand up a private Kubernetes cluster. Each chassis build job run on our hosted service has enough resources to containerize even the most memory intensive ML models (up to 8GB RAM and 2 CPUs). Follow the instructions on this page to connect and get started using Chassis right away. Download Chassis SDK To get started, make sure you set up a Python virtual enviornment and install the chassisml SDK. pip install chassisml Get Chassis Connection URL Sign up for the publicly-hosted service. Next, when you receive your connection link, use the URL and ChassisClient object to establish connection to the running service. The information you receive will look something like this: chassis_client = chassisml . ChassisClient ( \"https://chassis-xxxxxxxxxx.modzy.com\" ) Begin Using Chassis With your environment set up and connection URL in hand, you can now start to integrate the service into your MLOps pipelines. Check out this example to follow along and see Chassis in action. Just insert your URL into the aforementioned client connection and you're well on your way.","title":"Connect to Hosted Service"},{"location":"getting-started/deploy-connect/#connect-to-hosted-service","text":"Welcome! Connecting to this service eliminates the need for you to deploy and stand up a private Kubernetes cluster. Each chassis build job run on our hosted service has enough resources to containerize even the most memory intensive ML models (up to 8GB RAM and 2 CPUs). Follow the instructions on this page to connect and get started using Chassis right away.","title":"Connect to Hosted Service"},{"location":"getting-started/deploy-connect/#download-chassis-sdk","text":"To get started, make sure you set up a Python virtual enviornment and install the chassisml SDK. pip install chassisml","title":"Download Chassis SDK"},{"location":"getting-started/deploy-connect/#get-chassis-connection-url","text":"Sign up for the publicly-hosted service. Next, when you receive your connection link, use the URL and ChassisClient object to establish connection to the running service. The information you receive will look something like this: chassis_client = chassisml . ChassisClient ( \"https://chassis-xxxxxxxxxx.modzy.com\" )","title":"Get Chassis Connection URL"},{"location":"getting-started/deploy-connect/#begin-using-chassis","text":"With your environment set up and connection URL in hand, you can now start to integrate the service into your MLOps pipelines. Check out this example to follow along and see Chassis in action. Just insert your URL into the aforementioned client connection and you're well on your way.","title":"Begin Using Chassis"},{"location":"getting-started/deploy-manual/","text":"Deploy Service Manually Different Connection Options Before following this guide, note that you can connect to the Chassis service in one of two ways: Continue following this guide to install the Chassis service locally on a private Kubernetes cluster Bypass this guide and follow the instructions to connect to our publicly-hosted and free instance of the service Install required dependencies Install Docker Desktop Try to run docker ps If you get a permissions error, follow instructions here Install Helm Enable Kubernetes Follow these instructions to enable Kubernetes in Docker Desktop. Add the Helm repository helm repo add chassis https://modzy.github.io/chassis After that we just need to update the Helm repos to fetch Chassis data. helm repo update Install Chassis service Now we just need to install Chassis as normal using Helm. helm install chassis chassis/chassis Check the installation After having installed the service we can check that the Chassis service is correctly deployed. kubectl get svc/chassis Then you should see an output similar to this. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE chassis NodePort 10 .106.209.207 <none> 5000 :30496/TCP 15s We can also check that the pod that runs the service is correctly running. kubectl get pods Where we should find our pod listed. NAME READY STATUS RESTARTS AGE ( ... ) chassis-5c574d459c-rclx9 1 /1 Running 0 22s ( ... ) Query the service To conclude, we may want to query the service just to see that it answers as we expect. To do that, we need to port forward the service. kubectl port-forward svc/chassis 5000 :5000 Now that we have access to the service we can query it. curl localhost:5000 Which should output an alive message. Begin Using the Service Congratulations, you have now successfully deployed the service in a private Kubernetes cluster. To get started, make sure you set up a Python virtual enviornment and install the chassisml SDK. pip install chassisml For more resources, check out our tutorials and how-to guides","title":"Deploy Service Manually"},{"location":"getting-started/deploy-manual/#deploy-service-manually","text":"Different Connection Options Before following this guide, note that you can connect to the Chassis service in one of two ways: Continue following this guide to install the Chassis service locally on a private Kubernetes cluster Bypass this guide and follow the instructions to connect to our publicly-hosted and free instance of the service","title":"Deploy Service Manually"},{"location":"getting-started/deploy-manual/#install-required-dependencies","text":"Install Docker Desktop Try to run docker ps If you get a permissions error, follow instructions here Install Helm","title":"Install required dependencies"},{"location":"getting-started/deploy-manual/#enable-kubernetes","text":"Follow these instructions to enable Kubernetes in Docker Desktop.","title":"Enable Kubernetes"},{"location":"getting-started/deploy-manual/#add-the-helm-repository","text":"helm repo add chassis https://modzy.github.io/chassis After that we just need to update the Helm repos to fetch Chassis data. helm repo update","title":"Add the Helm repository"},{"location":"getting-started/deploy-manual/#install-chassis-service","text":"Now we just need to install Chassis as normal using Helm. helm install chassis chassis/chassis","title":"Install Chassis service"},{"location":"getting-started/deploy-manual/#check-the-installation","text":"After having installed the service we can check that the Chassis service is correctly deployed. kubectl get svc/chassis Then you should see an output similar to this. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE chassis NodePort 10 .106.209.207 <none> 5000 :30496/TCP 15s We can also check that the pod that runs the service is correctly running. kubectl get pods Where we should find our pod listed. NAME READY STATUS RESTARTS AGE ( ... ) chassis-5c574d459c-rclx9 1 /1 Running 0 22s ( ... )","title":"Check the installation"},{"location":"getting-started/deploy-manual/#query-the-service","text":"To conclude, we may want to query the service just to see that it answers as we expect. To do that, we need to port forward the service. kubectl port-forward svc/chassis 5000 :5000 Now that we have access to the service we can query it. curl localhost:5000 Which should output an alive message.","title":"Query the service"},{"location":"getting-started/deploy-manual/#begin-using-the-service","text":"Congratulations, you have now successfully deployed the service in a private Kubernetes cluster. To get started, make sure you set up a Python virtual enviornment and install the chassisml SDK. pip install chassisml For more resources, check out our tutorials and how-to guides","title":"Begin Using the Service"},{"location":"how-to-guides/arm-support/","text":"OS & Arm Support AI/ML models, Docker containers, and processors - what are they, how do they work together, and why mention them in relation to Chassisml? If you are familiar with the Chassisml service, you know that it is a tool Data Scientists can leverage to push their AI/ML models into a production application, without having any DevOps knowledge or experience. The way in which this happens is by auto-packaging model code into a Docker container, which allows the model to be shipped to various ModelOps/MLOps/Model serving platforms, across both the commercial and open-source landscapes. In most cases, these containers are built to run on Intel processors, which are more commonly found in larger devices such as desktop computers. This is great for running models in data centers or cloud-based infrastructure, but it does not bode well for running these models on any sort of mobile or edge device. ARM processors come in handy in these situations. As AI edge processing becomes more and more desirable (think AI running directly on a drone or security camera as an example), it is critical to be able to compile containers into an ARM-architecture supported format. This page walks through the process of automatically building a model container that can run on ARM, with the option to also make it GPU-compatible on an ARM architecture. Enable Arm Support To get started, we will install our required dependencies. import chassisml import pickle import cv2 import torch import getpass import numpy as np import torchvision.models as models from torchvision import transforms Next, we will load the pretrained ResNet50 model, define a data transformation object, and define a device variable. This is how we cast both our model and data to the CPU ( device=\"cpu\" ) or GPU ( device=\"cuda\" ). model = models . resnet50 ( pretrained = True ) model . eval () labels = pickle . load ( open ( './data/imagenet_labels.pkl' , 'rb' )) transform = transforms . Compose ([ transforms . ToPILImage (), transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) device = 'cpu' model . to ( device ) Most deep learning frameworks have built-in support for batch processing. This support includes different dataloader functionalities that will take an entire folder of data in some cases and process it in a way that can be fed to a neural network in the proper tensor form. We will define a batch_process function that takes a list of inputs, formats them into the structure our model expects, and runs inference on the batch of data. def batch_process ( inputs ): # preprocess list of inputs images = [] for input_bytes in inputs : decoded = cv2 . imdecode ( np . frombuffer ( input_bytes , np . uint8 ), - 1 ) resized = cv2 . resize ( decoded , ( 224 , 224 )) . reshape (( 1 , 224 , 224 , 3 )) images . append ( resized ) images_arr = np . concatenate ( images ) batch_t = torch . stack ( tuple ( transform ( i ) for i in images_arr ), dim = 0 ) . to ( device ) # run batch inference and add softmax layer output = model ( batch_t ) probs = torch . nn . functional . softmax ( output , dim = 1 ) softmax_preds = probs . detach () . cpu () . numpy () # postprocess all_formatted_results = [] for preds in softmax_preds : indices = np . argsort ( preds )[:: - 1 ] classes = [ labels [ idx ] for idx in indices [: 5 ]] scores = [ float ( preds [ idx ]) for idx in indices [: 5 ]] preds = [{ \"class\" : \" {} \" . format ( label ), \"score\" : round ( float ( score ), 3 )} for label , score in zip ( classes , scores )] preds . sort ( key = lambda x : x [ \"score\" ], reverse = True ) results = { \"classPredictions\" : preds } all_formatted_results . append ( results ) # output list of formatted results return all_formatted_results When we create our ChassisModel object, we will pass this batch_process function through as a parameter. NOTE: If you would also like to define a process function that only performs inference on a single piece of data instead of batch, you can do so as well and pass both through as parameters. In this case, our batch_process will work if we pass through either a single piece of data or batch. Now, initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( batch_process_fn = batch_process , batch_size = 4 ) Test chassis_model locally (both single and batch data). sample_filepath = './data/airplane.jpg' results = chassis_model . test ( sample_filepath ) print ( results ) results = chassis_model . test_batch ( sample_filepath ) print ( results ) Up until this point, creating a container that can run on ARM has been exactly the same as the normal Chassisml workflow. To enable ARM support, all we need to do is turn on the arm64 flag. Turn this flag on and publish our model with your specified Docker credentials. dockerhub_user = < my . username > dockerhob_pass = < my . password > response = chassis_model . publish ( model_name = \"PyTorch ResNet50 Image Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , arm64 = True ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) Enable Arm + GPU Support Note The ARM + GPU option is in alpha and has only been tested on the NVIDIA Jetson Nano device. Enabling ARM & GPU support requires one more flag set to true during the publish method. Repeate the steps outlined in the above section with the one difference being a slight change the device variable. model = models . resnet50 ( pretrained = True ) model . eval () labels = pickle . load ( open ( './data/imagenet_labels.pkl' , 'rb' )) transform = transforms . Compose ([ transforms . ToPILImage (), transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) device = 'cuda' model . to ( device ) Next, publish your model with both the arm64 and gpu flags turned on. dockerhub_user = < my . username > dockerhob_pass = < my . password > response = chassis_model . publish ( model_name = \"PyTorch ResNet50 Image Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , arm64 = True , gpu = True ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"Arm Support"},{"location":"how-to-guides/arm-support/#os-arm-support","text":"AI/ML models, Docker containers, and processors - what are they, how do they work together, and why mention them in relation to Chassisml? If you are familiar with the Chassisml service, you know that it is a tool Data Scientists can leverage to push their AI/ML models into a production application, without having any DevOps knowledge or experience. The way in which this happens is by auto-packaging model code into a Docker container, which allows the model to be shipped to various ModelOps/MLOps/Model serving platforms, across both the commercial and open-source landscapes. In most cases, these containers are built to run on Intel processors, which are more commonly found in larger devices such as desktop computers. This is great for running models in data centers or cloud-based infrastructure, but it does not bode well for running these models on any sort of mobile or edge device. ARM processors come in handy in these situations. As AI edge processing becomes more and more desirable (think AI running directly on a drone or security camera as an example), it is critical to be able to compile containers into an ARM-architecture supported format. This page walks through the process of automatically building a model container that can run on ARM, with the option to also make it GPU-compatible on an ARM architecture.","title":"OS &amp; Arm Support"},{"location":"how-to-guides/arm-support/#enable-arm-support","text":"To get started, we will install our required dependencies. import chassisml import pickle import cv2 import torch import getpass import numpy as np import torchvision.models as models from torchvision import transforms Next, we will load the pretrained ResNet50 model, define a data transformation object, and define a device variable. This is how we cast both our model and data to the CPU ( device=\"cpu\" ) or GPU ( device=\"cuda\" ). model = models . resnet50 ( pretrained = True ) model . eval () labels = pickle . load ( open ( './data/imagenet_labels.pkl' , 'rb' )) transform = transforms . Compose ([ transforms . ToPILImage (), transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) device = 'cpu' model . to ( device ) Most deep learning frameworks have built-in support for batch processing. This support includes different dataloader functionalities that will take an entire folder of data in some cases and process it in a way that can be fed to a neural network in the proper tensor form. We will define a batch_process function that takes a list of inputs, formats them into the structure our model expects, and runs inference on the batch of data. def batch_process ( inputs ): # preprocess list of inputs images = [] for input_bytes in inputs : decoded = cv2 . imdecode ( np . frombuffer ( input_bytes , np . uint8 ), - 1 ) resized = cv2 . resize ( decoded , ( 224 , 224 )) . reshape (( 1 , 224 , 224 , 3 )) images . append ( resized ) images_arr = np . concatenate ( images ) batch_t = torch . stack ( tuple ( transform ( i ) for i in images_arr ), dim = 0 ) . to ( device ) # run batch inference and add softmax layer output = model ( batch_t ) probs = torch . nn . functional . softmax ( output , dim = 1 ) softmax_preds = probs . detach () . cpu () . numpy () # postprocess all_formatted_results = [] for preds in softmax_preds : indices = np . argsort ( preds )[:: - 1 ] classes = [ labels [ idx ] for idx in indices [: 5 ]] scores = [ float ( preds [ idx ]) for idx in indices [: 5 ]] preds = [{ \"class\" : \" {} \" . format ( label ), \"score\" : round ( float ( score ), 3 )} for label , score in zip ( classes , scores )] preds . sort ( key = lambda x : x [ \"score\" ], reverse = True ) results = { \"classPredictions\" : preds } all_formatted_results . append ( results ) # output list of formatted results return all_formatted_results When we create our ChassisModel object, we will pass this batch_process function through as a parameter. NOTE: If you would also like to define a process function that only performs inference on a single piece of data instead of batch, you can do so as well and pass both through as parameters. In this case, our batch_process will work if we pass through either a single piece of data or batch. Now, initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( batch_process_fn = batch_process , batch_size = 4 ) Test chassis_model locally (both single and batch data). sample_filepath = './data/airplane.jpg' results = chassis_model . test ( sample_filepath ) print ( results ) results = chassis_model . test_batch ( sample_filepath ) print ( results ) Up until this point, creating a container that can run on ARM has been exactly the same as the normal Chassisml workflow. To enable ARM support, all we need to do is turn on the arm64 flag. Turn this flag on and publish our model with your specified Docker credentials. dockerhub_user = < my . username > dockerhob_pass = < my . password > response = chassis_model . publish ( model_name = \"PyTorch ResNet50 Image Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , arm64 = True ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"Enable Arm Support"},{"location":"how-to-guides/arm-support/#enable-arm-gpu-support","text":"Note The ARM + GPU option is in alpha and has only been tested on the NVIDIA Jetson Nano device. Enabling ARM & GPU support requires one more flag set to true during the publish method. Repeate the steps outlined in the above section with the one difference being a slight change the device variable. model = models . resnet50 ( pretrained = True ) model . eval () labels = pickle . load ( open ( './data/imagenet_labels.pkl' , 'rb' )) transform = transforms . Compose ([ transforms . ToPILImage (), transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) device = 'cuda' model . to ( device ) Next, publish your model with both the arm64 and gpu flags turned on. dockerhub_user = < my . username > dockerhob_pass = < my . password > response = chassis_model . publish ( model_name = \"PyTorch ResNet50 Image Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , arm64 = True , gpu = True ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"Enable Arm + GPU Support"},{"location":"how-to-guides/explainability/","text":"Adding in Explainability LIME SHAP","title":"Explainability"},{"location":"how-to-guides/explainability/#adding-in-explainability","text":"","title":"Adding in Explainability"},{"location":"how-to-guides/explainability/#lime","text":"","title":"LIME"},{"location":"how-to-guides/explainability/#shap","text":"","title":"SHAP"},{"location":"how-to-guides/frameworks/","text":"Building Containers for Common Machine Learning and Deep Learning Frameworks If you build models using a common machine learning framework and you want a way to containerize them automatically, you have come to the right place! This page provides guides for leveraging Chassis to create containers out of your models built from the most popular ML frameworks available. NOTE : This list of frameworks is not all-inclusive; instead, it is just a collection of examples from commonly-used frameworks we have seen data scientists gravitate towards. Can't find the framework you are looking for? Feel free to fork this repository, add an example or two from your framework of choice, and open a PR. Or come chat with us directly on Discord ! Requirements To follow these how-to guides, you must first install the chassisml Python SDK and connect to the Chassis service either on your local machine or to our publicly-hosted instance within your Python IDE. You also will need an account with Dockerhub . For help getting started, visit our Tutorials page. PyTorch This guide builds a simple Image Classification model with a ResNet50 architecture, avaialable directly in PyTorch's Torvision model library . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import chassisml import pickle import cv2 import torch import numpy as np import torchvision.models as models from torchvision import transforms Load model, labels, and any other dependencies required for inference. model = models . resnet50 ( pretrained = True ) model . eval () labels = pickle . load ( open ( './data/imagenet_labels.pkl' , 'rb' )) transform = transforms . Compose ([ transforms . ToPILImage (), transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) device = 'cpu' Next, define process function that will be passed as the input parameter to create a ChassisModel object. def process ( input_bytes ): # preprocess decoded = cv2 . imdecode ( np . frombuffer ( input_bytes , np . uint8 ), - 1 ) img_t = transform ( decoded ) batch_t = torch . unsqueeze ( img_t , 0 ) . to ( device ) # run inference predictions = model ( batch_t ) # postprocess percentage = torch . nn . functional . softmax ( predictions , dim = 1 )[ 0 ] _ , indices = torch . sort ( predictions , descending = True ) inference_result = { \"classPredictions\" : [ { \"class\" : labels [ idx . item ()], \"score\" : percentage [ idx ] . item ()} for idx in indices [ 0 ][: 5 ] ] } structured_output = { \"data\" : { \"result\" : inference_result , \"explanation\" : None , \"drift\" : None , } } return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/airplane.jpg' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"PyTorch ResNet50 Image Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) Scikit-learn This guide trains a Logistic Regression classifier on the Digits Dataset and auto-containerizes via the chassisml Python SDK. Visit the original classification exercise here . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import chassisml import numpy as np import getpass import json from sklearn.linear_model import LogisticRegression from sklearn import datasets Load dataset, perform data preprocessing, and train the model. # Import and normalize data X_digits , y_digits = datasets . load_digits ( return_X_y = True ) X_digits = X_digits / X_digits . max () n_samples = len ( X_digits ) # Split data into training and test sets X_train = X_digits [: int ( 0.9 * n_samples )] y_train = y_digits [: int ( 0.9 * n_samples )] X_test = X_digits [ int ( 0.9 * n_samples ) :] y_test = y_digits [ int ( 0.9 * n_samples ) :] # Train Model logistic = LogisticRegression ( max_iter = 1000 ) print ( \"LogisticRegression mean accuracy score: %f \" % logistic . fit ( X_train , y_train ) . score ( X_test , y_test ) ) # Save small sample input to use for testing later sample = X_test [: 5 ] . tolist () with open ( \"data/digits_sample.json\" , 'w' ) as out : json . dump ( sample , out ) Next, prepare a process function that will be passed as the input parameter to create a ChassisModel object. def process ( input_bytes ): inputs = np . array ( json . loads ( input_bytes )) inference_results = logistic . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/digits_sample.json' results = chassis_model . test ( sample_filepath ) print ( results ) Manually construct a conda environment to pass through as parameter to Chassis job. NOTE: By default, Chassis will infer all required dependencies, objects, and functions based on what is referenced within the process method. However, if you prefer to manually define your environment, you can do so as well. env = { \"name\" : \"sklearn-chassis\" , \"channels\" : [ 'conda-forge' ], \"dependencies\" : [ \"python=3.8.5\" , { \"pip\" : [ \"scikit-learn\" , \"numpy\" , \"chassisml\" ] } ] } Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"Sklearn Logistic Regression Digits Image Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , conda_env = env ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) XGBoost This guide trains an XGBoost regression model that predicts housing prices based on a series of features in the Boston Housing tabular dataset . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import cv2 import chassisml from io import StringIO import numpy as np import pandas as pd import getpass import json import xgboost as xgb from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error Load dataset, build XGBoost regressor model, and train the model. # load data boston = load_boston () X = pd . DataFrame ( boston . data , columns = boston . feature_names ) y = pd . Series ( boston . target ) X_train , X_test , y_train , y_test = train_test_split ( X , y ) # save sample data for testing later with open ( \"data/sample_house_data.csv\" , \"w\" ) as f : X_test [: 10 ] . to_csv ( f , index = False ) # build XGBoost regressor regressor = xgb . XGBRegressor ( n_estimators = 100 , reg_lambda = 1 , gamma = 0 , max_depth = 3 ) # train model regressor . fit ( X_train , y_train ) The fit() execution will print the following in your notebook: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, enable_categorical=False, gamma=0, gpu_id=-1, importance_type=None, interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=3, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=12, num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Run inference on your X_test subset and evaluate model performance. # run inference y_pred = regressor . predict ( X_test ) # evaluate model mean_squared_error ( y_test , y_pred ) >>>> 7.658965947061991 Next, prepare a process function that will be passed as the input parameter to create a ChassisModel object. def process ( input_bytes ): # load data inputs = pd . read_csv ( StringIO ( str ( input_bytes , \"utf-8\" ))) # run inference preds = regressor . predict ( inputs ) # structure results inference_result = { \"housePricePredictions\" : [ { \"row\" : i + 1 , \"price\" : preds [ i ] . round ( 0 ) * 1000 } for i in range ( len ( preds )) ] } structured_output = { \"data\" : { \"result\" : inference_result , \"explanation\" : None , \"drift\" : None , } } return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/sample_house_data.csv' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will successfully run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"XGBoost Boston Housing Price Predictions\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) LightGBM This guide builds a LightGBM classifier model to predict the likelihood of women being diagnosed with Breast Cancer. This guide was adapted from this Kaggle notebook . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import chassisml import numpy as np import getpass import json import pandas as pd from io import StringIO import lightgbm as lgb from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split Load dataset, perform data preprocessing, build and then train LightGBM classifier model. # load breast cancer dataset df = pd . read_csv ( './data/Breast_cancer_data.csv' ) # preprocess data X = df [[ 'mean_radius' , 'mean_texture' , 'mean_perimeter' , 'mean_area' , 'mean_smoothness' ]] y = df [ 'diagnosis' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) # build and train model clf = lgb . LGBMClassifier () clf . fit ( X_train , y_train ) Run inference on your test subset and evaluate model accuracy y_pred = clf . predict ( X_test ) accuracy = accuracy_score ( y_pred , y_test ) print ( 'LightGBM Model accuracy score: {0:0.4f} ' . format ( accuracy_score ( y_test , y_pred ))) You should see this in your console: LightGBM Model accuracy score: 0.9298 Next, prepare a process function that will be passed as the input parameter to create a ChassisModel object. labels = [ \"No Cancer\" , \"Cancer\" ] def process ( input_bytes ): inputs = pd . read_csv ( StringIO ( str ( input_bytes , \"utf-8\" ))) preds = clf . predict_proba ( inputs ) inference_result = { \"classPredictions\" : [ { \"row\" : i + 1 , \"class\" : labels [ np . argmax ( pred )], \"score\" : np . max ( pred )} for i , pred in enumerate ( preds ) ] } structured_output = { \"data\" : { \"result\" : inference_result , \"explanation\" : None , \"drift\" : None , } } return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = 'data/sample_cancer_data.csv' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will successfully run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"Chassis LightGBM Breast Cancer Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) Fastai This builds off Fastai's Tabular training tutorial , which predicts the income of adults based on various education and socioeconomic factors. To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import os import chassisml import numpy as np import getpass import json import pandas as pd from io import StringIO from fastai.tabular.all import TabularDataLoaders , RandomSplitter , TabularPandas , tabular_learner , Categorify , FillMissing , Normalize , range_of , accuracy Begin by loading and preprocessing dataset. df = pd . read_csv ( \"./data/adult_sample/adult.csv\" ) df . head () dls = TabularDataLoaders . from_csv ( \"./data/adult_sample/adult.csv\" , path = os . path . join ( os . getcwd (), \"data/adult_sample\" ), y_names = \"salary\" , cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], procs = [ Categorify , FillMissing , Normalize ]) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) splits = RandomSplitter ( valid_pct = 0.2 )( range_of ( df )) to = TabularPandas ( df , procs = [ Categorify , FillMissing , Normalize ], cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], y_names = 'salary' , splits = splits ) # save test subset test_df = df . copy () test_df . drop ([ 'salary' ], axis = 1 , inplace = True ) test_df [: 20 ] . to_csv ( \"./data/sample_adult_data.csv\" , index = False ) # rebuild dataloaders with preprocessed data dls = to . dataloaders ( bs = 64 ) Now, we will build and train our model. learn = tabular_learner ( dls , metrics = accuracy ) learn . fit_one_cycle ( 1 ) The output of this training experiment should look similar to the following table: epoch train_loss valid_loss accuracy time 0 0.370123 0.364591 0.832002 00:03 View the full results of the training experiment. learn . show_results () Now, we will test the inference flow that works with the Fastai framework. test_df = pd . read_csv ( \"./data/sample_adult_data.csv\" ) dl = learn . dls . test_dl ( test_df ) preds = learn . get_preds ( dl = dl )[ 0 ] . numpy () Next, prepare a process function that will be passed as the input parameter to create a ChassisModel object. labels = [ '<50k' , '>50k' ] def process ( input_bytes ): inputs = pd . read_csv ( StringIO ( str ( input_bytes , \"utf-8\" ))) dl = learn . dls . test_dl ( inputs ) preds = learn . get_preds ( dl = dl )[ 0 ] . numpy () inference_result = { \"classPredictions\" : [ { \"row\" : i + 1 , \"predictions\" : [ { \"class\" : labels [ j ], \"score\" : round ( pred [ j ], 4 )} for j in range ( 2 ) ] } for i , pred in enumerate ( preds ) ] } structured_output = { \"data\" : { \"result\" : inference_result , \"explanation\" : None , \"drift\" : None , } } return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/sample_adult_data.csv' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will successfully run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"Fast AI Salary Prediction\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) MXNet This guide leverages the MXNet framework to build a simple Image Classification model with a MobileNet architecture, avaialable directly in MXNet's Gluon Model Zoo . This guide is an adaptation of MXNet's tutorial for using pretrained models. To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import cv2 import chassisml import numpy as np import getpass import json import mxnet as mx import mxnet.image as mxnet_img from mxnet import gluon , nd from mxnet.gluon.model_zoo import vision import matplotlib.pyplot as plt Load pretrained MobileNet model from Gluon Model Zoo ctx = mx . cpu () mobileNet = vision . mobilenet0_5 ( pretrained = True , ctx = ctx ) Next, load ImageNet labels file and sample dog image for testing. We will visualize this image in our notebook for reference. # load imagenet labels for postprocessing imagenet_labels = np . array ( json . load ( open ( 'data/image_net_labels.json' , 'r' ))) print ( imagenet_labels [ 4 ]) # load sample image filename = \"data/dog.jpg\" # visualize image to test image = mx . image . imread ( filename ) plt . imshow ( image . asnumpy ()) An image of a boxer dog should appear under this cell. Now, we will define an inference method specific to the MXNet framework. def predict ( model , image , categories , k = 3 ): predictions = model ( transform ( image )) . softmax () top_pred = predictions . topk ( k = k )[ 0 ] . asnumpy () probs = [] labels = [] for index in top_pred : probability = predictions [ 0 ][ int ( index )] probs . append ( probability . asscalar ()) category = categories [ int ( index )] labels . append ( category ) return probs , labels Test this function locally to ensure your model is behaving how you intend it to. # test model probs , labels = predict ( mobileNet , image , imagenet_labels , 3 ) print ( probs ) print ( labels ) >>> [ 0.84015656 , 0.13626784 , 0.006610237 ] >>> [ 'boxer' , 'bull mastiff' , 'Rhodesian ridgeback' ] Next, define process function that will be passed as the input parameter to create a ChassisModel object. def process ( input_bytes ): # read image bytes img = mxnet_img . imdecode ( input_bytes ) # run inference probs , labels = predict ( mobileNet , img , imagenet_labels , 3 ) # structure results inference_result = { \"classPredictions\" : [ { \"class\" : labels [ i ], \"score\" : probs [ i ]} for i in range ( len ( probs )) ] } structured_output = { \"data\" : { \"result\" : inference_result , \"explanation\" : None , \"drift\" : None , } } return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/dog.jpg' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"MXNET MobileNet Image Classifiction\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) ONNX This guide leverages the chassisml SDK to auto-containerize a pretrained model from the ONNX Model Zoo . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import cv2 import pickle import tempfile import chassisml import numpy as np import getpass from shutil import rmtree import json import onnx from onnx import backend from onnx import numpy_helper import onnxruntime as ort import matplotlib.pyplot as plt Load pretrained MobileNet model from ONNX Model Zoo and confirm it is a valid ONNX model. model = onnx . load ( \"models/mobilenetv2-7.onnx\" ) onnx . checker . check_model ( model ) Next, load sample data and execute a few minor preprocessing steps. # format sample data and reshape img = cv2 . cvtColor ( cv2 . imread ( 'data/dog.jpg' ), cv2 . COLOR_BGR2RGB ) img_show = cv2 . resize ( img , ( 224 , 224 )) sample_img = np . reshape ( img_show , ( 1 , 3 , 224 , 224 )) . astype ( np . float32 ) # load imagenet labels labels = pickle . load ( open ( './data/imagenet_labels.pkl' , 'rb' )) # visualize image plt . figure () plt . imshow ( img ) plt . colorbar () plt . grid ( False ) plt . show () An image of a yellow lab dog should appear under this cell. Now, we will create an ONNX runtime inference session and test our model locally. NOTE: Some frameworks have their own ONNX runtime compatibility built in, but ONNX Runtime (ORT) can be used to run models built in many common frameworks - including PyTorch, Tensorflow, TFLite, Scikit-Learn, to name a few - but saved in the standardized ONNX format. # create onnx runtime inference session and print top prediction session = ort . InferenceSession ( \"models/mobilenetv2-7.onnx\" ) results = session . run ( None , { \"input\" : sample_img }) print ( \"Top Prediction: {} \" . format ( labels [ results [ 0 ] . argmax ()])) >>> \"Top Prediction: sunglass\" Next, define process function that will be passed as the input parameter to create a ChassisModel object. One strict requirement to use Chassis is that your model must be able to be loaded into memory and perform an inference call from it's loaded, pre-trained state. Due to the unique ORT syntax that requires a model file be passed through as a parameter when creating an ORT inference session (which means it must be loaded to memory as a part of the inference session instantiation, not loaded before), we will get a little creative and leverage Python's tempfile library to take the loaded model, save it to a temporary directory as a .onnx file, and load it back in during inference. def process ( input_bytes ): # save model to filepath for inference tmp_dir = tempfile . mkdtemp () import onnx onnx . save ( model , \" {} /model.onnx\" . format ( tmp_dir )) # preprocess data decoded = cv2 . cvtColor ( cv2 . imdecode ( np . frombuffer ( input_bytes , np . uint8 ), - 1 ), cv2 . COLOR_BGR2RGB ) img = cv2 . resize ( decoded , ( 224 , 224 )) img = np . reshape ( img , ( 1 , 3 , 224 , 224 )) . astype ( np . float32 ) # run inference session = ort . InferenceSession ( \" {} /model.onnx\" . format ( tmp_dir )) results = session . run ( None , { \"input\" : img }) # postprocess inference_result = labels [ results [ 0 ] . argmax ()] # format results structured_result = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result )}]} } } # remove temp directory rmtree ( tmp_dir ) return structured_result Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/dog.jpg' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"ONNX MobileNet Image Classifiction\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) PMML Predictive Model Markup Language (PMML) is an XML-based model format that is widely accepted and used to save machine learning models. This guide leverages the chassisml SDK to auto-containerize a pretrained model from this PMML Scikit-Learn package model library . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import chassisml import numpy as np import getpass import json from io import StringIO from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import pandas as pd import numpy as np from sklearn_pmml_model.ensemble import PMMLForestClassifier First, load and preprocess training data. # Prepare data iris = load_iris () X = pd . DataFrame ( iris . data ) X . columns = np . array ( iris . feature_names ) y = pd . Series ( np . array ( iris . target_names )[ iris . target ]) y . name = \"Class\" Xtr , Xte , ytr , yte = train_test_split ( X , y , test_size = 0.33 , random_state = 123 ) # Create sample data for testing later with open ( \"data/sample_iris.csv\" , \"w\" ) as f : Xte [: 10 ] . to_csv ( f , index = False ) Next, load the .pmml model file with the sklearn_pmml convenience package. We can then run a sample inference test with our test subset. # Load model clf = PMMLForestClassifier ( pmml = \"models/randomForest.pmml\" ) labels = clf . classes_ . tolist () # Test model clf . predict ( Xte ) clf . score ( Xte , yte ) Now, we will define some pre- and post-processing methods that our model can use during inference. def preprocess_inputs ( raw_input_bytes ): # load data inputs = pd . read_csv ( StringIO ( str ( raw_input_bytes , \"utf-8\" ))) return inputs def postprocess_outputs ( raw_predictions ): # process output inference_result = { \"result\" :[ { \"row\" : i + 1 , \"classPredictions\" : [ { \"class\" : labels [ idx ], \"score\" : results [ idx ]} for idx in np . argsort ( results )[:: - 1 ] ] } for i , results in enumerate ( raw_predictions ) ] } # format output structured_output = { \"data\" : { \"result\" : inference_result [ \"result\" ], \"explanation\" : None , \"drift\" : None , } } return structured_output Next, define process function that will be passed as the input parameter to create a ChassisModel object. def process ( input_bytes ): # load data inputs = preprocess_inputs ( input_bytes ) # make predictions output = clf . predict_proba ( inputs ) # process output structured_output = postprocess_outputs ( output ) return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/sample_iris.csv' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"PMML Random Forest Iris Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) Tensorflow & Keras Coming Soon Spacy Coming Soon Spark MLlib Coming Soon","title":"Model Containers by Framework"},{"location":"how-to-guides/frameworks/#building-containers-for-common-machine-learning-and-deep-learning-frameworks","text":"If you build models using a common machine learning framework and you want a way to containerize them automatically, you have come to the right place! This page provides guides for leveraging Chassis to create containers out of your models built from the most popular ML frameworks available. NOTE : This list of frameworks is not all-inclusive; instead, it is just a collection of examples from commonly-used frameworks we have seen data scientists gravitate towards. Can't find the framework you are looking for? Feel free to fork this repository, add an example or two from your framework of choice, and open a PR. Or come chat with us directly on Discord ! Requirements To follow these how-to guides, you must first install the chassisml Python SDK and connect to the Chassis service either on your local machine or to our publicly-hosted instance within your Python IDE. You also will need an account with Dockerhub . For help getting started, visit our Tutorials page.","title":"Building Containers for Common Machine Learning and Deep Learning Frameworks"},{"location":"how-to-guides/frameworks/#pytorch","text":"This guide builds a simple Image Classification model with a ResNet50 architecture, avaialable directly in PyTorch's Torvision model library . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import chassisml import pickle import cv2 import torch import numpy as np import torchvision.models as models from torchvision import transforms Load model, labels, and any other dependencies required for inference. model = models . resnet50 ( pretrained = True ) model . eval () labels = pickle . load ( open ( './data/imagenet_labels.pkl' , 'rb' )) transform = transforms . Compose ([ transforms . ToPILImage (), transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) device = 'cpu' Next, define process function that will be passed as the input parameter to create a ChassisModel object. def process ( input_bytes ): # preprocess decoded = cv2 . imdecode ( np . frombuffer ( input_bytes , np . uint8 ), - 1 ) img_t = transform ( decoded ) batch_t = torch . unsqueeze ( img_t , 0 ) . to ( device ) # run inference predictions = model ( batch_t ) # postprocess percentage = torch . nn . functional . softmax ( predictions , dim = 1 )[ 0 ] _ , indices = torch . sort ( predictions , descending = True ) inference_result = { \"classPredictions\" : [ { \"class\" : labels [ idx . item ()], \"score\" : percentage [ idx ] . item ()} for idx in indices [ 0 ][: 5 ] ] } structured_output = { \"data\" : { \"result\" : inference_result , \"explanation\" : None , \"drift\" : None , } } return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/airplane.jpg' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"PyTorch ResNet50 Image Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"PyTorch"},{"location":"how-to-guides/frameworks/#scikit-learn","text":"This guide trains a Logistic Regression classifier on the Digits Dataset and auto-containerizes via the chassisml Python SDK. Visit the original classification exercise here . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import chassisml import numpy as np import getpass import json from sklearn.linear_model import LogisticRegression from sklearn import datasets Load dataset, perform data preprocessing, and train the model. # Import and normalize data X_digits , y_digits = datasets . load_digits ( return_X_y = True ) X_digits = X_digits / X_digits . max () n_samples = len ( X_digits ) # Split data into training and test sets X_train = X_digits [: int ( 0.9 * n_samples )] y_train = y_digits [: int ( 0.9 * n_samples )] X_test = X_digits [ int ( 0.9 * n_samples ) :] y_test = y_digits [ int ( 0.9 * n_samples ) :] # Train Model logistic = LogisticRegression ( max_iter = 1000 ) print ( \"LogisticRegression mean accuracy score: %f \" % logistic . fit ( X_train , y_train ) . score ( X_test , y_test ) ) # Save small sample input to use for testing later sample = X_test [: 5 ] . tolist () with open ( \"data/digits_sample.json\" , 'w' ) as out : json . dump ( sample , out ) Next, prepare a process function that will be passed as the input parameter to create a ChassisModel object. def process ( input_bytes ): inputs = np . array ( json . loads ( input_bytes )) inference_results = logistic . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/digits_sample.json' results = chassis_model . test ( sample_filepath ) print ( results ) Manually construct a conda environment to pass through as parameter to Chassis job. NOTE: By default, Chassis will infer all required dependencies, objects, and functions based on what is referenced within the process method. However, if you prefer to manually define your environment, you can do so as well. env = { \"name\" : \"sklearn-chassis\" , \"channels\" : [ 'conda-forge' ], \"dependencies\" : [ \"python=3.8.5\" , { \"pip\" : [ \"scikit-learn\" , \"numpy\" , \"chassisml\" ] } ] } Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"Sklearn Logistic Regression Digits Image Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , conda_env = env ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"Scikit-learn"},{"location":"how-to-guides/frameworks/#xgboost","text":"This guide trains an XGBoost regression model that predicts housing prices based on a series of features in the Boston Housing tabular dataset . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import cv2 import chassisml from io import StringIO import numpy as np import pandas as pd import getpass import json import xgboost as xgb from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error Load dataset, build XGBoost regressor model, and train the model. # load data boston = load_boston () X = pd . DataFrame ( boston . data , columns = boston . feature_names ) y = pd . Series ( boston . target ) X_train , X_test , y_train , y_test = train_test_split ( X , y ) # save sample data for testing later with open ( \"data/sample_house_data.csv\" , \"w\" ) as f : X_test [: 10 ] . to_csv ( f , index = False ) # build XGBoost regressor regressor = xgb . XGBRegressor ( n_estimators = 100 , reg_lambda = 1 , gamma = 0 , max_depth = 3 ) # train model regressor . fit ( X_train , y_train ) The fit() execution will print the following in your notebook: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, enable_categorical=False, gamma=0, gpu_id=-1, importance_type=None, interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=3, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=12, num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Run inference on your X_test subset and evaluate model performance. # run inference y_pred = regressor . predict ( X_test ) # evaluate model mean_squared_error ( y_test , y_pred ) >>>> 7.658965947061991 Next, prepare a process function that will be passed as the input parameter to create a ChassisModel object. def process ( input_bytes ): # load data inputs = pd . read_csv ( StringIO ( str ( input_bytes , \"utf-8\" ))) # run inference preds = regressor . predict ( inputs ) # structure results inference_result = { \"housePricePredictions\" : [ { \"row\" : i + 1 , \"price\" : preds [ i ] . round ( 0 ) * 1000 } for i in range ( len ( preds )) ] } structured_output = { \"data\" : { \"result\" : inference_result , \"explanation\" : None , \"drift\" : None , } } return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/sample_house_data.csv' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will successfully run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"XGBoost Boston Housing Price Predictions\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"XGBoost"},{"location":"how-to-guides/frameworks/#lightgbm","text":"This guide builds a LightGBM classifier model to predict the likelihood of women being diagnosed with Breast Cancer. This guide was adapted from this Kaggle notebook . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import chassisml import numpy as np import getpass import json import pandas as pd from io import StringIO import lightgbm as lgb from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split Load dataset, perform data preprocessing, build and then train LightGBM classifier model. # load breast cancer dataset df = pd . read_csv ( './data/Breast_cancer_data.csv' ) # preprocess data X = df [[ 'mean_radius' , 'mean_texture' , 'mean_perimeter' , 'mean_area' , 'mean_smoothness' ]] y = df [ 'diagnosis' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) # build and train model clf = lgb . LGBMClassifier () clf . fit ( X_train , y_train ) Run inference on your test subset and evaluate model accuracy y_pred = clf . predict ( X_test ) accuracy = accuracy_score ( y_pred , y_test ) print ( 'LightGBM Model accuracy score: {0:0.4f} ' . format ( accuracy_score ( y_test , y_pred ))) You should see this in your console: LightGBM Model accuracy score: 0.9298 Next, prepare a process function that will be passed as the input parameter to create a ChassisModel object. labels = [ \"No Cancer\" , \"Cancer\" ] def process ( input_bytes ): inputs = pd . read_csv ( StringIO ( str ( input_bytes , \"utf-8\" ))) preds = clf . predict_proba ( inputs ) inference_result = { \"classPredictions\" : [ { \"row\" : i + 1 , \"class\" : labels [ np . argmax ( pred )], \"score\" : np . max ( pred )} for i , pred in enumerate ( preds ) ] } structured_output = { \"data\" : { \"result\" : inference_result , \"explanation\" : None , \"drift\" : None , } } return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = 'data/sample_cancer_data.csv' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will successfully run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"Chassis LightGBM Breast Cancer Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"LightGBM"},{"location":"how-to-guides/frameworks/#fastai","text":"This builds off Fastai's Tabular training tutorial , which predicts the income of adults based on various education and socioeconomic factors. To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import os import chassisml import numpy as np import getpass import json import pandas as pd from io import StringIO from fastai.tabular.all import TabularDataLoaders , RandomSplitter , TabularPandas , tabular_learner , Categorify , FillMissing , Normalize , range_of , accuracy Begin by loading and preprocessing dataset. df = pd . read_csv ( \"./data/adult_sample/adult.csv\" ) df . head () dls = TabularDataLoaders . from_csv ( \"./data/adult_sample/adult.csv\" , path = os . path . join ( os . getcwd (), \"data/adult_sample\" ), y_names = \"salary\" , cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], procs = [ Categorify , FillMissing , Normalize ]) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) splits = RandomSplitter ( valid_pct = 0.2 )( range_of ( df )) to = TabularPandas ( df , procs = [ Categorify , FillMissing , Normalize ], cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], y_names = 'salary' , splits = splits ) # save test subset test_df = df . copy () test_df . drop ([ 'salary' ], axis = 1 , inplace = True ) test_df [: 20 ] . to_csv ( \"./data/sample_adult_data.csv\" , index = False ) # rebuild dataloaders with preprocessed data dls = to . dataloaders ( bs = 64 ) Now, we will build and train our model. learn = tabular_learner ( dls , metrics = accuracy ) learn . fit_one_cycle ( 1 ) The output of this training experiment should look similar to the following table: epoch train_loss valid_loss accuracy time 0 0.370123 0.364591 0.832002 00:03 View the full results of the training experiment. learn . show_results () Now, we will test the inference flow that works with the Fastai framework. test_df = pd . read_csv ( \"./data/sample_adult_data.csv\" ) dl = learn . dls . test_dl ( test_df ) preds = learn . get_preds ( dl = dl )[ 0 ] . numpy () Next, prepare a process function that will be passed as the input parameter to create a ChassisModel object. labels = [ '<50k' , '>50k' ] def process ( input_bytes ): inputs = pd . read_csv ( StringIO ( str ( input_bytes , \"utf-8\" ))) dl = learn . dls . test_dl ( inputs ) preds = learn . get_preds ( dl = dl )[ 0 ] . numpy () inference_result = { \"classPredictions\" : [ { \"row\" : i + 1 , \"predictions\" : [ { \"class\" : labels [ j ], \"score\" : round ( pred [ j ], 4 )} for j in range ( 2 ) ] } for i , pred in enumerate ( preds ) ] } structured_output = { \"data\" : { \"result\" : inference_result , \"explanation\" : None , \"drift\" : None , } } return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/sample_adult_data.csv' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will successfully run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"Fast AI Salary Prediction\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"Fastai"},{"location":"how-to-guides/frameworks/#mxnet","text":"This guide leverages the MXNet framework to build a simple Image Classification model with a MobileNet architecture, avaialable directly in MXNet's Gluon Model Zoo . This guide is an adaptation of MXNet's tutorial for using pretrained models. To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import cv2 import chassisml import numpy as np import getpass import json import mxnet as mx import mxnet.image as mxnet_img from mxnet import gluon , nd from mxnet.gluon.model_zoo import vision import matplotlib.pyplot as plt Load pretrained MobileNet model from Gluon Model Zoo ctx = mx . cpu () mobileNet = vision . mobilenet0_5 ( pretrained = True , ctx = ctx ) Next, load ImageNet labels file and sample dog image for testing. We will visualize this image in our notebook for reference. # load imagenet labels for postprocessing imagenet_labels = np . array ( json . load ( open ( 'data/image_net_labels.json' , 'r' ))) print ( imagenet_labels [ 4 ]) # load sample image filename = \"data/dog.jpg\" # visualize image to test image = mx . image . imread ( filename ) plt . imshow ( image . asnumpy ()) An image of a boxer dog should appear under this cell. Now, we will define an inference method specific to the MXNet framework. def predict ( model , image , categories , k = 3 ): predictions = model ( transform ( image )) . softmax () top_pred = predictions . topk ( k = k )[ 0 ] . asnumpy () probs = [] labels = [] for index in top_pred : probability = predictions [ 0 ][ int ( index )] probs . append ( probability . asscalar ()) category = categories [ int ( index )] labels . append ( category ) return probs , labels Test this function locally to ensure your model is behaving how you intend it to. # test model probs , labels = predict ( mobileNet , image , imagenet_labels , 3 ) print ( probs ) print ( labels ) >>> [ 0.84015656 , 0.13626784 , 0.006610237 ] >>> [ 'boxer' , 'bull mastiff' , 'Rhodesian ridgeback' ] Next, define process function that will be passed as the input parameter to create a ChassisModel object. def process ( input_bytes ): # read image bytes img = mxnet_img . imdecode ( input_bytes ) # run inference probs , labels = predict ( mobileNet , img , imagenet_labels , 3 ) # structure results inference_result = { \"classPredictions\" : [ { \"class\" : labels [ i ], \"score\" : probs [ i ]} for i in range ( len ( probs )) ] } structured_output = { \"data\" : { \"result\" : inference_result , \"explanation\" : None , \"drift\" : None , } } return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/dog.jpg' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"MXNET MobileNet Image Classifiction\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"MXNet"},{"location":"how-to-guides/frameworks/#onnx","text":"This guide leverages the chassisml SDK to auto-containerize a pretrained model from the ONNX Model Zoo . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import cv2 import pickle import tempfile import chassisml import numpy as np import getpass from shutil import rmtree import json import onnx from onnx import backend from onnx import numpy_helper import onnxruntime as ort import matplotlib.pyplot as plt Load pretrained MobileNet model from ONNX Model Zoo and confirm it is a valid ONNX model. model = onnx . load ( \"models/mobilenetv2-7.onnx\" ) onnx . checker . check_model ( model ) Next, load sample data and execute a few minor preprocessing steps. # format sample data and reshape img = cv2 . cvtColor ( cv2 . imread ( 'data/dog.jpg' ), cv2 . COLOR_BGR2RGB ) img_show = cv2 . resize ( img , ( 224 , 224 )) sample_img = np . reshape ( img_show , ( 1 , 3 , 224 , 224 )) . astype ( np . float32 ) # load imagenet labels labels = pickle . load ( open ( './data/imagenet_labels.pkl' , 'rb' )) # visualize image plt . figure () plt . imshow ( img ) plt . colorbar () plt . grid ( False ) plt . show () An image of a yellow lab dog should appear under this cell. Now, we will create an ONNX runtime inference session and test our model locally. NOTE: Some frameworks have their own ONNX runtime compatibility built in, but ONNX Runtime (ORT) can be used to run models built in many common frameworks - including PyTorch, Tensorflow, TFLite, Scikit-Learn, to name a few - but saved in the standardized ONNX format. # create onnx runtime inference session and print top prediction session = ort . InferenceSession ( \"models/mobilenetv2-7.onnx\" ) results = session . run ( None , { \"input\" : sample_img }) print ( \"Top Prediction: {} \" . format ( labels [ results [ 0 ] . argmax ()])) >>> \"Top Prediction: sunglass\" Next, define process function that will be passed as the input parameter to create a ChassisModel object. One strict requirement to use Chassis is that your model must be able to be loaded into memory and perform an inference call from it's loaded, pre-trained state. Due to the unique ORT syntax that requires a model file be passed through as a parameter when creating an ORT inference session (which means it must be loaded to memory as a part of the inference session instantiation, not loaded before), we will get a little creative and leverage Python's tempfile library to take the loaded model, save it to a temporary directory as a .onnx file, and load it back in during inference. def process ( input_bytes ): # save model to filepath for inference tmp_dir = tempfile . mkdtemp () import onnx onnx . save ( model , \" {} /model.onnx\" . format ( tmp_dir )) # preprocess data decoded = cv2 . cvtColor ( cv2 . imdecode ( np . frombuffer ( input_bytes , np . uint8 ), - 1 ), cv2 . COLOR_BGR2RGB ) img = cv2 . resize ( decoded , ( 224 , 224 )) img = np . reshape ( img , ( 1 , 3 , 224 , 224 )) . astype ( np . float32 ) # run inference session = ort . InferenceSession ( \" {} /model.onnx\" . format ( tmp_dir )) results = session . run ( None , { \"input\" : img }) # postprocess inference_result = labels [ results [ 0 ] . argmax ()] # format results structured_result = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result )}]} } } # remove temp directory rmtree ( tmp_dir ) return structured_result Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/dog.jpg' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"ONNX MobileNet Image Classifiction\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"ONNX"},{"location":"how-to-guides/frameworks/#pmml","text":"Predictive Model Markup Language (PMML) is an XML-based model format that is widely accepted and used to save machine learning models. This guide leverages the chassisml SDK to auto-containerize a pretrained model from this PMML Scikit-Learn package model library . To follow along, you can reference the Jupyter notebook example and data files here . Import required dependencies. import chassisml import numpy as np import getpass import json from io import StringIO from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import pandas as pd import numpy as np from sklearn_pmml_model.ensemble import PMMLForestClassifier First, load and preprocess training data. # Prepare data iris = load_iris () X = pd . DataFrame ( iris . data ) X . columns = np . array ( iris . feature_names ) y = pd . Series ( np . array ( iris . target_names )[ iris . target ]) y . name = \"Class\" Xtr , Xte , ytr , yte = train_test_split ( X , y , test_size = 0.33 , random_state = 123 ) # Create sample data for testing later with open ( \"data/sample_iris.csv\" , \"w\" ) as f : Xte [: 10 ] . to_csv ( f , index = False ) Next, load the .pmml model file with the sklearn_pmml convenience package. We can then run a sample inference test with our test subset. # Load model clf = PMMLForestClassifier ( pmml = \"models/randomForest.pmml\" ) labels = clf . classes_ . tolist () # Test model clf . predict ( Xte ) clf . score ( Xte , yte ) Now, we will define some pre- and post-processing methods that our model can use during inference. def preprocess_inputs ( raw_input_bytes ): # load data inputs = pd . read_csv ( StringIO ( str ( raw_input_bytes , \"utf-8\" ))) return inputs def postprocess_outputs ( raw_predictions ): # process output inference_result = { \"result\" :[ { \"row\" : i + 1 , \"classPredictions\" : [ { \"class\" : labels [ idx ], \"score\" : results [ idx ]} for idx in np . argsort ( results )[:: - 1 ] ] } for i , results in enumerate ( raw_predictions ) ] } # format output structured_output = { \"data\" : { \"result\" : inference_result [ \"result\" ], \"explanation\" : None , \"drift\" : None , } } return structured_output Next, define process function that will be passed as the input parameter to create a ChassisModel object. def process ( input_bytes ): # load data inputs = preprocess_inputs ( input_bytes ) # make predictions output = clf . predict_proba ( inputs ) # process output structured_output = postprocess_outputs ( output ) return structured_output Initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( process_fn = process ) Test chassis_model locally. sample_filepath = './data/sample_iris.csv' results = chassis_model . test ( sample_filepath ) print ( results ) Before kicking off the Chassis job, we can test our ChassisModel in the environment that will be built within the container. Note : Chassis will infer the required packages, functions, or variables required to successfully run inference within the process function. This step ensures when the conda environment is created within the Docker container, your model will run. NOTE : test_env function not available in publicly-hosted service. test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Lastly, publish your model with your Docker credentials. dockerhub_user = < my . username > dockerhub_pass = < my . password > response = chassis_model . publish ( model_name = \"PMML Random Forest Iris Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"PMML"},{"location":"how-to-guides/frameworks/#tensorflow-keras","text":"Coming Soon","title":"Tensorflow &amp; Keras"},{"location":"how-to-guides/frameworks/#spacy","text":"Coming Soon","title":"Spacy"},{"location":"how-to-guides/frameworks/#spark-mllib","text":"Coming Soon","title":"Spark MLlib"},{"location":"how-to-guides/gpu-support/","text":"GPU Support Data scientists who spend most of their time building and training new machine learning models are likely familiar with two buzzwords that are often brought up when discussing the required resources needed to conduct such experiments: Graphics Processing Unit (GPU) and batch processing . GPUs are designed to quickly process large quantities of data concurrently and in turn makes the batch processing of data more efficient. This is pivotal process for not only model training, but also model inference. If you are familiar with the Chassisml service, you will know that by default, Chassisml will automatically create containers that run on CPU. But what if there is a real need for batch inference in a production setting that processes significantly quicker running on a GPU? This page walks through the process of implementing both GPU and batch inference support. To follow along, you can reference the Jupyter notebook example and data files here . Enable Batch Processing Batch processing goes hand in hand with GPU support. Enabling GPU support does accerate the model inferences execution, but to truly unlock the full potential of a GPU, batch processing is critical. So, we will build a simple Image Classification model with a ResNet50 architecture, avaialable directly in PyTorch's Torvision model library , and implement a batch processing function that takes advantage of GPU access. Note To add this support, you must have access to a GPU to test locally before submitting a Chassisml job. Most Python ML frameworks will also require you to set up CUDA on your machine. To get started, we will install our required dependencies. import chassisml import pickle import cv2 import torch import getpass import numpy as np import torchvision.models as models from torchvision import transforms Next, we will load the pretrained ResNet50 model, define a data transformation object, and define a device variable (this is how we cast both our model and data to the GPU). model = models . resnet50 ( pretrained = True ) model . eval () labels = pickle . load ( open ( './data/imagenet_labels.pkl' , 'rb' )) transform = transforms . Compose ([ transforms . ToPILImage (), transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) device = 'cuda' model . to ( device ) Most deep learning frameworks have built-in support for batch processing. This support includes different dataloader functionalities that will take an entire folder of data in some cases and process it in a way that can be fed to a neural network in the proper tensor form. We will define a batch_process function that takes a list of inputs, formats them into the structure our model expects, and runs inference on the batch of data. def batch_process ( inputs ): # preprocess list of inputs images = [] for input_bytes in inputs : decoded = cv2 . imdecode ( np . frombuffer ( input_bytes , np . uint8 ), - 1 ) resized = cv2 . resize ( decoded , ( 224 , 224 )) . reshape (( 1 , 224 , 224 , 3 )) images . append ( resized ) images_arr = np . concatenate ( images ) batch_t = torch . stack ( tuple ( transform ( i ) for i in images_arr ), dim = 0 ) . to ( device ) # run batch inference and add softmax layer output = model ( batch_t ) probs = torch . nn . functional . softmax ( output , dim = 1 ) softmax_preds = probs . detach () . cpu () . numpy () # postprocess all_formatted_results = [] for preds in softmax_preds : indices = np . argsort ( preds )[:: - 1 ] classes = [ labels [ idx ] for idx in indices [: 5 ]] scores = [ float ( preds [ idx ]) for idx in indices [: 5 ]] preds = [{ \"class\" : \" {} \" . format ( label ), \"score\" : round ( float ( score ), 3 )} for label , score in zip ( classes , scores )] preds . sort ( key = lambda x : x [ \"score\" ], reverse = True ) results = { \"classPredictions\" : preds } all_formatted_results . append ( results ) # output list of formatted results return all_formatted_results When we create our ChassisModel object, we will pass this batch_process function through as a parameter. NOTE: If you would also like to define a process function that only performs inference on a single piece of data instead of batch, you can do so as well and pass both through as parameters. In this case, our batch_process will work if we pass through either a single piece of data or batch. Now, initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( batch_process_fn = batch_process , batch_size = 4 ) Test chassis_model locally (both single and batch data). sample_filepath = './data/airplane.jpg' results = chassis_model . test ( sample_filepath ) print ( results ) results = chassis_model . test_batch ( sample_filepath ) print ( results ) Enable GPU Support Up until this point, creating a container that can run on GPU has been very similar to the normal Chassisml workflow, with the one difference being the need to define a batch_process_fn method. The last procedural difference is a simple flag to turn on GPU support. Turn this flag on and publish our model with your specified Docker credentials. dockerhub_user = < my . username > dockerhob_pass = < my . password > response = chassis_model . publish ( model_name = \"PyTorch ResNet50 Image Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , gpu = True ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"GPU Support"},{"location":"how-to-guides/gpu-support/#gpu-support","text":"Data scientists who spend most of their time building and training new machine learning models are likely familiar with two buzzwords that are often brought up when discussing the required resources needed to conduct such experiments: Graphics Processing Unit (GPU) and batch processing . GPUs are designed to quickly process large quantities of data concurrently and in turn makes the batch processing of data more efficient. This is pivotal process for not only model training, but also model inference. If you are familiar with the Chassisml service, you will know that by default, Chassisml will automatically create containers that run on CPU. But what if there is a real need for batch inference in a production setting that processes significantly quicker running on a GPU? This page walks through the process of implementing both GPU and batch inference support. To follow along, you can reference the Jupyter notebook example and data files here .","title":"GPU Support"},{"location":"how-to-guides/gpu-support/#enable-batch-processing","text":"Batch processing goes hand in hand with GPU support. Enabling GPU support does accerate the model inferences execution, but to truly unlock the full potential of a GPU, batch processing is critical. So, we will build a simple Image Classification model with a ResNet50 architecture, avaialable directly in PyTorch's Torvision model library , and implement a batch processing function that takes advantage of GPU access. Note To add this support, you must have access to a GPU to test locally before submitting a Chassisml job. Most Python ML frameworks will also require you to set up CUDA on your machine. To get started, we will install our required dependencies. import chassisml import pickle import cv2 import torch import getpass import numpy as np import torchvision.models as models from torchvision import transforms Next, we will load the pretrained ResNet50 model, define a data transformation object, and define a device variable (this is how we cast both our model and data to the GPU). model = models . resnet50 ( pretrained = True ) model . eval () labels = pickle . load ( open ( './data/imagenet_labels.pkl' , 'rb' )) transform = transforms . Compose ([ transforms . ToPILImage (), transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) device = 'cuda' model . to ( device ) Most deep learning frameworks have built-in support for batch processing. This support includes different dataloader functionalities that will take an entire folder of data in some cases and process it in a way that can be fed to a neural network in the proper tensor form. We will define a batch_process function that takes a list of inputs, formats them into the structure our model expects, and runs inference on the batch of data. def batch_process ( inputs ): # preprocess list of inputs images = [] for input_bytes in inputs : decoded = cv2 . imdecode ( np . frombuffer ( input_bytes , np . uint8 ), - 1 ) resized = cv2 . resize ( decoded , ( 224 , 224 )) . reshape (( 1 , 224 , 224 , 3 )) images . append ( resized ) images_arr = np . concatenate ( images ) batch_t = torch . stack ( tuple ( transform ( i ) for i in images_arr ), dim = 0 ) . to ( device ) # run batch inference and add softmax layer output = model ( batch_t ) probs = torch . nn . functional . softmax ( output , dim = 1 ) softmax_preds = probs . detach () . cpu () . numpy () # postprocess all_formatted_results = [] for preds in softmax_preds : indices = np . argsort ( preds )[:: - 1 ] classes = [ labels [ idx ] for idx in indices [: 5 ]] scores = [ float ( preds [ idx ]) for idx in indices [: 5 ]] preds = [{ \"class\" : \" {} \" . format ( label ), \"score\" : round ( float ( score ), 3 )} for label , score in zip ( classes , scores )] preds . sort ( key = lambda x : x [ \"score\" ], reverse = True ) results = { \"classPredictions\" : preds } all_formatted_results . append ( results ) # output list of formatted results return all_formatted_results When we create our ChassisModel object, we will pass this batch_process function through as a parameter. NOTE: If you would also like to define a process function that only performs inference on a single piece of data instead of batch, you can do so as well and pass both through as parameters. In this case, our batch_process will work if we pass through either a single piece of data or batch. Now, initialize Chassis Client and create Chassis model. Replace the URL with your Chassis connection. If you followed these installation instructions , keep the local host URL as is, but if you are connected to the publicly-hosted Chassis instance, replace the URL with the URL you receive after signing up . chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) chassis_model = chassis_client . create_model ( batch_process_fn = batch_process , batch_size = 4 ) Test chassis_model locally (both single and batch data). sample_filepath = './data/airplane.jpg' results = chassis_model . test ( sample_filepath ) print ( results ) results = chassis_model . test_batch ( sample_filepath ) print ( results )","title":"Enable Batch Processing"},{"location":"how-to-guides/gpu-support/#enable-gpu-support","text":"Up until this point, creating a container that can run on GPU has been very similar to the normal Chassisml workflow, with the one difference being the need to define a batch_process_fn method. The last procedural difference is a simple flag to turn on GPU support. Turn this flag on and publish our model with your specified Docker credentials. dockerhub_user = < my . username > dockerhob_pass = < my . password > response = chassis_model . publish ( model_name = \"PyTorch ResNet50 Image Classification\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , gpu = True ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id )","title":"Enable GPU Support"},{"location":"tutorials/ds-connect/","text":"Build a Model In this tutorial, we will: Connect to the Chassis service with the Python SDK Load our model into memory (model can be saved as a file to be loaded or trained and loaded from scratch) Define a process function for inferencing Submit a job request to the Chassis service After completing these steps, we will have a new container image uploaded to Docker Hub that we will be able to use locally or deploy to a serving platform of our choice. Please note that in addition to the tutorial on this page, there are example Jupyter notebooks available in the Chassis repo here . Instructions to run those notebooks are provided in the README in that repo subdirectory. Install the SDK First step is to install the SDK and additional packages required for this tutorial using pip . pip install chassisml scikit-learn mlflow joblib requests Build or import the model We can start from an existing model saved locally or create a new one, as long as the model can be loaded into memory. In this tutorial, we will create a simple classifier with the Scikit-Learn library. Import required libraries Since we are going to train our own model as an example, we need to import all the libraries that we will need to do that. import chassisml import sklearn import numpy as np import json from joblib import dump , load Create the model from sklearn import datasets , svm from sklearn.model_selection import train_test_split digits = datasets . load_digits () data = digits . images . reshape (( len ( digits . images ), - 1 )) # Create a classifier: a support vector classifier clf = svm . SVC ( gamma = 0.001 ) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) # Learn the digits on the train subset clf . fit ( X_train , y_train ) Define Process Method Notice our Scikit-Learn model ( clf ) is now loaded into memory, which is exactly what we need to format it the way Chassis expects. This means we can now prepare the process function, which must take raw bytes as input. This function is responsible for preprocessing the bytes, running inference, and returning formatted results. It can leverage anything (variables, functions, objects) defined in our environment. Notice we reference our model clf as defined above: def process ( input_bytes ): inputs = np . array ( json . loads ( input_bytes )) / 2 inference_results = clf . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results The process function can call other functions if needed. Next, we initialize our Chassis client, which we'll use to communicate with the Chassis service. Here, we assume our instance of Chassis is running locally on port 5000: chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) Now let's create a Chassis model with our process function, test it locally with a local input file, and then also test the creation of the environment and the execution of the model with a sample input file within that created environment (this will occur within the Chassis service): NOTE : test_env function not available in publicly-hosted service. # create Chassis model chassis_model = chassis_client . create_model ( process_fn = process ) # save sample data for testing sample = X_test [: 1 ] . tolist () with open ( \"./digits_sample.json\" , 'w' ) as out : json . dump ( sample , out ) # test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here): sample_filepath = './digits_sample.json' results = chassis_model . test ( sample_filepath ) print ( results ) # test environment and model within Chassis service, must pass filepath here: test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) Build the image and publish to Modzy Now that we have our model in the proper Chassis format, we need to make a request against the Chassis service to build the Docker image that exposes it. You can optionally define your desired conda environment and pass it to publish() , but if you don't, Chassis will automatically infer the dependencies for you based on what is required to run the process function. We just need to provide a model name and semantic version, dockerhub credentials, and we can optionally provide a sample input file and Modzy API key if we'd like to publish the model to Modzy: dockerhub_user = < my . username > dockerhob_pass = < my . password > response = chassis_model . publish ( model_name = \"Sklearn Digits\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) The block_until_complete call will terminate once the Chassis job completes. Pull the image Now that the process has completely finished we can pull and see our built image. The image name will be the model_name specified in the publish() call, but lowercase and with dashes instead of spaces. The image tag will be the model_version . docker pull <my.username>/sklearn-digits:0.0.1 docker images <my.username>/sklearn-digits:0.0.1 If everything has gone as expected we will see something similar to this. REPOSITORY TAG IMAGE ID CREATED SIZE <my.username>/sklearn-digits latest 0e5c5815f2ec 3 minutes ago 2 .19GB Tutorial in Action Follow along as we walk through this tutorial step by step! .video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Build a Model"},{"location":"tutorials/ds-connect/#build-a-model","text":"In this tutorial, we will: Connect to the Chassis service with the Python SDK Load our model into memory (model can be saved as a file to be loaded or trained and loaded from scratch) Define a process function for inferencing Submit a job request to the Chassis service After completing these steps, we will have a new container image uploaded to Docker Hub that we will be able to use locally or deploy to a serving platform of our choice. Please note that in addition to the tutorial on this page, there are example Jupyter notebooks available in the Chassis repo here . Instructions to run those notebooks are provided in the README in that repo subdirectory.","title":"Build a Model"},{"location":"tutorials/ds-connect/#install-the-sdk","text":"First step is to install the SDK and additional packages required for this tutorial using pip . pip install chassisml scikit-learn mlflow joblib requests","title":"Install the SDK"},{"location":"tutorials/ds-connect/#build-or-import-the-model","text":"We can start from an existing model saved locally or create a new one, as long as the model can be loaded into memory. In this tutorial, we will create a simple classifier with the Scikit-Learn library.","title":"Build or import the model"},{"location":"tutorials/ds-connect/#import-required-libraries","text":"Since we are going to train our own model as an example, we need to import all the libraries that we will need to do that. import chassisml import sklearn import numpy as np import json from joblib import dump , load","title":"Import required libraries"},{"location":"tutorials/ds-connect/#create-the-model","text":"from sklearn import datasets , svm from sklearn.model_selection import train_test_split digits = datasets . load_digits () data = digits . images . reshape (( len ( digits . images ), - 1 )) # Create a classifier: a support vector classifier clf = svm . SVC ( gamma = 0.001 ) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) # Learn the digits on the train subset clf . fit ( X_train , y_train )","title":"Create the model"},{"location":"tutorials/ds-connect/#define-process-method","text":"Notice our Scikit-Learn model ( clf ) is now loaded into memory, which is exactly what we need to format it the way Chassis expects. This means we can now prepare the process function, which must take raw bytes as input. This function is responsible for preprocessing the bytes, running inference, and returning formatted results. It can leverage anything (variables, functions, objects) defined in our environment. Notice we reference our model clf as defined above: def process ( input_bytes ): inputs = np . array ( json . loads ( input_bytes )) / 2 inference_results = clf . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results The process function can call other functions if needed. Next, we initialize our Chassis client, which we'll use to communicate with the Chassis service. Here, we assume our instance of Chassis is running locally on port 5000: chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) Now let's create a Chassis model with our process function, test it locally with a local input file, and then also test the creation of the environment and the execution of the model with a sample input file within that created environment (this will occur within the Chassis service): NOTE : test_env function not available in publicly-hosted service. # create Chassis model chassis_model = chassis_client . create_model ( process_fn = process ) # save sample data for testing sample = X_test [: 1 ] . tolist () with open ( \"./digits_sample.json\" , 'w' ) as out : json . dump ( sample , out ) # test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here): sample_filepath = './digits_sample.json' results = chassis_model . test ( sample_filepath ) print ( results ) # test environment and model within Chassis service, must pass filepath here: test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result )","title":"Define Process Method"},{"location":"tutorials/ds-connect/#build-the-image-and-publish-to-modzy","text":"Now that we have our model in the proper Chassis format, we need to make a request against the Chassis service to build the Docker image that exposes it. You can optionally define your desired conda environment and pass it to publish() , but if you don't, Chassis will automatically infer the dependencies for you based on what is required to run the process function. We just need to provide a model name and semantic version, dockerhub credentials, and we can optionally provide a sample input file and Modzy API key if we'd like to publish the model to Modzy: dockerhub_user = < my . username > dockerhob_pass = < my . password > response = chassis_model . publish ( model_name = \"Sklearn Digits\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) The block_until_complete call will terminate once the Chassis job completes.","title":"Build the image and publish to Modzy"},{"location":"tutorials/ds-connect/#pull-the-image","text":"Now that the process has completely finished we can pull and see our built image. The image name will be the model_name specified in the publish() call, but lowercase and with dashes instead of spaces. The image tag will be the model_version . docker pull <my.username>/sklearn-digits:0.0.1 docker images <my.username>/sklearn-digits:0.0.1 If everything has gone as expected we will see something similar to this. REPOSITORY TAG IMAGE ID CREATED SIZE <my.username>/sklearn-digits latest 0e5c5815f2ec 3 minutes ago 2 .19GB","title":"Pull the image"},{"location":"tutorials/ds-connect/#tutorial-in-action","text":"Follow along as we walk through this tutorial step by step! .video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Tutorial in Action"},{"location":"tutorials/ds-deploy-modzy/","text":"Deploy Model to Modzy To deploy a model to Modzy, we must first create a model following the same steps outlined in the Build a Model tutorial. If you have already worked throgh this, the following snippet of code is exactly the same. Create the model You can copy this code directly to your Python editor of choice, assuming you have installed the Chassisml SDK and other required packages: pip install chassisml scikit-learn mlflow joblib requests import chassisml import sklearn import numpy as np import json from joblib import dump , load # create model from sklearn import datasets , svm from sklearn.model_selection import train_test_split digits = datasets . load_digits () data = digits . images . reshape (( len ( digits . images ), - 1 )) # Create a classifier: a support vector classifier clf = svm . SVC ( gamma = 0.001 ) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) # Learn the digits on the train subset clf . fit ( X_train , y_train ) # build process method def process ( input_bytes ): inputs = np . array ( json . loads ( input_bytes )) / 2 inference_results = clf . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results # intialize Chassis client chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) # create Chassis model chassis_model = chassis_client . create_model ( process_fn = process ) # save sample data for testing sample = X_test [: 1 ] . tolist () with open ( \"./digits_sample.json\" , 'w' ) as out : json . dump ( sample , out ) # test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here): sample_filepath = './digits_sample.json' results = chassis_model . test ( sample_filepath ) print ( results ) # test environment and model within Chassis service, must pass filepath here: test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) # define Docker Hub credentials dockerhub_user = < my . username > dockerhob_pass = < my . password > Define Modzy data At this point, we have built a valid Chassis model that can be published directly to Docker Hub, as demonstrated in here . Now in this tutorial, we will take it a step further and deploy this model directly to Modzy by simply modifying the publish method. To follow along, you must first define the following Modzy-specific fields: Modzy Credentials modzy_url : Valid URL to Modzy instance (e.g., \"https://app.modzy.com\") modzy_api_key : Valid API key associated with your Modzy account. Sign up for a free account here modzy_sample_input_filepath : Sample data to run through model container for inference example MODZY_URL = \"<my-modzy-instance-url>\" MODZY_API_KEY = \"<my-modzy-api-key>\" MODZY_SAMPLE_FILEPATH = sample_filepath Deploy to Modzy We can now make the job request to the Chassisml service, which will build a container image with our model, push that image to Docker Hub, and deploy the built image all the way through to Modzy. dockerhub_user = < my . username > dockerhob_pass = < my . password > response = chassis_model . publish ( model_name = \"Sklearn Digits\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , modzy_url = MODZY_URL , modzy_api_key = MODZY_API_KEY , modzy_sample_input_path = MODZY_SAMPLE_FILEPATH ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) If everything has gone well we should see something similar to this. Starting build job... Ok! Print status of job completion and your new model URL: if final_status [ \"result\" ] is not None : print ( \"New model URL: {} \" . format ( final_status [ \"result\" ][ \"container_url\" ])) else : print ( \"Chassis job failed \\n\\n {} \" . format ( final_status )) Submit Inference Job to Modzy Pending a successful Chassis job completion, we can now submit an inference to our model sitting in the Modzy platform. To do so, the only additional requirement we need is the Modzy Python SDK. pip install modzy-sdk Once this is successfully installed in our envrionment, we can establish our connection to the Modzy API client. from modzy import ApiClient client = ApiClient ( base_url = MODZY_URL , api_key = MODZY_API_KEY ) Next, define three pieces of information we need to submit an inference to our model. input_name = final_status [ 'result' ][ 'inputs' ][ 0 ][ 'name' ] model_id = final_status [ 'result' ] . get ( \"model\" ) . get ( \"modelId\" ) model_version = final_status [ 'result' ] . get ( \"version\" ) Finally, submit an inference job, retrieve the results, and print them to your console. inference_job = client . jobs . submit_file ( model_id , model_version , { input_name : sample_filepath }) inference_job_result = client . results . block_until_complete ( inference_job , timeout = None ) inference_job_results_json = inference_job_result . get_first_outputs ()[ 'results.json' ] print ( inference_job_results_json ) Tutorial in Action Follow along as we walk through this tutorial step by step! .video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Deploy Model to Modzy"},{"location":"tutorials/ds-deploy-modzy/#deploy-model-to-modzy","text":"To deploy a model to Modzy, we must first create a model following the same steps outlined in the Build a Model tutorial. If you have already worked throgh this, the following snippet of code is exactly the same.","title":"Deploy Model to Modzy"},{"location":"tutorials/ds-deploy-modzy/#create-the-model","text":"You can copy this code directly to your Python editor of choice, assuming you have installed the Chassisml SDK and other required packages: pip install chassisml scikit-learn mlflow joblib requests import chassisml import sklearn import numpy as np import json from joblib import dump , load # create model from sklearn import datasets , svm from sklearn.model_selection import train_test_split digits = datasets . load_digits () data = digits . images . reshape (( len ( digits . images ), - 1 )) # Create a classifier: a support vector classifier clf = svm . SVC ( gamma = 0.001 ) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) # Learn the digits on the train subset clf . fit ( X_train , y_train ) # build process method def process ( input_bytes ): inputs = np . array ( json . loads ( input_bytes )) / 2 inference_results = clf . predict ( inputs ) structured_results = [] for inference_result in inference_results : structured_output = { \"data\" : { \"result\" : { \"classPredictions\" : [{ \"class\" : str ( inference_result ), \"score\" : str ( 1 )}]} } } structured_results . append ( structured_output ) return structured_results # intialize Chassis client chassis_client = chassisml . ChassisClient ( \"http://localhost:5000\" ) # create Chassis model chassis_model = chassis_client . create_model ( process_fn = process ) # save sample data for testing sample = X_test [: 1 ] . tolist () with open ( \"./digits_sample.json\" , 'w' ) as out : json . dump ( sample , out ) # test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here): sample_filepath = './digits_sample.json' results = chassis_model . test ( sample_filepath ) print ( results ) # test environment and model within Chassis service, must pass filepath here: test_env_result = chassis_model . test_env ( sample_filepath ) print ( test_env_result ) # define Docker Hub credentials dockerhub_user = < my . username > dockerhob_pass = < my . password >","title":"Create the model"},{"location":"tutorials/ds-deploy-modzy/#define-modzy-data","text":"At this point, we have built a valid Chassis model that can be published directly to Docker Hub, as demonstrated in here . Now in this tutorial, we will take it a step further and deploy this model directly to Modzy by simply modifying the publish method. To follow along, you must first define the following Modzy-specific fields: Modzy Credentials modzy_url : Valid URL to Modzy instance (e.g., \"https://app.modzy.com\") modzy_api_key : Valid API key associated with your Modzy account. Sign up for a free account here modzy_sample_input_filepath : Sample data to run through model container for inference example MODZY_URL = \"<my-modzy-instance-url>\" MODZY_API_KEY = \"<my-modzy-api-key>\" MODZY_SAMPLE_FILEPATH = sample_filepath","title":"Define Modzy data"},{"location":"tutorials/ds-deploy-modzy/#deploy-to-modzy","text":"We can now make the job request to the Chassisml service, which will build a container image with our model, push that image to Docker Hub, and deploy the built image all the way through to Modzy. dockerhub_user = < my . username > dockerhob_pass = < my . password > response = chassis_model . publish ( model_name = \"Sklearn Digits\" , model_version = \"0.0.1\" , registry_user = dockerhub_user , registry_pass = dockerhub_pass , modzy_url = MODZY_URL , modzy_api_key = MODZY_API_KEY , modzy_sample_input_path = MODZY_SAMPLE_FILEPATH ) job_id = response . get ( 'job_id' ) final_status = chassis_client . block_until_complete ( job_id ) If everything has gone well we should see something similar to this. Starting build job... Ok! Print status of job completion and your new model URL: if final_status [ \"result\" ] is not None : print ( \"New model URL: {} \" . format ( final_status [ \"result\" ][ \"container_url\" ])) else : print ( \"Chassis job failed \\n\\n {} \" . format ( final_status ))","title":"Deploy to Modzy"},{"location":"tutorials/ds-deploy-modzy/#submit-inference-job-to-modzy","text":"Pending a successful Chassis job completion, we can now submit an inference to our model sitting in the Modzy platform. To do so, the only additional requirement we need is the Modzy Python SDK. pip install modzy-sdk Once this is successfully installed in our envrionment, we can establish our connection to the Modzy API client. from modzy import ApiClient client = ApiClient ( base_url = MODZY_URL , api_key = MODZY_API_KEY ) Next, define three pieces of information we need to submit an inference to our model. input_name = final_status [ 'result' ][ 'inputs' ][ 0 ][ 'name' ] model_id = final_status [ 'result' ] . get ( \"model\" ) . get ( \"modelId\" ) model_version = final_status [ 'result' ] . get ( \"version\" ) Finally, submit an inference job, retrieve the results, and print them to your console. inference_job = client . jobs . submit_file ( model_id , model_version , { input_name : sample_filepath }) inference_job_result = client . results . block_until_complete ( inference_job , timeout = None ) inference_job_results_json = inference_job_result . get_first_outputs ()[ 'results.json' ] print ( inference_job_results_json )","title":"Submit Inference Job to Modzy"},{"location":"tutorials/ds-deploy-modzy/#tutorial-in-action","text":"Follow along as we walk through this tutorial step by step! .video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Tutorial in Action"},{"location":"tutorials/ds-deploy/","text":"Deploy Model to KServe Install Required Dependencies Install Docker Desktop Try to run docker ps If you get a permissions error, follow instructions here Install KServe: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.7/hack/quick_install.sh\" | bash Required variables There are some environment variables that must be defined for KServe to work: INTERFACE: kserve HTTP_PORT: port where kserve will be running PROTOCOL: it can be v1 or v2 MODEL_NAME: a name for the model must be defined Deploy the model For this tutorial, we will use the Chassis-generated container image uploaded as bmunday131/sklearn-digits . To deploy to KServe, we will use the file that defines the InferenceService for the protocol v1 of KServe. apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : chassisml-sklearn-demo spec : predictor : containers : - image : bmunday131/sklearn-digits:0.0.1 name : chassisml-sklearn-demo-container imagePullPolicy : IfNotPresent env : - name : INTERFACE value : kserve - name : HTTP_PORT value : \"8080\" - name : PROTOCOL value : v1 - name : MODEL_NAME value : digits ports : - containerPort : 8080 protocol : TCP In this case, the variable MODEL_NAME should not be necessary since it's defined when creating the image. kubectl apply -f custom_v1.yaml This should output a success message. Define required variables to query the pod This is needed in order to be able to communicate with the deployed image. The SERVICE_NAME must match the name defined in the metadata.name of the InferenceService created above. The MODEL_NAME must match the name of your model. It can be defined by the data scientist when making the request against Chassis service or overwritten in the InferenceService as defined above. Mac: minikube tunnel # in another terminal: export INGRESS_HOST = localhost export INGRESS_PORT = 80 Linux: export INGRESS_HOST = $( minikube ip ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) Mac or Linux: export SERVICE_NAME = chassisml-sklearn-demo export MODEL_NAME = digits export SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ SERVICE_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) Query the model Please note that you must base64 encode each input instance. For example: import json import base64 as b64 instances = [[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ]] input_dict = { 'instances' : [ b64 . b64encode ( str ( entry ) . encode ()) . decode () for entry in instances ]} json . dump ( input_dict , open ( 'kserve_input.json' , 'w' )) Now you can just make a request to predict some data. Take into account that you must download inputsv1.json before making the request. curl -H \"Host: ${ SERVICE_HOSTNAME } \" \"http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict\" -d@inputsv1.json | jq The output should be similar to this: { \"predictions\" : [ { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } } ] } In this case, the data was prepared for the protocol v1, but we can deploy the image using the protocol v2 and make the request using the data for v2 . Deploy the model locally The model can also be deployed locally: docker run --rm -p 8080 :8080 \\ -e INTERFACE = kserve \\ -e HTTP_PORT = 8080 \\ -e PROTOCOL = v2 \\ -e MODEL_NAME = digits \\ carmilso/chassisml-sklearn-demo:latest So we can query it this way. Take into account that you must download inputsv2.json before making the request: curl localhost:8080/v2/models/digits/infer -d@inputsv2.json Tutorial in Action Follow along as we walk through this tutorial step by step! .video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Deploy Model to KServe"},{"location":"tutorials/ds-deploy/#deploy-model-to-kserve","text":"","title":"Deploy Model to KServe"},{"location":"tutorials/ds-deploy/#install-required-dependencies","text":"Install Docker Desktop Try to run docker ps If you get a permissions error, follow instructions here Install KServe: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.7/hack/quick_install.sh\" | bash","title":"Install Required Dependencies"},{"location":"tutorials/ds-deploy/#required-variables","text":"There are some environment variables that must be defined for KServe to work: INTERFACE: kserve HTTP_PORT: port where kserve will be running PROTOCOL: it can be v1 or v2 MODEL_NAME: a name for the model must be defined","title":"Required variables"},{"location":"tutorials/ds-deploy/#deploy-the-model","text":"For this tutorial, we will use the Chassis-generated container image uploaded as bmunday131/sklearn-digits . To deploy to KServe, we will use the file that defines the InferenceService for the protocol v1 of KServe. apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : chassisml-sklearn-demo spec : predictor : containers : - image : bmunday131/sklearn-digits:0.0.1 name : chassisml-sklearn-demo-container imagePullPolicy : IfNotPresent env : - name : INTERFACE value : kserve - name : HTTP_PORT value : \"8080\" - name : PROTOCOL value : v1 - name : MODEL_NAME value : digits ports : - containerPort : 8080 protocol : TCP In this case, the variable MODEL_NAME should not be necessary since it's defined when creating the image. kubectl apply -f custom_v1.yaml This should output a success message.","title":"Deploy the model"},{"location":"tutorials/ds-deploy/#define-required-variables-to-query-the-pod","text":"This is needed in order to be able to communicate with the deployed image. The SERVICE_NAME must match the name defined in the metadata.name of the InferenceService created above. The MODEL_NAME must match the name of your model. It can be defined by the data scientist when making the request against Chassis service or overwritten in the InferenceService as defined above. Mac: minikube tunnel # in another terminal: export INGRESS_HOST = localhost export INGRESS_PORT = 80 Linux: export INGRESS_HOST = $( minikube ip ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) Mac or Linux: export SERVICE_NAME = chassisml-sklearn-demo export MODEL_NAME = digits export SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ SERVICE_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 )","title":"Define required variables to query the pod"},{"location":"tutorials/ds-deploy/#query-the-model","text":"Please note that you must base64 encode each input instance. For example: import json import base64 as b64 instances = [[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ]] input_dict = { 'instances' : [ b64 . b64encode ( str ( entry ) . encode ()) . decode () for entry in instances ]} json . dump ( input_dict , open ( 'kserve_input.json' , 'w' )) Now you can just make a request to predict some data. Take into account that you must download inputsv1.json before making the request. curl -H \"Host: ${ SERVICE_HOSTNAME } \" \"http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict\" -d@inputsv1.json | jq The output should be similar to this: { \"predictions\" : [ { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } } ] } In this case, the data was prepared for the protocol v1, but we can deploy the image using the protocol v2 and make the request using the data for v2 .","title":"Query the model"},{"location":"tutorials/ds-deploy/#deploy-the-model-locally","text":"The model can also be deployed locally: docker run --rm -p 8080 :8080 \\ -e INTERFACE = kserve \\ -e HTTP_PORT = 8080 \\ -e PROTOCOL = v2 \\ -e MODEL_NAME = digits \\ carmilso/chassisml-sklearn-demo:latest So we can query it this way. Take into account that you must download inputsv2.json before making the request: curl localhost:8080/v2/models/digits/infer -d@inputsv2.json","title":"Deploy the model locally"},{"location":"tutorials/ds-deploy/#tutorial-in-action","text":"Follow along as we walk through this tutorial step by step! .video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Tutorial in Action"},{"location":"tutorials/ds-postman/","text":"Run Model in Postman After building a container with Chassisml, there are several ways to interact with it: via Modzy , KServe , or locally on your machine. This tutorial demonstrates how to make API calls to a container locally using Postman. Note To follow this tutorial, you do not need your own container as we provide an example Chassisml container for convenience. However, if you do you have your own model container, you can switch out the example container with your own. Getting started Install Required Dependencies Install Docker Open a terminal on your machine, and try to run docker ps If you get a permissions error, follow instructions here Install Postman Download proto file Download Sample Container docker pull modzy/chassisml-image-classification Spin up Container Before interacting with the container using Postman, we need to first spin up the container and port forward it to a port we will use in the local URL configuration in Postman. docker run -p 5000 :45000 -it modzy/chassisml-image-classification:latest In this docker command, we use the following parameters: -p : Forwards the port serving the gRPC server inside the container (45000) to a local port (5000) -it : Runs container interactively, so you can see the logs from the container as you make API calls to it To learn more, visit the Docker run reference documentation. After running the container, you should see this logs message printed to your terminal: INFO:interfaces.modzy.grpc_model.src.model_server:gRPC Server running on port 45000 Send gRPC Requests to Running Container Request Set Up Once the container is running locally, open the Postman app. On the upper left part of the screen, select New and open gRPC Request . Enter your local server URL into the first box. In the second box, select the Import protobuf definition from local file and upload the proto file you downloaded during setup. Give your protobuf a name and version. The three remote procedure calls defined in the protobuf file (to which our Chassisml container adheres to) will appear in the Select a method dropdown. We are now ready to invoke requests to the three remote procedure calls this container can respond to as defined in the model protobuf file. Invoke Methods Status() First, we will invoke the Status() method, which will execute any model instantiation required for model inference. Simply select the ModzyModel / Status option and click Invoke . In the response section on the bottom of your screen, you should see the following information returned: { \"status_code\" : 200 , \"status\" : \"OK\" , \"message\" : \"Model Initialized Successfully.\" , \"model_info\" : { \"model_name\" : \"MXNET MobileNet Image Classifiction\" , \"model_version\" : \"0.0.1\" , \"model_author\" : \"Chassis\" , \"model_type\" : \"grpc\" }, \"description\" : { \"summary\" : \"Chassis model.\" , \"details\" : \"Chassis model.\" , \"technical\" : \"Chassis model.\" }, \"inputs\" : [ { \"filename\" : \"input\" , \"accepted_media_types\" : [ \"application/json\" ], \"max_size\" : \"5M\" , \"description\" : \"Input file.\" } ], \"outputs\" : [ { \"filename\" : \"results.json\" , \"media_type\" : \"application/json\" , \"max_size\" : \"1M\" , \"description\" : \"Output file.\" } ], \"resources\" : {}, \"timeout\" : { \"status\" : \"60s\" , \"run\" : \"60s\" }, \"features\" : { \"batch_size\" : 1 } } The 200 status_code tells us the model was successfully spun up and instantiated. Notice the response also includes some metadata about our model. Run() Next, we will invoke the Run() method, which expects raw bytes as input to perform inference on, and in turn will respond with the model predictions. To do so, first change the method from ModzyModel / Status to ModzyModel / Run . As defined by the model protobuf file, this method expects a request that contains three inputs: message RunRequest { repeated InputItem inputs = 1 ; bool detect_drift = 2 ; bool explain = 3 ; } Where the InputItem is defined as: message InputItem { map < string , bytes > input = 1 ; } Conveniently, Postman makes it very easy to create a properly-formatted input request message. At the bottom of the Request section, click Generate Example Message button. Replace the example content in the \"Message\" box (right above Generate Example Message button) with this content: { \"detect_drift\" : false , \"explain\" : false , \"inputs\" : [ { \"input\" : { \"input\" : \"base64-encoded-image\" } } ] } NOTE : Before invoking the Run() method, you must first replace the contents of the \"input\" string with an actual raw byte representation of an image. For the sake of this tutorial, we did not paste the full base64 encoding of an image, but to replicate the results listed below, download this image and upload it to this base64 encoder to get the full encoding. Replace \"base-64-encoded-image\" with the base64 encoded representation of the image (wrapped in quotes). After pasting in the base64 equivalent of the image into the value string inside the request message, click Invoke to run a sample inference. You should see the following JSON printed in the Response section. { \"status_code\" : 200 , \"status\" : \"OK\" , \"message\" : \"Inference executed\" , \"outputs\" : [ { \"output\" : { \"results.json\" : \"eyJkYXRhIjp7InJlc3VsdCI6eyJjbGFzc1ByZWRpY3Rpb25zIjpbeyJjbGFzcyI6ImJveGVyIiwic2NvcmUiOjAuODE0MzAyNDQ0NDU4MDA3OH0seyJjbGFzcyI6ImJ1bGwgbWFzdGlmZiIsInNjb3JlIjowLjE2MjA1NTQ2MjU5ODgwMDY2fSx7ImNsYXNzIjoiU3RhZmZvcmRzaGlyZSBidWxsdGVycmllciwgU3RhZmZvcmRzaGlyZSBidWxsIHRlcnJpZXIiLCJzY29yZSI6MC4wMDYxOTQ1Nzg5NDkzNjIwMzk2fV19LCJleHBsYW5hdGlvbiI6bnVsbCwiZHJpZnQiOm51bGx9fQ==\" }, \"success\" : true } ] } You will noticed the contents of the output object are also base64 encoded, as defined in the protobuf file. Decode the raw base64 string here . The decoded result should look as follows: { \"data\" : { \"result\" : { \"classPredictions\" : [ { \"class\" : \"boxer\" , \"score\" : 0.8143024444580078 }, { \"class\" : \"bull mastiff\" , \"score\" : 0.16205546259880066 }, { \"class\" : \"Staffordshire bullterrier, Staffordshire bull terrier\" , \"score\" : 0.0061945789493620396 } ] }, \"explanation\" : null , \"drift\" : null } } In practice, encoding and decoding data is easy to do in your program of choice, so these manual steps normally would not be required in a production API application. However, to conveniently construct and test gRPC APIs in postman, these steps are required. Shutdown() Lastly, invoke the Shutdown() method the same way you invoked the Status() method. This will shutdown the container running locally on your machine. Tutorial in Action Follow along as we walk through this tutorial step by step! .video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Run Model in Postman"},{"location":"tutorials/ds-postman/#run-model-in-postman","text":"After building a container with Chassisml, there are several ways to interact with it: via Modzy , KServe , or locally on your machine. This tutorial demonstrates how to make API calls to a container locally using Postman. Note To follow this tutorial, you do not need your own container as we provide an example Chassisml container for convenience. However, if you do you have your own model container, you can switch out the example container with your own.","title":"Run Model in Postman"},{"location":"tutorials/ds-postman/#getting-started","text":"","title":"Getting started"},{"location":"tutorials/ds-postman/#install-required-dependencies","text":"Install Docker Open a terminal on your machine, and try to run docker ps If you get a permissions error, follow instructions here Install Postman Download proto file","title":"Install Required Dependencies"},{"location":"tutorials/ds-postman/#download-sample-container","text":"docker pull modzy/chassisml-image-classification","title":"Download Sample Container"},{"location":"tutorials/ds-postman/#spin-up-container","text":"Before interacting with the container using Postman, we need to first spin up the container and port forward it to a port we will use in the local URL configuration in Postman. docker run -p 5000 :45000 -it modzy/chassisml-image-classification:latest In this docker command, we use the following parameters: -p : Forwards the port serving the gRPC server inside the container (45000) to a local port (5000) -it : Runs container interactively, so you can see the logs from the container as you make API calls to it To learn more, visit the Docker run reference documentation. After running the container, you should see this logs message printed to your terminal: INFO:interfaces.modzy.grpc_model.src.model_server:gRPC Server running on port 45000","title":"Spin up Container"},{"location":"tutorials/ds-postman/#send-grpc-requests-to-running-container","text":"","title":"Send gRPC Requests to Running Container"},{"location":"tutorials/ds-postman/#request-set-up","text":"Once the container is running locally, open the Postman app. On the upper left part of the screen, select New and open gRPC Request . Enter your local server URL into the first box. In the second box, select the Import protobuf definition from local file and upload the proto file you downloaded during setup. Give your protobuf a name and version. The three remote procedure calls defined in the protobuf file (to which our Chassisml container adheres to) will appear in the Select a method dropdown. We are now ready to invoke requests to the three remote procedure calls this container can respond to as defined in the model protobuf file.","title":"Request Set Up"},{"location":"tutorials/ds-postman/#invoke-methods","text":"","title":"Invoke Methods"},{"location":"tutorials/ds-postman/#status","text":"First, we will invoke the Status() method, which will execute any model instantiation required for model inference. Simply select the ModzyModel / Status option and click Invoke . In the response section on the bottom of your screen, you should see the following information returned: { \"status_code\" : 200 , \"status\" : \"OK\" , \"message\" : \"Model Initialized Successfully.\" , \"model_info\" : { \"model_name\" : \"MXNET MobileNet Image Classifiction\" , \"model_version\" : \"0.0.1\" , \"model_author\" : \"Chassis\" , \"model_type\" : \"grpc\" }, \"description\" : { \"summary\" : \"Chassis model.\" , \"details\" : \"Chassis model.\" , \"technical\" : \"Chassis model.\" }, \"inputs\" : [ { \"filename\" : \"input\" , \"accepted_media_types\" : [ \"application/json\" ], \"max_size\" : \"5M\" , \"description\" : \"Input file.\" } ], \"outputs\" : [ { \"filename\" : \"results.json\" , \"media_type\" : \"application/json\" , \"max_size\" : \"1M\" , \"description\" : \"Output file.\" } ], \"resources\" : {}, \"timeout\" : { \"status\" : \"60s\" , \"run\" : \"60s\" }, \"features\" : { \"batch_size\" : 1 } } The 200 status_code tells us the model was successfully spun up and instantiated. Notice the response also includes some metadata about our model.","title":"Status()"},{"location":"tutorials/ds-postman/#run","text":"Next, we will invoke the Run() method, which expects raw bytes as input to perform inference on, and in turn will respond with the model predictions. To do so, first change the method from ModzyModel / Status to ModzyModel / Run . As defined by the model protobuf file, this method expects a request that contains three inputs: message RunRequest { repeated InputItem inputs = 1 ; bool detect_drift = 2 ; bool explain = 3 ; } Where the InputItem is defined as: message InputItem { map < string , bytes > input = 1 ; } Conveniently, Postman makes it very easy to create a properly-formatted input request message. At the bottom of the Request section, click Generate Example Message button. Replace the example content in the \"Message\" box (right above Generate Example Message button) with this content: { \"detect_drift\" : false , \"explain\" : false , \"inputs\" : [ { \"input\" : { \"input\" : \"base64-encoded-image\" } } ] } NOTE : Before invoking the Run() method, you must first replace the contents of the \"input\" string with an actual raw byte representation of an image. For the sake of this tutorial, we did not paste the full base64 encoding of an image, but to replicate the results listed below, download this image and upload it to this base64 encoder to get the full encoding. Replace \"base-64-encoded-image\" with the base64 encoded representation of the image (wrapped in quotes). After pasting in the base64 equivalent of the image into the value string inside the request message, click Invoke to run a sample inference. You should see the following JSON printed in the Response section. { \"status_code\" : 200 , \"status\" : \"OK\" , \"message\" : \"Inference executed\" , \"outputs\" : [ { \"output\" : { \"results.json\" : \"eyJkYXRhIjp7InJlc3VsdCI6eyJjbGFzc1ByZWRpY3Rpb25zIjpbeyJjbGFzcyI6ImJveGVyIiwic2NvcmUiOjAuODE0MzAyNDQ0NDU4MDA3OH0seyJjbGFzcyI6ImJ1bGwgbWFzdGlmZiIsInNjb3JlIjowLjE2MjA1NTQ2MjU5ODgwMDY2fSx7ImNsYXNzIjoiU3RhZmZvcmRzaGlyZSBidWxsdGVycmllciwgU3RhZmZvcmRzaGlyZSBidWxsIHRlcnJpZXIiLCJzY29yZSI6MC4wMDYxOTQ1Nzg5NDkzNjIwMzk2fV19LCJleHBsYW5hdGlvbiI6bnVsbCwiZHJpZnQiOm51bGx9fQ==\" }, \"success\" : true } ] } You will noticed the contents of the output object are also base64 encoded, as defined in the protobuf file. Decode the raw base64 string here . The decoded result should look as follows: { \"data\" : { \"result\" : { \"classPredictions\" : [ { \"class\" : \"boxer\" , \"score\" : 0.8143024444580078 }, { \"class\" : \"bull mastiff\" , \"score\" : 0.16205546259880066 }, { \"class\" : \"Staffordshire bullterrier, Staffordshire bull terrier\" , \"score\" : 0.0061945789493620396 } ] }, \"explanation\" : null , \"drift\" : null } } In practice, encoding and decoding data is easy to do in your program of choice, so these manual steps normally would not be required in a production API application. However, to conveniently construct and test gRPC APIs in postman, these steps are required.","title":"Run()"},{"location":"tutorials/ds-postman/#shutdown","text":"Lastly, invoke the Shutdown() method the same way you invoked the Status() method. This will shutdown the container running locally on your machine.","title":"Shutdown()"},{"location":"tutorials/ds-postman/#tutorial-in-action","text":"Follow along as we walk through this tutorial step by step! .video-wrapper { position: relative; display: block; height: 0; padding: 0; overflow: hidden; padding-bottom: 56.25%; border: 1px solid gray; } .video-wrapper > iframe { position: absolute; top: 0; bottom: 0; left: 0; width: 100%; height: 100%; border: 0; }","title":"Tutorial in Action"}]}