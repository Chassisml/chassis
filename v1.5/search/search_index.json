{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"None","text":"<p>Welcome to Chassis.</p>"},{"location":"release-notes/","title":"Release Notes","text":""},{"location":"release-notes/#v150-beta","title":"v1.5.0 beta","text":"<p>August 2023  Experimental</p> <p>v1.5 is now in beta. This release culminates a significant refactor of the Chassis codebase and results in faster build times and smaller, faster model containers.</p> <ul> <li>Container build times are now 2-5x faster, especially for models with few <code>pip</code> dependencies</li> <li>Model containers built with Chassis v1.5 are now up to 10x smaller (though a 2-4x size reductions is more typical) than those generated by previous versions</li> <li>Docker builds are now supported, so installing Chassis on a K8s cluster is no longer a requirement</li> <li>The Chassis build server has been completely refactored: It is now fully configurable via Helm charts, no longer has an object storage dependency, supports multi-platform builds (e.g. <code>amd64</code>, <code>arm32v5</code>, <code>arm64v8</code>, etc.), and now uses considerably less CPU and RAM</li> <li>The chassisml SDK has been completely rewritten, improving usability, performance, size, and number of extra dependencies</li> <li>A convenience OMI client is now available that makes it easier to run inferences through OMI model containers</li> </ul>"},{"location":"release-notes/#v145","title":"v1.4.5","text":"<p>July 2022</p> <p>v1.4.5 release of Chassis inlcudes:</p> <ul> <li>Support for private docker registry configuration</li> <li>Removal of automatic model deployment to Modzy</li> </ul>"},{"location":"release-notes/#v100","title":"v1.0.0","text":"<p>March 2022</p> <p>v1.0.0 release of Chassis includes:</p> <ul> <li>GPU support</li> <li>Batch processing support </li> <li>Arm64 compatibility</li> <li>Open Model Interface compliance check</li> </ul>"},{"location":"release-notes/#v001","title":"v0.0.1","text":"<p>July 2021</p> <p>First working release of Chassis.</p>"},{"location":"get-involved/","title":"Get Involved","text":""},{"location":"get-involved/#community","title":"Community","text":"<p>Join the <code>#chassisml</code> channel on Modzy's Discord Server where our maintainers answer questions, share updates, and meet to plan changes and improvements.</p> <p>We also have a <code>#chassis-model-builder</code> Slack channel on the MLOps.community Slack!</p>"},{"location":"get-involved/#contributors","title":"Contributors","text":"<p>We are actively looking for new contributors to join us! In particular we're already focused on the next chapter: Chassis v2.0. We have some ambitious plans for our next major release and would eagerly accept contributions from experts in matrix math, Rust, and hardware optimization on CPUs, GPUs, and FPGAs. Also, we're happy to accept pull requests at modzy/chassis.</p> <p> A full list of contributors can be found here.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to Chassis!</p> <p>The getting started section of the Chassis docs site provides two easy-to-follow guides that will demonstrate how to use the Chassis SDK to build your first ML container on your computer using your local Docker daemon.</p> <p>There are two guides available:</p> <ol> <li>Quickstart Guide (&lt;5 minutes): Build your first container with Chassis in just minutes. In this guide, leverage a pre-trained scikit-learn classification model that comes with the <code>chassis</code> package to execute your first container build with a few lines of code.</li> <li>Full Chassis Workflow (~10 minutes): Learn how to transform your model into a single <code>predict</code> function with a few more lines of code. In this guide, you will unpack the pre-baked quickstart model and see how to construct a <code>ChassisModel</code> object. This will serve as a starting point for you to containerize your own model!</li> </ol> <p>What you will need</p> <p>Both guides in this section require two simple prerequisites to follow along:</p> <ol> <li>Python (v3.8 or greater supported)</li> <li>Docker (Installation instructions here)</li> </ol> <p>You can verify Docker it is successfully installed by typing <code>docker run hello-world</code> in your terminal.</p> <p></p> <p>First, you will need to set up a Python virtual enviornment and install the Chassis SDK. Include <code>--pre</code> to install the pre-release beta version and <code>[quickstart]</code> to install the extra dependencies required to use the quickstart model.</p> <pre><code>pip install --pre \"chassisml[quickstart]\"\n</code></pre>"},{"location":"getting-started/#build-container","title":"Build Container","text":"<p>With the SDK installed, you can now begin to build your first model container. Chassis's quickstart mode provides a pre-trained scikit-learn digits classification model as a simple import, so you do not need to bring your own model.</p> <p>Container Build</p> Python <p>Paste the below code snippet into your Python file (Jupyter notebook or script in other preferred IDE) to build a model container from the Chassis quickstart scikit-learn model.</p> <pre><code>import chassis.guides as guides\nfrom chassis.builder import DockerBuilder\n# Import a pre-trained scikit-learn digit classification model with pre-defined container metadata\nmodel = guides.QuickstartDigitsClassifier\n# Test the model with a picture of a handwritten \"5\"\nresults = model.test(guides.DigitsSampleData)\n# View test results\nprint(results)\n# Configure container builder option as Docker\nbuilder = DockerBuilder(model)\n# Build container for the model locally\njob_results = builder.build_image(\"my-first-chassis-model\")\n# View container info after the build completes\nprint(job_results)\n</code></pre> <p>Execute this snippet to kick off the local Docker build.</p> <p>This local container build should take just under a minute. The <code>job_results</code> of a successful build will display the details of your new container (note: the \"Image ID\" digest will be different for each build):</p> <pre><code>Generating Dockerfile...Done!\nCopying libraries...Done!\nWriting metadata...Done!\nCompiling pip requirements...Done!\nCopying files...Done!\nStarting Docker build...Done!\nImage ID: sha256:d222014ffe7bacd27382fb00cb8686321e738d7c80d65f0290f4c303459d3d65\nImage Tags: ['my-first-chassis-model:latest']\nCleaning local context\nCompleted:       True\nSuccess:         True\nImage Tag:       my-first-chassis-model:latest\n</code></pre> <p>Congratulations! You just built your first ML container from a scikit-learn digits classification model. Next, run a sample inference through this container with Chassis's OMI inference client.</p>"},{"location":"getting-started/#run-inference","title":"Run Inference","text":"<p>To quickly test your new model container, you can leverage Chassis's <code>OMIClient.test_container</code> convenience function. When executed, this function will spin up your container, run inference on sample data, and return the prediction results.</p> <p>Open a Python file (new or existing) and paste the following inference code. Again, we will use Chassis's quickstart mode to import load a sample piece of data.</p> <p>Inference</p> Python <p>The below inference code leverages Chassis's <code>OMIClient</code>. This client provides a convenience wrapper around a gRPC client that allows you to interact with the gRPC server within your model container.</p> <pre><code>import asyncio\nfrom chassis.client import OMIClient\nfrom chassis.guides import DigitsSampleData\nasync def run_test():\n# Execute the test_container method to spin up the container, run inference, and return the results\nres = await OMIClient.test_container(container_name=\"my-first-chassis-model\", inputs=DigitsSampleData, pull=False)\n# Parse results from output item\nresult = res.outputs[0].output[\"results.json\"]\n# View results\nprint(f\"Result: {result}\")\nif __name__ == '__main__':\nasyncio.run(run_test())\n</code></pre> <p>Execute this code to perform an inference against your running container.</p> <p>A successful inference run should yield the following result:</p> <pre><code>Result: b'[{\"data\": {\"result\": {\"classPredictions\": [{\"class\": 5, \"score\": 0.71212}]}}}]'\n</code></pre> <p>What's next?</p> <p>After completing this quickstart guide, you might be wondering how to integrate your own model into this workflow. This guide intentionally abstracts out much of the model configuration for a quick and easy experience to get up and running.</p> <p>Visit the Full Chassis Workflow guide to learn how to use Chassis with your own model!</p>"},{"location":"getting-started/full-workflow/","title":"Full Chassis Workflow","text":"<p>In this guide, we will transform a pre-trained scikit-learn digits classification model into a <code>ChassisModel</code> object that we will use to build a container.</p> <p>If you did not follow the Quickstart Guide, you will need to first set up a Python virtual enviornment and install the Chassis SDK. Include <code>--pre</code> to install the pre-release beta version and <code>[quickstart]</code> to install the extra dependencies required to use the quickstart model.</p> <pre><code>pip install --pre \"chassisml[quickstart]\"\n</code></pre>"},{"location":"getting-started/full-workflow/#build-container","title":"Build Container","text":"<p>Next, open a Python file (new or existing) and paste the following inference code. If you did follow the Quickstart guide, you will notice there is more code in the below example. That is because this example demonstrates the process of taking an in-memory model object, constructing a custom <code>predict</code> function, and using both to create your own <code>ChassisModel</code> object.</p> <p>Model Configuration &amp; Container Build</p> <p>Paste the below code snippet into your Python file (Jupyter notebook or script in other preferred IDE) to build a model container from scratch using a pre-trained model and sample data file embedded in the Chassis library. Make sure to check out the code annotations for detailed descriptions of each step!</p> <pre><code>import time\nimport json\nimport pickle\nimport cloudpickle\nimport numpy as np\nfrom typing import Mapping\nfrom chassisml import ChassisModel # (1)\nfrom chassis.builder import DockerBuilder # (2)\nimport chassis.guides as guides\n# load model # (3)\nmodel = pickle.load(guides.DigitsClassifier)\n# define predict function # (4)\ndef predict(input_bytes: Mapping[str, bytes]) -&gt; dict[str, bytes]:\ninputs = np.array(json.loads(input_bytes['input']))\ninference_results = model.predict_proba(inputs)\nstructured_results = []\nfor inference_result in inference_results:\nstructured_output = {\n\"data\": {\n\"result\": {\"classPredictions\": [{\"class\": np.argmax(inference_result).item(), \"score\": round(np.max(inference_result).item(), 5)}]}\n}\n}\nstructured_results.append(structured_output)\nreturn {'results.json': json.dumps(structured_results).encode()}\n# create chassis model object, add required dependencies, and define metadata\nchassis_model = ChassisModel(process_fn=predict)                # (5)\nchassis_model.add_requirements([\"scikit-learn\", \"numpy\"])       # (6)\nchassis_model.metadata.model_name = \"Digits Classifier\"         # (7)\nchassis_model.metadata.model_version = \"0.0.1\"\nchassis_model.metadata.add_input(\nkey=\"input\",\naccepted_media_types=[\"application/json\"],\nmax_size=\"10M\",\ndescription=\"Numpy array representation of digits image\"\n)\nchassis_model.metadata.add_output(\nkey=\"results.json\",\nmedia_type=\"application/json\",\nmax_size=\"1M\",\ndescription=\"Top digit prediction and confidence score\"\n)\n# test model # (8)\nresults = chassis_model.test(guides.DigitsSampleData)\nprint(results)\n# build container # (9)\nbuilder = DockerBuilder(chassis_model)\nstart_time = time.time()\nres = builder.build_image(name=\"my-first-chassis-model\", tag=\"0.0.1\", show_logs=True)\nend_time = time.time()\nprint(res)\nprint(f\"Container image built in {round((end_time-start_time)/60, 5)} minutes\")\n</code></pre> <ol> <li>First, we will import the <code>ChassisModel</code> class from the Chassis SDK. If you have not already done so, make sure you install it via PyPi: <code>pip install chassisml</code></li> <li>In addition to the <code>ChassisModel</code> object, we need to import a Builder object. The two available options, <code>DockerBuilder</code> and <code>RemoteBuilder</code>, will both build the same container but in different execution environments. Since we'd like to build a container locally with Docker, we will import the <code>DockerBuilder</code> object.</li> <li>Next, we will load our model. For this example, we have a pre-trained Scikit-learn classifier embedded into the Chassis library (<code>chassis.guides.DigitsClassifier</code>). When integrating Chassis into your own code, this can be done however you load your model. You might load your model from a pickle file, checkpoint file, multiple configuration files, etc. The key is that you load your model into memory so it can be accessed in the below <code>predict</code> function.</li> <li>Here, we will define a single predict function, which you can think of as an inference function for your model. This function can access in-memory objects (e.g., <code>model</code> loaded above), and the only requirement is it must convert input data from raw bytes form to the data type your model expects. See this guide for help on converting common data types. In this example, we process the raw bytes data using <code>numpy</code> and <code>json</code>, pass this processed data through to our model for predictions (<code>model.predict</code>), and perform some postprocessing to return the results in a human-readable manner. You can customize this function based on your model and preferences.</li> <li>Now, we will simply create a <code>ChassisModel</code> object directly from our predict function.</li> <li>With our <code>ChassisModel</code> object defined, there are a few optional methods we can call. Here, we will add the Python libraries our model will need to run. You can pass a list of packages you would list in a <code>requirements.txt</code> file that will be installed with Pip.</li> <li>In the next few lines, we will define the four minimum metadata fields that are required before building our container. These fields represent your model's name, version, inputs, and outputs. NOTE: There are many other optional fields you can choose to document if preferred.</li> <li>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> object by passing through sample data. For convenience, we can use the sample data embedded in the Chassis library specific to this Digits Classifier.</li> <li>After our test has passed, we can define our builder object, which as mentioned before, will be <code>DockerBuilder</code>. This builder object uses your local Docker daemon to build a model container and store it on your machine. First, we will simply pass our <code>ChassisModel</code> object to our builder, and build the container image using the <code>build_image</code> function.</li> </ol> <p>Execute this snippet to kick off the local Docker build</p> <p>This local container build should take just under a minute. The <code>job_results</code> of a successful build will display the details of your new container (note: the \"Image ID\" digest will be different for each build):</p> <pre><code>Generating Dockerfile...Done!\nCopying libraries...Done!\nWriting metadata...Done!\nCompiling pip requirements...Done!\nCopying files...Done!\nStarting Docker build...Done!\nImage ID: sha256:d222014ffe7bacd27382fb00cb8686321e738d7c80d65f0290f4c303459d3d65\nImage Tags: ['my-first-chassis-model:latest']\nCleaning local context\nCompleted:       True\nSuccess:         True\nImage Tag:       my-first-chassis-model:latest\n</code></pre> <p>Congratulations! You just transformed a scikit-learn digits classifier into a production container! Next, run a sample inference through this container with Chassis's OMI inference client.</p>"},{"location":"getting-started/full-workflow/#run-inference","title":"Run Inference","text":"<p>Before submitting data to your model container, you must first spin it up. To do so, open a terminal on your machine and run the container:</p> <pre><code>docker run --rm -it -p 45000:45000 my-first-chassis-model\n</code></pre> <p>When your container is spun up and running, you should see the following message in your logs:</p> <pre><code>Serving on: 45000\n</code></pre> <p>Next, open a Python file (new or existing) and paste the following inference code. Again, we will use a convenience import with Chassis's quickstart mode to load a sample piece of data.</p> <p>Inference</p> Jupyter NotebookOther Python IDE <p>The below inference code leverages Chassis's <code>OMIClient</code> for inference. This client provides a convenience wrapper around a gRPC client that allows you to interact with the gRPC server within your model container.</p> <pre><code>from chassis.client import OMIClient\nfrom chassis.guides import DigitsSampleData\n# Instantiate OMI Client connection to model running on localhost:45000 \nclient = OMIClient(\"localhost\", 45000)\n# Call and view results of status RPC \nstatus = await client.status()\nprint(f\"Status: {status}\")\n# Submit inference with quickstart sample data\nres = await client.run(DigitsSampleData)\n# Parse results from output item \nresult = res.outputs[0].output[\"results.json\"]\n# View results\nprint(f\"Result: {result}\")\n</code></pre> <p>Execute this code to perform an inference against your running container.</p> <p>The below inference code leverages Chassis's <code>OMIClient</code> for inference. Notice this code is slighly different than when running it in a Jupyter notebook, due to the built-in async functionality that comes with IPython.</p> <pre><code>import asyncio\nfrom chassis.client import OMIClient\nfrom chassis.guides import DigitsSampleData\nasync def run_test():\n# Instantiate OMI Client connection to model running on localhost:45000\nasync with OMIClient(\"localhost\", 45000) as client:\n# Call and view results of status RPC\nstatus = await client.status()\nprint(f\"Status: {status}\")\n# Submit inference with quickstart sample data\nres = await client.run(DigitsSampleData)\n# Parse results from output item\nresult = res.outputs[0].output[\"results.json\"]\n# View results\nprint(f\"Result: {result}\")\nif __name__ == '__main__':\nasyncio.run(run_test())\n</code></pre> <p>Execute this code to perform an inference against your running container.</p> <p>A successful inference run should yield the following result:</p> <pre><code>Result: b'[{\"data\": {\"result\": {\"classPredictions\": [{\"class\": 5, \"score\": 0.71212}]}}}]'\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>First, you will need to set up a Python virtual enviornment and install the Chassis SDK. Include <code>--pre</code> to install the pre-release beta version and <code>[quickstart]</code> to install the extra dependencies required to use the quickstart model.</p> <pre><code>pip install --pre \"chassisml[quickstart]\"\n</code></pre>"},{"location":"getting-started/quickstart/#build-container","title":"Build Container","text":"<p>With the SDK installed, you can now begin to build your first model container. Chassis's quickstart mode provides a pre-trained scikit-learn digits classification model as a simple import, so you do not need to bring your own model.</p> <p>Container Build</p> Python <p>Paste the below code snippet into your Python file (Jupyter notebook or script in other preferred IDE) to build a model container from the Chassis quickstart scikit-learn model.</p> <pre><code>import chassis.guides as guides\nfrom chassis.builder import DockerBuilder\n# Import a pre-trained scikit-learn digit classification model with pre-defined container metadata\nmodel = guides.QuickstartDigitsClassifier\n# Test the model with a picture of a handwritten \"5\"\nresults = model.test(guides.DigitsSampleData)\n# View test results\nprint(results)\n# Configure container builder option as Docker\nbuilder = DockerBuilder(model)\n# Build container for the model locally\njob_results = builder.build_image(\"my-first-chassis-model\")\n# View container info after the build completes\nprint(job_results)\n</code></pre> <p>Execute this snippet to kick off the local Docker build.</p> <p>This local container build should take just under a minute. The <code>job_results</code> of a successful build will display the details of your new container (note: the \"Image ID\" digest will be different for each build):</p> <pre><code>Generating Dockerfile...Done!\nCopying libraries...Done!\nWriting metadata...Done!\nCompiling pip requirements...Done!\nCopying files...Done!\nStarting Docker build...Done!\nImage ID: sha256:d222014ffe7bacd27382fb00cb8686321e738d7c80d65f0290f4c303459d3d65\nImage Tags: ['my-first-chassis-model:latest']\nCleaning local context\nCompleted:       True\nSuccess:         True\nImage Tag:       my-first-chassis-model:latest\n</code></pre> <p>Congratulations! You just built your first ML container from a scikit-learn digits classification model. Next, run a sample inference through this container with Chassis's OMI inference client.</p>"},{"location":"getting-started/quickstart/#run-inference","title":"Run Inference","text":"<p>To quickly test your new model container, you can leverage Chassis's <code>OMIClient.test_container</code> convenience function. When executed, this function will spin up your container, run inference on sample data, and return the prediction results.</p> <p>Open a Python file (new or existing) and paste the following inference code. Again, we will use Chassis's quickstart mode to import load a sample piece of data.</p> <p>Inference</p> Jupyter NotebookOther Python IDE <p>The below inference code leverages Chassis's <code>OMIClient</code>. This client provides a convenience wrapper around a gRPC client that allows you to interact with the gRPC server within your model container.</p> <pre><code>from chassis.client import OMIClient\nfrom chassis.guides import DigitsSampleData\n# Execute the test_container method to spin up the container, run inference, and return the results\nres = await OMIClient.test_container(container_name=\"my-first-chassis-model\", inputs=DigitsSampleData, pull=False)\n# Parse results from output item \nresult = res.outputs[0].output[\"results.json\"]\n# View results\nprint(f\"Result: {result}\")\n</code></pre> <p>Execute this code to perform an inference against your running container.   </p> <p>The below inference code leverages Chassis's <code>OMIClient</code> to run inference. Notice this code is slighly different than when running it in a Jupyter notebook, due to the built-in async functionality that comes with IPython. </p> <pre><code>import asyncio\nfrom chassis.client import OMIClient\nfrom chassis.guides import DigitsSampleData\nasync def run_test():\n# Execute the test_container method to spin up the container, run inference, and return the results\nres = await OMIClient.test_container(container_name=\"my-first-chassis-model\", inputs=DigitsSampleData, pull=False)\n# Parse results from output item\nresult = res.outputs[0].output[\"results.json\"]\n# View results\nprint(f\"Result: {result}\")\nif __name__ == '__main__':\nasyncio.run(run_test())\n</code></pre> <p>Execute this code to perform an inference against your running container.     </p> <p>A successful inference run should yield the following result:</p> <pre><code>Result: b'[{\"data\": {\"result\": {\"classPredictions\": [{\"class\": 5, \"score\": 0.71212}]}}}]'\n</code></pre> <p>What's next?</p> <p>After completing this quickstart guide, you might be wondering how to integrate your own model into this workflow. This guide intentionally abstracts out much of the model configuration for a quick and easy experience to get up and running.</p> <p>Visit the Full Chassis Workflow guide to learn how to use Chassis with your own model!</p>"},{"location":"guides/common-data-types/","title":"Support for Common Data Types","text":"<p>When using Chassis, you must define a <code>predict</code> function that serves as your model inference function. This function may include one or multiple input parameters, depending on your model. This parameter will always be a mapping of <code>str</code> to <code>bytes</code> (i.e., <code>Mapping[str, bytes]</code>), which means the beginning of your <code>process</code> function must account for this and be able to convert it to the expected data type for your model. This guide includes examples of how to decode raw bytes for common data types.</p> <p>Assume <code>input</code> is the key that represents a single input to your model in the mapping of <code>str</code> to <code>bytes</code> as your predict parameter.</p>"},{"location":"guides/common-data-types/#text","title":"Text","text":"Bytes DecodingProcess Function <pre><code>text = input_data['input'].decode()\n</code></pre> <pre><code>import json\nfrom typing import Mapping\ndef predict(input_data: Mapping[str, bytes]) -&gt; dict(str, bytes):\ntext = input_data['input'].decode()\n'''\n    Perform processing and inference on text\n    '''\nreturn {\"results.json\": json.dumps(output).encode()}\n</code></pre>"},{"location":"guides/common-data-types/#imagery","title":"Imagery","text":"<p>OpenCV</p> Bytes DecodingProcess Function <pre><code>import cv2\nimport numpy as np\nimg = cv2.imdecode(np.frombuffer(input_data['input'], np.uint8), -1)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n</code></pre> <pre><code>import json\nfrom typing import Mapping    \ndef predict(input_data: Mapping[str, bytes]) -&gt; dict(str, bytes):\nimg = cv2.imdecode(np.frombuffer(input_data['input'], np.uint8), -1)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n'''\n    Perform processing and inference on img\n    '''\nreturn {\"results.json\": json.dumps(output).encode()}\n</code></pre> <p>Pillow</p> Bytes DecodingProcess Function <pre><code>import io\nfrom PIL import Image\nimg = Image.open(io.BytesIO(input_data['input'])).convert(\"RGB\")\n</code></pre> <pre><code>import json\nfrom typing import Mapping    \ndef predict(input_data: Mapping[str, bytes]) -&gt; dict(str, bytes):\nimg = Image.open(io.BytesIO(input_data['input'])).convert(\"RGB\")\n'''\n    Perform processing and inference on img\n    '''\nreturn {\"results.json\": json.dumps(output).encode()}\n</code></pre> <p>See also:</p> <ul> <li>Native PyTorch function</li> <li>Native Tensorflow function</li> </ul>"},{"location":"guides/common-data-types/#tabular","title":"Tabular","text":"Bytes DecodingProcess Function <pre><code>from io import StringIO\nimport pandas as pd\ninput_table = pd.read_csv(StringIO(str(input_data['input'], \"utf-8\")))\n</code></pre> <pre><code>import json\nfrom typing import Mapping    \ndef predict(input_data: Mapping[str, bytes]) -&gt; dict(str, bytes):\ninput_table = pd.read_csv(StringIO(str(input_data['input'], \"utf-8\")))\n'''\n    Perform processing and inference on input_table\n    '''\nreturn {\"results.json\": json.dumps(output).encode()}\n</code></pre>"},{"location":"guides/install-service/","title":"Install Remote Service","text":"<p>This tutorial provides end-to-end instructions for installing the Chassis service on a remote Kubernetes cluster. </p> <p>What you will need</p> <p>To follow this tutorial, make sure you have met the following requirements</p> <ul> <li> Kubernetes: we recommend any non-development Kubernetes distribution to stand up and host the service </li> <li> Docker registry: required to stand up remote service. Note: only HTTP API v2 compliant Docker registries are supported </li> <li> Helm: used to install the Chassis helm chart. Follow these installation instructions if needed</li> </ul> <p>With these prerequisites met, you can now install the Chassis remote service with your custom configuration, all using helm.</p>"},{"location":"guides/install-service/#add-the-helm-repository","title":"Add the Helm repository","text":"<p>First, add the Chassis helm repository to your cluster.</p> <pre><code>helm repo add chassis https://modzy.github.io/chassis\n</code></pre> <p>Next, update the Helm repositories to fetch <code>Chassis</code> data.</p> <pre><code>helm repo update\n</code></pre>"},{"location":"guides/install-service/#configure-private-docker-registry-settings","title":"Configure private Docker registry settings","text":"<p>To install the remote Chassis service, you must configure the Chassis values yaml file with your own Docker registry (only Docker v2 registries are supported). When properly configured, the remote service will build container images and push them to this registry.  </p> <p>If your registry is private and requires credentials, you can generate a Kubernetes secret and pass the name of this secret into your <code>values.yml</code> file.</p> <p>1. Generate Kuberentes secret containing registry credentials (Optional)</p> <p>If your registry requires credentials, generate a Kubernetes secret of type <code>dockerconfigjson</code> that contains Docker registry credentials with push/pull permissions.</p> <p>Kubernetes Secret</p> Terminal <p>Run this code directly in your terminal</p> <pre><code>kubectl create secret docker-registry &lt;registry-secret-name&gt; \\\n--docker-server=&lt;private-registry-url&gt; \\\n--docker-email=&lt;private-registry-email&gt; \\\n--docker-username=&lt;private-registry-user&gt; \\\n--docker-password=&lt;private-registry-password&gt;\n</code></pre> <p>Note: Keep the name of your secret (<code>&lt;registry-secret-name&gt;</code>) for the next step</p> <p>Visit Managing Secrets using kubectl for more details.</p> <p>2. Create Values File for Helm Chart</p> <p>Next, create a <code>values.yml</code> file that contains fields required to install the remote service. Below is an example file that contains most of the fields you might want to change. Follow these guidelines when filling out your file:</p> <ul> <li> Registry (Mandatory): Section to add in your Docker registry configuration, including the url, secret name if it requires credentials to access, repository prefix, and whether or not it is insecure</li> <li> Persistence (Optional): Configuration of persistence volumes</li> <li> Builder (Optional): Configuration for service-specific fields, including timeouts, resources allocated to builder pods, and duration of time until removing finished jobs</li> </ul> <p>Values File</p> YAML <p>Note: If you are using a Docker registry that requires credentials, make sure you paste the name of the Kubernetes secret you created above in the <code>credentialsSecretName</code> field under the <code>registry</code> section. </p> values.yml<pre><code>registry:\n# The base URL to the destination registry that Chassis will push final\nurl: \"\"\n# The name of a Kubernetes secret of type \"dockerconfigjson\"\ncredentialsSecretName: \"\" # \"&lt;registry-secret-name&gt;\" - same name as Kubernetes secret created above\n# Optional prefix to be applied to image repositories created by Chassis\nrepositoryPrefix: \"\"\n# Set to true if the registry is considered insecure. An insecure registry\n# is one that is hosted using HTTP or uses an untrusted TLS certificate.\ninsecure: false\npersistence:\n# The max context size that the API supports is 20Gi so setting the value to\n# lower than 20Gi could result in undefined behavior. If you need to support\n# deploying multiple large models simultaneously then you should increase\n# this value.\nsize: 20Gi\naccessMode: ReadWriteOnce\n#storageClass: default\nbuilder:\n# Set the timeout for how long a build job can take before it is canceled.\ntimeout: 3600 # in seconds; 3600 == one hour\n# Set the amount of time before the Job is cleaned up and removed from\n# Kubernetes.\nttlSecondsAfterFinished: 3600 # in seconds; 3600 == one hour\n# Set the resource requests and limits used by the builder job pods.\nresources: {}\n# limits:\n#   cpu: 100m\n#   memory: 128Mi\n# requests:\n#   cpu: 100m\n#   memory: 128Mi\n</code></pre>"},{"location":"guides/install-service/#install-chassis-service","title":"Install <code>Chassis</code> service","text":"<p>Now we just need to install <code>Chassis</code> as normal using Helm.</p> <p>Install Service</p> Terminal <pre><code>helm install chassis chassis/chassis -f values.yaml\n</code></pre>"},{"location":"guides/install-service/#check-the-installation","title":"Check the installation","text":"<p>After having installed the service we can check that the <code>Chassis</code> service is correctly deployed.</p> <p>Check Service</p> Terminal <pre><code>kubectl get svc/chassis\n</code></pre> <p>Then you should see an output similar to this.</p> <pre><code>NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nchassis   NodePort   10.106.209.207   &lt;none&gt;        80:30496/TCP   15s\n</code></pre> <p>We can also check that the pod that runs the service is correctly running.</p> <pre><code>kubectl get pods\n</code></pre> <p>Where we should find our pod listed.</p> <pre><code>NAME                       READY   STATUS    RESTARTS   AGE\n(...)\nchassis-5c574d459c-rclx9   1/1     Running   0          22s\n(...)\n</code></pre>"},{"location":"guides/install-service/#query-the-service","title":"Query the service","text":"<p>To conclude, we can query the service just to see that it answers as we expect.</p> <p>To do that, we need to port forward the service.</p> <pre><code>kubectl port-forward svc/chassis 8080:8080\n</code></pre> <p>Now that we have access to the service we can query it.</p> <pre><code>curl localhost:8080\n</code></pre> <p>Which should output an alive message.</p>"},{"location":"guides/install-service/#begin-using-remote-service","title":"Begin Using Remote Service","text":"<p>Congratulations, you have now successfully deployed the service in a private Kubernetes cluster. To get started, make sure you set up a Python virtual enviornment and install the <code>chassisml</code> SDK.</p> <pre><code>pip install chassisml\n</code></pre> <p>For more resources, check out our guides.</p>"},{"location":"guides/frameworks/","title":"Commom ML/DL Frameworks","text":"<p>This collection of how-to guides provides sample workflows that leverage Chassis to create containers out of your models built from the most popular ML frameworks. NOTE: This list of frameworks is not all-inclusive. </p> <p>Can't find the framework you are looking for? Feel free to fork this repository, add an example or two from your framework of choice, and open a PR. Or come chat with us directly on Discord!</p> <p>What you will need</p> <p>To follow along with the how-to guides, you will need to install the Chassis SDK:</p> <pre><code>pip install --pre chassisml\n</code></pre> <p>Additional dependencies that are example-specific will be listed on each page, respectively. If you would like to learn more about the Chassis workflow before diving in to these framework-specific examples, be sure to check out the Getting Started section!</p> <p>If you prefer to jump straight to the source code, check out the examples section in the Chassis GitHub repository!</p> <p>Chassis Examples on GitHub </p>"},{"location":"guides/frameworks/#included-framework-examples","title":"Included Framework Examples","text":"<p>Jump straight to the framework you are looking for!</p> <p> Diffusers</p> <p> Torch</p> <p> Transformers</p>"},{"location":"guides/frameworks/diffusers/","title":"Diffusers","text":"<p>This page provides examples that build model containers from models built with the  Diffusers library.</p> <p>Each example requires the <code>diffusers</code> package be installed at a minimum. Use pip to install this in your Python environment:</p> <pre><code>pip install diffusers\n</code></pre> <p>GPU Recommended</p> <p>Like most computer vision-based and generative AI-based models, fast performance is heavily dependent on available hardware resources. For a smoother experience, it is highly recommended that you only follow these examples if you have access to a GPU. </p>"},{"location":"guides/frameworks/diffusers/#instruct-pix2pix","title":"Instruct Pix2Pix","text":"<p>This Diffusers example was adapted from the <code>InstructPix2Pix</code> model from Hugging Face. View the original model card here.</p> <p>Source Code</p> <p>If you prefer to view the source code directly, reference this example and utility data files here.</p> <p>First, install the additional dependencies required by this model.</p> <pre><code>pip install accelerate safetensors transformers\n</code></pre> <p>Now, simply copy the below example code in your Python environment and editor of choice. </p> <p>Example</p> <p>The below code can be used to build a model container out of a stable diffusion example model. Be sure to view the code annotations for more details.</p> Python <pre><code>import time\nimport torch\nfrom PIL import Image\nfrom diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler #(1)\nimport io\nfrom typing import Mapping, Dict\nfrom chassisml import ChassisModel #(2)\nfrom chassis.builder import DockerBuilder, BuildOptions #(3)\n# load model and define pipeline from diffusers package #(4)\nmodel_id = \"timbrooks/instruct-pix2pix\"\npipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\npipe.to(\"cuda\") #(5)\npipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n# define predict function # (6)\ndef predict(inputs: Mapping[str, bytes]) -&gt; Dict[str, bytes]:\nimg = Image.open(io.BytesIO(inputs['image'])).convert(\"RGB\")\ntext = inputs['prompt'].decode()\nimage_results = pipe(text, image=img, num_inference_steps=10, image_guidance_scale=1).images\nwith io.BytesIO() as output:\nimage_results[0].save(output, format=\"PNG\")\ncontents = output.getvalue()\nreturn {'results.png': contents} #(7)\n# create chassis model object\nchassis_model = ChassisModel(process_fn=predict)                                                       # (8)\n# add metadata &amp; requirements\nchassis_model.add_requirements([\"diffusers\", \"accelerate\", \"safetensors\", \"transformers\",   \"Pillow\"]) # (9)\nchassis_model.metadata.model_name = \"Stable Diffusion Instruct Pix2Pix\"                                # (10)\nchassis_model.metadata.model_version = \"0.0.1\"\nchassis_model.metadata.add_input(                                                                      # (11)\nkey=\"image\",\naccepted_media_types=[\"image/jpeg\", \"image/png\"],\nmax_size=\"10M\",\ndescription=\"Input image to be modified or transformed\"\n)\nchassis_model.metadata.add_input(\nkey=\"prompt\",\naccepted_media_types=[\"text/plain\"],\nmax_size=\"1M\",\ndescription=\"Text prompt with instructions for the model\"\n)\nchassis_model.metadata.add_output(\nkey=\"results.png\",\nmedia_type=\"image/png\",\nmax_size=\"10M\",\ndescription=\"AI-generated image based on input image and instructions prompt\"\n)\n# test model  # (12)\nimg_path = open(\"data/example.jpg\", 'rb').read()\ntext = b\"turn him into captain america\"\nresults = chassis_model.test({\"image\": img_path, \"prompt\": text})\n# test output results  # (13)\ntest = Image.open(io.BytesIO(results[0]['results.png'])).convert(\"RGB\")\ntest.save(\"example_out.png\")\n# define build options and builder object # (14)\nbuild_options = BuildOptions(cuda_version=\"11.0.3\")\nbuilder = DockerBuilder(package=chassis_model, options=build_options)\n# local docker mode (#15)\nstart_time = time.time()\nres = builder.build_image(name=\"stable-diffusion-instructpix2pix\", show_logs=True)\nend_time = time.time()\nprint(res)\nprint(f\"Container image built in {round((end_time-start_time)/60, 5)} minutes\")    \n</code></pre> <ol> <li>Here, we simply import the objects we will need from the <code>diffusers</code> library</li> <li>Now, we will import the <code>ChassisModel</code> class from the Chassis SDK</li> <li>In addition to the <code>ChassisModel</code> object, we need to import a Builder object. The two available options, <code>DockerBuilder</code> and <code>RemoteBuilder</code>, will both build the same container but in different execution environments. Since we'd like to build a container locally with Docker, we will import the <code>DockerBuilder</code> object.     </li> <li>Here, we are loading our model in the exact way the model card on Hugging Face recommends</li> <li>Running on GPU will dramatically improve performance</li> <li>Here, we will define a single predict function, which you can think of as an inference function for your model. This function can access in-memory objects (e.g., <code>model</code> loaded above), and the only requirement is it must convert input data from raw bytes form to the data type your model expects. See this guide for help on converting common data types. In this example, we process the raw bytes data using the Pillow library and the <code>io.BytesIO</code> functionality, pass this processed data through to our model for predictions, and encode the output image in the PNG image format.</li> <li>Notice we are returning a Dictionary where the key-value pair is <code>str</code> --&gt; <code>bytes</code>, but in this case, the <code>bytes</code> object is an image.</li> <li>Now, we will simply create a <code>ChassisModel</code> object directly from our predict function.</li> <li>With our <code>ChassisModel</code> object defined, there are a few optional methods we can call. Here, we will add the Python libraries our model will need to run. You can pass a list of packages you would list in a <code>requirements.txt</code> file that will be installed with Pip.</li> <li>In the next few lines, we will define the four minimum metadata fields that are required before building our container. These fields represent your model's name, version, inputs, and outputs. NOTE: There are many other optional fields you can choose to document if preferred. </li> <li>Notice we are adding two inputs. Chassis supports multi-input and multi-output model use cases.</li> <li>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> object by passing through sample data.</li> <li>Since our <code>predict</code> function returns encoded bytes, we can use the Pillow library to convert to a numpy array and save the output to verify the model output. </li> <li>After our test has passed, we can prepare our model build context. To do so, we will first define a <code>BuildOptions</code> object to pass the specific <code>cuda</code> version we prefer to use. Then, we define our builder object, which as mentioned before, will be <code>DockerBuilder</code>. This builder object uses your local Docker daemon to build a model container and store it on your machine.</li> <li>With our builder object defined with our model and build options as parameters, we can kick off the build using the <code>DockerBuilder.build_image</code> function to build a Docker container locally.</li> </ol> <p>Using the data inputs provided in the sample data folder, the inputs and outputs of this model should look similar to the following:</p> <p>Sample input</p> <p></p> <p>Prompt</p> <p>\"turn him into captain america\"</p> <p>Sample output</p> <p></p>"},{"location":"guides/frameworks/torch/","title":"Torch","text":"<p>This page provides examples that build model containers from models developed with  PyTorch.</p> <p>Each example requires the <code>torch</code> package be installed at a minimum. Use pip to install this in your Python environment:</p> <p>PyTorch Installation Tip</p> <p>NOTE: The PyTorch installation will vary slightly depending on your host machine, GPU accessibility, and preferences. Visit the PyTorch Installation Guide to ensure you are properly installing the package.</p> <pre><code>pip install torch\n</code></pre>"},{"location":"guides/frameworks/torch/#resnet-50-image-classification","title":"ResNet 50 Image Classification","text":"<p>This example was adapted from the <code>torchvision</code> guides for using pre-trained models and weights. View the torchvision documentation for reference.</p> <p>Source Code</p> <p>If you prefer to view the source code directly, reference this example and utility data files here.</p> <p>First, install the additional dependencies required by this model.</p> <pre><code>pip install torchvision Pillow\n</code></pre> <p>Now, simply copy the below example code in your Python environment and editor of choice. </p> <p>Example</p> <p>The below code can be used to build a model container from a <code>torchvision</code> pre-trained image classification model. Be sure to view the code annotations for more details.</p> Python <pre><code>import time\nimport io\nimport json\nimport torch\nfrom PIL import Image\nfrom typing import Mapping, Dict\nfrom torchvision.models import resnet50, ResNet50_Weights #(1)\nfrom chassisml import ChassisModel #(2)\nfrom chassis.builder import DockerBuilder #(3)\n# load and initialize model with the best available weights #(4)\nweights = ResNet50_Weights.DEFAULT \nmodel = resnet50(weights=weights)\nmodel.eval() #(5)\n# define the inference transforms\npreprocess = weights.transforms() #(6)\n# define predict function #(7)\ndef predict(inputs: Mapping[str, bytes]) -&gt; Dict[str, bytes]:\n# preprocess\ndecoded = Image.open(io.BytesIO(inputs['image'])).convert(\"RGB\")\nbatch = preprocess(decoded).unsqueeze(0)\n# run inference and apply softmax\nprediction = model(batch).squeeze(0).softmax(0)\n# postprocess and format results\n_, indices = torch.sort(prediction, descending=True)\nprint(indices[:5])\ninference_result = {\n\"classPredictions\": [\n{\"class\": weights.meta[\"categories\"][idx.item()], \"score\": round(prediction[idx].item(),4)}\nfor idx in indices[:5] ]\n}\nstructured_output = {\n\"data\": {\n\"result\": inference_result,\n\"explanation\": None,\n\"drift\": None,\n}\n}\nreturn {'results.json', json.dumps(structured_output).encode()}\n# create chassis model object\nchassis_model = ChassisModel(process_fn=predict)                        # (8)\n# add metadata &amp; requirements\nchassis_model.add_requirements([\"torch\", \"torchvision\", \"Pillow\"])      # (9)\nchassis_model.metadata.model_name = \"ResNet 50 Image Classification\"    # (10)\nchassis_model.metadata.model_version = \"0.0.1\"\nchassis_model.metadata.add_input(                                                                    \nkey=\"image\",\naccepted_media_types=[\"image/jpeg\", \"image/png\"],\nmax_size=\"10M\",\ndescription=\"Image to be classified by model\"\n)\nchassis_model.metadata.add_output(\nkey=\"results.json\",\nmedia_type=\"application/json\",\nmax_size=\"1M\",\ndescription=\"Classification predictions including human-readable label and the corresponding confidence score\"\n)\n# test model # (11)\nimg_bytes = open(\"data/airplane.jpg\", 'rb').read()\nresults = chassis_model.test({\"image\": img_bytes})\nprint(results)\n# define builder object # (12)\nbuilder = DockerBuilder(package=chassis_model)    \n# local docker mode # (13)\nstart_time = time.time()\nres = builder.build_image(name=\"resnet5-image-classification\", show_logs=True)\nend_time = time.time()\nprint(res)\nprint(f\"Container image built in {round((end_time-start_time)/60, 5)} minutes\")\n</code></pre> <ol> <li>Here, we simply import the pre-trained ResNet 50 weights file and architecture</li> <li>Now, we will import the <code>ChassisModel</code> class from the Chassis SDK    </li> <li>In addition to the <code>ChassisModel</code> object, we need to import a Builder object. The two available options, <code>DockerBuilder</code> and <code>RemoteBuilder</code>, will both build the same container but in different execution environments. Since we'd like to build a container locally with Docker, we will import the <code>DockerBuilder</code> object.     </li> <li>Here, we will load our model using the torchvision models method and will pass the pre-trained weights as a parameter</li> <li>It is important to set your model to <code>eval</code> mode. This will turn off layers only applicable for training</li> <li>Our in-memory model object includes an embedded preprocessing object that we will extract into its own variable</li> <li>Here, we will define a single predict function, which you can think of as an inference function for your model. This function can access in-memory objects (e.g., <code>preprocess</code> and <code>model</code>), and the only requirement is it must convert input data from raw bytes form to the data type your model expects. See this guide for help on converting common data types. In this example, we process the raw bytes and convert to an image using the Pillow library, pass this processed data through to our model for predictions, and encode the output as base64-encoded JSON.  </li> <li>Now, we will simply create a <code>ChassisModel</code> object directly from our predict function.</li> <li>With our <code>ChassisModel</code> object defined, there are a few optional methods we can call. Here, we will add the Python libraries our model will need to run. You can pass a list of packages you would list in a <code>requirements.txt</code> file that will be installed with Pip.</li> <li>In the next few lines, we will define the four minimum metadata fields that are required before building our container. These fields represent your model's name, version, inputs, and outputs. NOTE: There are many other optional fields you can choose to document if preferred. </li> <li>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> object by passing through sample data.</li> <li>After our test has passed, we can prepare our model build context. To do so, we will define our builder object, which as mentioned before, will be <code>DockerBuilder</code>. This builder object uses your local Docker daemon to build a model container and store it on your machine.</li> <li>With our builder object defined with our model predict function, we can kick off the build using the <code>DockerBuilder.build_image</code> function to build a Docker container locally.</li> </ol>"},{"location":"guides/frameworks/transformers/","title":"Transformers","text":"<p>This page provides examples that build model containers from models developed from the  Transformers library.</p> <p>Each example requires the <code>transformers</code> package be installed at a minimum. Use pip to install this in your Python environment:</p> <pre><code>pip install transformers\n</code></pre>"},{"location":"guides/frameworks/transformers/#text-classification","title":"Text Classification","text":"<p>This example was adapted from the <code>bhadresh-savani/distilbert-base-uncased-emotion</code> model from Hugging Face. View the original model card here.</p> <p>Source Code</p> <p>If you prefer to view the source code directly, reference this example and utility data files here.</p> <p>First, install the additional dependencies required by this model.</p> <pre><code>pip install torch numpy\n</code></pre> <p>Now, simply copy the below example code in your Python environment and editor of choice. </p> <p>Example</p> <p>The below code can be used to build a model container out of a transformers example model. Be sure to view the code annotations for more details.</p> Python <pre><code>import time\nimport json\nimport torch\nimport numpy as np\nfrom typing import Mapping, Dict\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification #(1)\nfrom chassisml import ChassisModel #(2)\nfrom chassis.builder import DockerBuilder #(3)\n# load model with AutoTokenizer package #(4)\ntokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/distilbert-base-uncased-emotion\")\ndistilbert_model = AutoModelForSequenceClassification.from_pretrained(\"bhadresh-savani/distilbert-base-uncased-emotion\")\n# define labels from model config #(5)\nlabels = distilbert_model.config.id2label\n# define predict function #(6)\ndef predict(inputs: Mapping[str, bytes]) -&gt; Dict[str, bytes]:\ntext = inputs['input.txt'].decode()\ninputs = tokenizer(text, return_tensors=\"pt\")\n# run preprocessed data through model\nwith torch.no_grad():\nlogits = distilbert_model(**inputs).logits\nsoftmax = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()\n# postprocess \nindices = np.argsort(softmax)[0][::-1]\nresults = {\n\"data\": {\n\"result\": {\n\"classPredictions\": [{\"class\": labels[i], \"score\": round(softmax[0][i].item(), 4)} for i in indices]\n}\n}\n}        \nreturn {'results.json': json.dumps(results).encode()}\n# create chassis model object\nchassis_model = ChassisModel(process_fn=predict)                       # (7)\n# add metadata &amp; requirements\nchassis_model.add_requirements([\"transformers\", \"torch\", \"numpy\"])     # (8)\nchassis_model.metadata.model_name = \"DistilBERT Text Classification\"   # (9)\nchassis_model.metadata.model_version = \"0.0.1\"\nchassis_model.metadata.add_input(                                                                    \nkey=\"input.txt\",\naccepted_media_types=[\"text/plain\"],\nmax_size=\"10M\",\ndescription=\"Input text to be classified by model\"\n)\nchassis_model.metadata.add_output(\nkey=\"results.json\",\nmedia_type=\"application/json\",\nmax_size=\"1M\",\ndescription=\"Classification predictions of six emotions classes and the corresponding confidence score\"\n)\n# test model  # (10)\ntext = open(\"data/input.txt\", 'rb').read()\nresults = chassis_model.test({\"input.txt\": text})\nprint(results)\n# define builder object # (11)\nbuilder = DockerBuilder(package=chassis_model)    \n# local docker mode #(12)\nstart_time = time.time()\nres = builder.build_image(name=\"distilbert-text-classification\", show_logs=True)\nend_time = time.time()\nprint(res)\nprint(f\"Container image built in {round((end_time-start_time)/60, 5)} minutes\")\n</code></pre> <ol> <li>Here, we simply import the objects we will need from the <code>transformers</code> library</li> <li>Now, we will import the <code>ChassisModel</code> class from the Chassis SDK    </li> <li>In addition to the <code>ChassisModel</code> object, we need to import a Builder object. The two available options, <code>DockerBuilder</code> and <code>RemoteBuilder</code>, will both build the same container but in different execution environments. Since we'd like to build a container locally with Docker, we will import the <code>DockerBuilder</code> object.     </li> <li>Here, we are loading our model in the exact way the model card on Hugging Face recommends</li> <li>With our loaded model, we can access the human-readable labels the model can predict. We will load them into a <code>labels</code> variable and reference it in our <code>predict</code> function below</li> <li>Here, we will define a single predict function, which you can think of as an inference function for your model. This function can access in-memory objects (e.g., <code>tokenizer</code>, <code>distilbert_model</code>, and <code>labels</code> which are all in-memory objects loaded above), and the only requirement is it must convert input data from raw bytes form to the data type your model expects. See this guide for help on converting common data types. In this example, we process the raw bytes text data with a Python native <code>decode()</code> function, pass this processed data through to our model for predictions, and encode the output as base64-encoded JSON.  </li> <li>Now, we will simply create a <code>ChassisModel</code> object directly from our predict function.</li> <li>With our <code>ChassisModel</code> object defined, there are a few optional methods we can call. Here, we will add the Python libraries our model will need to run. You can pass a list of packages you would list in a <code>requirements.txt</code> file that will be installed with Pip.</li> <li>In the next few lines, we will define the four minimum metadata fields that are required before building our container. These fields represent your model's name, version, inputs, and outputs. NOTE: There are many other optional fields you can choose to document if preferred. </li> <li>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> object by passing through sample data.</li> <li>After our test has passed, we can prepare our model build context. To do so, we will define our builder object, which as mentioned before, will be <code>DockerBuilder</code>. This builder object uses your local Docker daemon to build a model container and store it on your machine.</li> <li>With our builder object defined with our model predict function, we can kick off the build using the <code>DockerBuilder.build_image</code> function to build a Docker container locally.</li> </ol>"},{"location":"guides/frameworks/transformers/#code-llama-llm","title":"Code Llama (LLM)","text":"<p>This example was adapted from the <code>codellama/CodeLlama-7b-hf</code> Code Llama model in the Hugging Face Transformers format. View the original model card here.</p> <p>Source Code</p> <p>If you prefer to view the source code directly, reference this example and utility data files here.</p> <p>GPU Recommended</p> <p>Large Language Models (LLMs) are very resource intensive and run better with accelerated hardware. For a smoother experience, it is highly recommended that you only follow these examples if you have access to a GPU.</p> <p>Disk Storage Needed</p> <p>This LLM results in a container that is roughly 20GB in size. Ensure your machine has enough disk space left to handle this container before continuing.     </p> <p>First, install the additional dependencies required by this model.</p> <pre><code>pip install torch accelerate\n</code></pre> <p>Now, simply copy the below example code in your Python environment and editor of choice. </p> <p>Example</p> <p>The below code can be used to build a model container from Meta's Code Llama base model (7B params). Be sure to view the code annotations for more details.</p> Python <pre><code>import json\nimport time\nfrom typing import Mapping, Dict    \nimport torch\nimport transformers\nfrom transformers import AutoTokenizer #(1)\ntorch.cuda.empty_cache() \nfrom chassisml import ChassisModel #(2)\nfrom chassis.builder import DockerBuilder, BuildOptions #(3)\n# load model and dependencies #(4)\nmodel = \"codellama/CodeLlama-7b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model,\ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\n)\n# define predict function #(5)\ndef predict(inputs: Mapping[str, bytes]) -&gt; Dict[str, bytes]:\nprompt = inputs['input'].decode()\nsequences = pipeline(\nprompt,\ndo_sample=True,\ntop_k=10,\ntemperature=0.1,\ntop_p=0.50,\nnum_return_sequences=1,\neos_token_id=tokenizer.eos_token_id,\nmax_length=500,\n)\nreturn {\"generated_text\": json.dumps(sequences[0][\"generated_text\"]).encode()}\n# create chassis model object\nchassis_model = ChassisModel(process_fn=predict)                            # (6)\n# add metadata &amp; requirements\nchassis_model.add_requirements([\"transformers\", \"torch\", \"accelerate\"])     # (7)\nchassis_model.metadata.model_name = \"Code Llama\"                            # (8)\nchassis_model.metadata.model_version = \"0.0.1\"\nchassis_model.metadata.add_input(                                                                    \nkey=\"input\",\naccepted_media_types=[\"text/plain\"],\nmax_size=\"10M\",\ndescription=\"Code prompt\"\n)\nchassis_model.metadata.add_output(\nkey=\"generated_text\",\nmedia_type=\"application/json\",\nmax_size=\"10M\",\ndescription=\"Generated code by Code Llama model\"\n)\n# test model  # (9)\ninput = \"# Python hello world function\".encode()\nresults = chassis_model.test({\"input\": input})\nprint(results)\n# define builder object # (10)\nbuild_options = BuildOptions(cuda_version=\"11.0.3\")\nbuilder = DockerBuilder(chassis_model, build_options)\n# local docker mode #(11)\nstart_time = time.time()\nres = builder.build_image(name=\"distilbert-text-classification\", show_logs=True)\nend_time = time.time()\nprint(res)\nprint(f\"Container image built in {round((end_time-start_time)/60, 5)} minutes\")\n</code></pre> <ol> <li>Here, we simply import the objects we will need from the <code>transformers</code> library</li> <li>Now, we will import the <code>ChassisModel</code> class from the Chassis SDK    </li> <li>In addition to the <code>ChassisModel</code> object, we need to import a Builder object. The two available options, <code>DockerBuilder</code> and <code>RemoteBuilder</code>, will both build the same container but in different execution environments. Since we'd like to build a container locally with Docker, we will import the <code>DockerBuilder</code> object. Notice we also import <code>BuildOptions</code>. We will use this object to define the cuda version our models needs to run.     </li> <li>Here, we are loading our model in the exact way the model card on Hugging Face recommends. In this example, we will use the transformers pipeline functionality.</li> <li>Here, we will define a single predict function, which you can think of as an inference function for your model. This function can access in-memory objects (e.g., <code>tokenizer</code> and <code>pipeline</code>, which are both in-memory objects loaded above), and the only requirement is it must convert input data from raw bytes form to the data type your model expects. See this guide for help on converting common data types. In this example, we process the raw bytes text data with a Python native <code>decode()</code> function, pass this processed data through to our model for predictions, and encode the output as base64-encoded JSON.  </li> <li>Now, we will simply create a <code>ChassisModel</code> object directly from our predict function.</li> <li>With our <code>ChassisModel</code> object defined, there are a few optional methods we can call. Here, we will add the Python libraries our model will need to run. You can pass a list of packages you would list in a <code>requirements.txt</code> file that will be installed with Pip.</li> <li>In the next few lines, we will define the four minimum metadata fields that are required before building our container. These fields represent your model's name, version, inputs, and outputs. NOTE: There are many other optional fields you can choose to document if preferred. </li> <li>Before kicking off the Chassis job, we can test our <code>ChassisModel</code> object by passing through sample data.</li> <li>After our test has passed, we can prepare our model build context. To do so, we will define our builder object, which as mentioned before, will be <code>DockerBuilder</code>. This builder object uses your local Docker daemon to build a model container and store it on your machine. We will also define a <code>BuildOptions</code> variable, which allows us to configure container-level dependencies, including cuda version, architecture(s) to build, and more.</li> <li>With our builder object defined with our model predict function, we can kick off the build using the <code>DockerBuilder.build_image</code> function to build a Docker container locally.</li> </ol>"},{"location":"reference/","title":"Chassis Basics","text":"<p>Chassis's goal is to make it easier for Python developers to turn machine learning models into fully-feature, containerized ML services.</p> <p>Since machine learning development is typically performed in Python, Chassis was made to be a Python-first library, meaning that anything you need to define about how your ML model must operate as a service (e.g. writing a custom preprocessor, specifying if a GPU is required, asking for a specific Python or package versions, etc.) can be done simply in Python. After specifying how you want your model service to work, Chassis takes over the process of turning your trained model into a self-contained, ML-specific container image.</p> 1: Define model details in Python 2: Run builder.build_image() 3: Receive ready-to-run model image Define model info with a simple python script Send model info off to be built into a container Get back a self-contained ML container image You will:<ul> <li>Load a pre-trained model</li> <li>Define a predict function</li> <li>List PIP dependencies</li> <li>Set harware requirements</li> <li>[Optional] Add model docs</li> </ul> Chassis will:<ul> <li>Select base image</li> <li>Implement APIs</li> <li>Define layers</li> <li>Construct image</li> </ul> Resulting image:<ul> <li>Bakes in model, dependencies, and server</li> <li>Provides <code>run</code>, <code>status</code>, and <code>shutdown</code> RPCs</li> <li>Runs on Docker, containerd, Modzy, and more</li> </ul>"},{"location":"reference/#chassis-container-contents","title":"Chassis Container Contents","text":"<p>One might wonder, what does a chassis container include? In short, it bakes in all of the code, dependencies, and other resources needed to run the model it was built around.     This includes all of the artifacts that define the model itself, pre and post-proessing, the inference function, and the weights file. This also include the specific Python version and packages that your model relies upon. If you optionally provide model documentation, it provides a model card, of sorts, that can be accessed anytime the container is running. Finally, the container provides a server that makes the model available for inference.</p>"},{"location":"reference/basics/","title":"Chassis Basics","text":"<p>Chassis's goal is to make it easier for Python developers to turn machine learning models into fully-feature, containerized ML services.</p> <p>Since machine learning development is typically performed in Python, Chassis was made to be a Python-first library, meaning that anything you need to define about how your ML model must operate as a service (e.g. writing a custom preprocessor, specifying if a GPU is required, asking for a specific Python or package versions, etc.) can be done simply in Python. After specifying how you want your model service to work, Chassis takes over the process of turning your trained model into a self-contained, ML-specific container image.</p> 1: Define model details in Python 2: Run <code>builder.build_image()</code> 3: Receive ready-to-run model image Define model info with a simple python script Send model info off to be built into a container Get back a self-contained ML container image You will:<ul> <li>Load a pre-trained model</li> <li>Define a predict function</li> <li>List PIP dependencies</li> <li>Set harware requirements</li> <li>[Optional] Add model docs</li> </ul> Chassis will:<ul> <li>Select base image</li> <li>Implement APIs</li> <li>Define layers</li> <li>Construct image</li> </ul> Resulting image:<ul> <li>Bakes in model, dependencies, and server</li> <li>Provides <code>run</code>, <code>status</code>, and <code>shutdown</code> RPCs</li> <li>Runs on Docker, containerd, Modzy, and more</li> </ul>"},{"location":"reference/basics/#chassis-container-contents","title":"Chassis Container Contents","text":"<p>One might wonder, what does a chassis container include? In short, it bakes in all of the code, dependencies, and other resources needed to run the model it was built around.     This includes all of the artifacts that define the model itself, pre and post-proessing, the inference function, and the weights file. This also include the specific Python version and packages that your model relies upon. If you optionally provide model documentation, it provides a model card, of sorts, that can be accessed anytime the container is running. Finally, the container provides a server that makes the model available for inference.</p>"},{"location":"reference/interfaces/","title":"Interfaces","text":"<p>Chassis containers currently support two interfaces: the Open Model Interface (OMI) and KServe v1. These interfaces provide a way to interact with containerized models, primarily for running inferences. When building a model container with Chassis, you can specify which of these two interfaces you wish to use. Below you'll find more information about how to interact with model containers using each type of interface provided.</p>"},{"location":"reference/interfaces/#omi","title":"OMI","text":"<p>The Open Model Interace is a specification for a multi-platform OCI-compliant container image designed specifically for machine learning models.</p> <p>The OMI server provides a gRPC interface defined by OMI's protofile. This interface provides three remote procedure calls (RPCs) that are used to interact with a chassis contianer: <code>Status</code>, <code>Run</code>, and <code>Shutdown</code>. The Run RPC is most important, as it provides a simple way to run inferences through models packaged into chassis container images.</p> RPC Input Description Example Response Status None Returns the status of a running model. <code>{     \"inputs\": [...],     \"outputs\": [...],     \"status_code\": 200,     \"status\": \"OK\",     \"message\": \"Model Initialized Successfully.\",     \"model_info\": {         \"model_name\": \"Digits Classifier\",         \"model_version\": \"0.0.1\",         \"model_author\": \"\",         \"model_type\": \"grpc\",         \"source\": \"chassis\"     }... }</code> Run A run request message which includes one or more key value pairs, each representing a single model input. Submits data to a running model for inference and returns the model's results. <code>[{\"data\": {\"result\": {\"classPredictions\": [{\"class\": 5, \"score\": 0.71212}]}}}]</code> Shutdown None Sends the model container a shutdown message. <code>{    \"status_code\": 200, \"status\": \"OK\", \"message\": \"Model Shutdown Successfully.\" }</code> <p>Best ways to work with OMI models</p> <p>Because OMI models use gRPC rather than RESTful APIs, there are 3 ways to build applications that can interact with an OMI model:</p> <ol> <li>[RECOMMENDED] Via the OMI Python client which is automatically installed when you <code>pip install chassisml</code></li> <li>By building a language-specific client directly from the OMI protofile (best option for non-Python applications)</li> <li>By building a language-specific client using server reflection on a running chassis model container</li> </ol>"},{"location":"reference/interfaces/#kserve-v1","title":"KServe V1","text":"<p>KServe's V1 protocol offers a standardized prediction workflow across all model frameworks. This protocol version is still supported, but it is recommended that users migrate to the V2 protocol for better performance and standardization among serving runtimes. However, if a use case requires a more flexible schema than protocol v2 provides, v1 protocol is still an option.</p> API Verb Path Request Payload Response Payload List Models GET /v1/models <code>{\"models\": [&lt;model_name&gt;]}</code> Model Ready GET /v1/models/ <code>{\"name\": &lt;model_name&gt;,\"ready\": $bool}</code> Predict POST /v1/models/:predict {\"instances\": []} ** <code>{\"predictions\": []}</code> Explain POST /v1/models/:explain {\"instances\": []} ** <code>{\"predictions\": [], \"explanations\": []}</code> <p>See Kserve's documentation for full details: https://kserve.github.io/website/0.10/modelserving/data_plane/v1_protocol/</p>"},{"location":"reference/sdk/navigation/","title":"Navigation","text":"<ul> <li>chassis<ul> <li>builder<ul> <li>buildable</li> <li>context</li> <li>docker</li> <li>errors</li> <li>options</li> <li>remote</li> <li>response</li> <li>utils</li> </ul> </li> <li>client<ul> <li>omi</li> </ul> </li> <li>ftypes<ul> <li>types</li> </ul> </li> <li>guides<ul> <li>quickstart</li> </ul> </li> <li>metadata<ul> <li>model_metadata</li> </ul> </li> <li>runtime<ul> <li>constants</li> <li>model_runner</li> <li>numpy_encoder</li> </ul> </li> <li>scripts<ul> <li>normalize_requirements</li> </ul> </li> <li>server<ul> <li>kserve<ul> <li>server</li> </ul> </li> <li>omi<ul> <li>server</li> </ul> </li> </ul> </li> </ul> </li> <li>chassisml<ul> <li>v1<ul> <li>chassis_client</li> <li>chassis_model</li> <li>helpers</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/sdk/chassis/builder/buildable/","title":"buildable","text":""},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.REQUIREMENTS_SUBSTITUTIONS","title":"REQUIREMENTS_SUBSTITUTIONS  <code>module-attribute</code>","text":"<pre><code>REQUIREMENTS_SUBSTITUTIONS = {'opencv-python=': 'opencv-python-headless='}\n</code></pre>"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable","title":"Buildable","text":"<pre><code>Buildable()\n</code></pre> <p><code>Buildable</code> is an abstract base class the encodes all the behavior needed to build a Chassis container.</p> <p>Attributes:</p> Name Type Description <code>packaged</code> <p><code>True</code> if running in a built container.</p> <code>metadata</code> <p>The metadata for the model. It is initialized with no values.</p> <code>requirements</code> <code>set[str]</code> <p>A set of pip requirements needed by this model.</p> <code>apt_packages</code> <code>set[str]</code> <p>A set of <code>apt-get</code> packages required by this model.</p> <code>additional_files</code> <code>set[str]</code> <p>A set of additional files required by the model at runtime.</p> <code>python_modules</code> <code>dict</code> <p>A dictionary of Python objects that will be serialized using <code>cloudpickle</code> before being copied into the container. The key should be one of the constants defined in chassis.runtime.constants.</p>"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.packaged","title":"packaged  <code>instance-attribute</code>","text":"<pre><code>packaged = False\n</code></pre>"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata = ModelMetadata.default()\n</code></pre>"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.requirements","title":"requirements  <code>instance-attribute</code>","text":"<pre><code>requirements: set[str] = set()\n</code></pre>"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.apt_packages","title":"apt_packages  <code>instance-attribute</code>","text":"<pre><code>apt_packages: set[str] = set()\n</code></pre>"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.additional_files","title":"additional_files  <code>instance-attribute</code>","text":"<pre><code>additional_files: set[str] = set()\n</code></pre>"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.python_modules","title":"python_modules  <code>instance-attribute</code>","text":"<pre><code>python_modules: dict = {}\n</code></pre>"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.merge_package","title":"merge_package","text":"<pre><code>merge_package(package)\n</code></pre> <p>Allows for merging two Buildable objects. This will ensure that any pip requirements, apt packages, files, or modules are merged.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Buildable</code> <p>Another <code>Buildable</code> object to merge into this one.</p> required"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.add_requirements","title":"add_requirements","text":"<pre><code>add_requirements(reqs)\n</code></pre> <p>Declare a pip requirement for your model.</p> <p>The value of each requirement can be anything supported by a line in a <code>requirements.txt</code> file, including version constraints, etc.</p> <p>All pip requirements declared via this method will be automatically installed when the container is built.</p> <p>Parameters:</p> Name Type Description Default <code>reqs</code> <code>Union[str, list[str]]</code> <p>Single python package (str) or list of python packages that   are required dependencies to run the <code>ChassisModel.process_fn</code>   attribute. These values are the same values that would follow   <code>pip install</code> or that would be added to a Python dependencies   txt file (e.g., <code>requirements.txt</code>)</p> required"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.add_apt_packages","title":"add_apt_packages","text":"<pre><code>add_apt_packages(packages)\n</code></pre> <p>Add an OS package that will be installed via <code>apt-get</code>.</p> <p>If your model requires additional OS packages that are not part of the standard Python container, you can declare them here. Each package declared here will be <code>apt-get install</code>'d when the container is built.</p> <p>Parameters:</p> Name Type Description Default <code>packages</code> <code>Union[str, list]</code> <p>Single OS-level package (str) or list of OS-level packages       that are required dependencies to run the       <code>ChassisModel.process_fn</code> attribute. These values are the       same values that can be installed via <code>apt-get install</code>.</p> required"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.get_packaged_path","title":"get_packaged_path","text":"<pre><code>get_packaged_path(path)\n</code></pre> <p>Convenience method for developers wanting to implement their own subclasses of Buildable. This method will return the final path in the built container of any additional files, etc.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local path of a file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path the file will have in the final built container.</p>"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.verify_prerequisites","title":"verify_prerequisites","text":"<pre><code>verify_prerequisites(options)\n</code></pre> <p>Raises an exception if the object is not yet ready for building.</p> <p>Models require having a name, version, and at least one input and one output.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>BuildOptions</code> <p>The <code>BuildOptions</code> used for the build.</p> required"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.prepare_context","title":"prepare_context","text":"<pre><code>prepare_context(options=DefaultBuildOptions)\n</code></pre> <p>Constructs the build context that will be used to build the container.</p> <p>A build context is a directory containing a <code>Dockerfile</code> and any other resources the <code>Dockerfile</code> needs to build the container.</p> <p>This method is called just before the build is initiated and compiles all the resources necessary to build the container. This includes the <code>Dockerfile</code>, required Chassis library code, the server implementation indicated by the <code>BuildOptions</code>, the cloudpickle'd model, the serialized model metadata, copies of any additional files, and a <code>requirements.txt</code>.</p> <p>Typically, you won't call this method directly, it will be called automatically by a Builder. The one instance where you might want to use this method directly is if you want to inspect the contents of the build context before sending it to a Builder.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>BuildOptions</code> <p>The <code>BuildOptions</code> to be used for this build.</p> <code>DefaultBuildOptions</code> <p>Returns:</p> Type Description <code>BuildContext</code> <p>A <code>BuildContext</code> object.</p>"},{"location":"reference/sdk/chassis/builder/buildable/#chassis.builder.buildable.Buildable.render_dockerfile","title":"render_dockerfile","text":"<pre><code>render_dockerfile(options)\n</code></pre> <p>Renders an appropriate <code>Dockerfile</code> for this object with the supplied <code>BuildOptions</code>.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>BuildOptions</code> <p>The <code>BuildOptions</code> that will be used for this build.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string containing the contents for a <code>Dockerfile</code>.</p>"},{"location":"reference/sdk/chassis/builder/context/","title":"context","text":""},{"location":"reference/sdk/chassis/builder/context/#chassis.builder.context.BuildContext","title":"BuildContext","text":"<pre><code>BuildContext(base_dir=None, platforms=None)\n</code></pre> <p>This class provides configuration options for the build context of a Chassis model container.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>Optional[str]</code> <p>Optional directory path to save build context of model container</p> <code>None</code> <code>platforms</code> <code>Optional[List[str]]</code> <p>List of target platforms to build and compile container        versions. If multiple provided, the <code>RemoteBuilder</code> will        build a separate version for each architecture and push them        to the designated registry under the same container repository        and tag.</p> <code>None</code>"},{"location":"reference/sdk/chassis/builder/context/#chassis.builder.context.BuildContext.base_dir","title":"base_dir  <code>instance-attribute</code>","text":"<pre><code>base_dir = base_dir if base_dir is not None else tempfile.mkdtemp()\n</code></pre>"},{"location":"reference/sdk/chassis/builder/context/#chassis.builder.context.BuildContext.chassis_dir","title":"chassis_dir  <code>instance-attribute</code>","text":"<pre><code>chassis_dir = os.path.join(self.base_dir, 'chassis')\n</code></pre>"},{"location":"reference/sdk/chassis/builder/context/#chassis.builder.context.BuildContext.data_dir","title":"data_dir  <code>instance-attribute</code>","text":"<pre><code>data_dir = os.path.join(self.base_dir, PACKAGE_DATA_PATH)\n</code></pre>"},{"location":"reference/sdk/chassis/builder/context/#chassis.builder.context.BuildContext.platforms","title":"platforms  <code>instance-attribute</code>","text":"<pre><code>platforms: List[str] = platforms\n</code></pre>"},{"location":"reference/sdk/chassis/builder/context/#chassis.builder.context.BuildContext.cleanup","title":"cleanup","text":"<pre><code>cleanup()\n</code></pre> <p>Removes the folder used to stage files for the context.</p>"},{"location":"reference/sdk/chassis/builder/docker/","title":"docker","text":""},{"location":"reference/sdk/chassis/builder/docker/#chassis.builder.docker.DockerBuilder","title":"DockerBuilder","text":"<pre><code>DockerBuilder(package, options=DefaultBuildOptions)\n</code></pre> <p>Enables building Chassis images locally using Docker.</p> <p>To use this builder, you need to have Docker installed and have access to the Docker socket. If your Docker socket is in a non-standard location, use the appropriate Docker environment variables to tell the Docker SDK how to connect. Any environment variable supported by the official Docker Python SDK is supported.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Buildable</code> <p>Chassis model object that serves as the primary model code      to be containerized.</p> required <code>options</code> <code>BuildOptions</code> <p>Object that provides specific build configuration options.      See chassis.builder.options.BuildOptions for more      details.</p> <code>DefaultBuildOptions</code>"},{"location":"reference/sdk/chassis/builder/docker/#chassis.builder.docker.DockerBuilder.build_image","title":"build_image","text":"<pre><code>build_image(name, tag='latest', cache=False, show_logs=False, clean_context=True)\n</code></pre> <p>Builds a local container image using the host machine Docker daemon.</p> <p>To enable faster builds if you're iterating, you can set <code>cache=True</code> to not remove intermediate containers when the build completes. This uses more disk space but can significantly speed up subsequent builds.</p> <p>To see the full log output during the Docker build, set <code>show_logs=True</code>.</p> Note <p>The logs will be printed at the end and will not stream as it's executing.</p> <p>This method will automatically remove the build context (e.g. the temporary folder that all the files were staged in) at the end of the build. If you need to troubleshoot the files that are available to the Dockerfile, set <code>clean_context=False</code> and the directory will not be removed at the end of the build, allowing you to inspect it. If you want to inspect the context contents before building, see chassisml.ChassisModel.prepare_context.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of container image repository</p> required <code>tag</code> <code>str</code> <p>Tag of container image.</p> <code>'latest'</code> <code>cache</code> <code>bool</code> <p>If True caches container image layers.</p> <code>False</code> <code>show_logs</code> <code>bool</code> <p>If True shows logs of the completed job.</p> <code>False</code> <code>clean_context</code> <code>bool</code> <p>If False does not remove build context folder.</p> <code>True</code> <p>Returns:</p> Type Description <code>BuildResponse</code> <p>A <code>BuildResponse</code> object with details from the build job.</p> <p>Raises:</p> Type Description <code>BuildError</code> <p>If build job fails</p> <p>Example: <pre><code>from chassisml import ChassisModel\nfrom chassis.builder import DockerBuilder\nmodel = ChassisModel(process_fn=predict)\nbuilder = DockerBuilder(model)\nres = builder.build_image(name=\"chassis-model\")\n</code></pre></p>"},{"location":"reference/sdk/chassis/builder/errors/","title":"errors","text":""},{"location":"reference/sdk/chassis/builder/errors/#chassis.builder.errors.RequiredFieldMissing","title":"RequiredFieldMissing","text":"<pre><code>RequiredFieldMissing(message)\n</code></pre>"},{"location":"reference/sdk/chassis/builder/errors/#chassis.builder.errors.RequiredFieldMissing.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"reference/sdk/chassis/builder/options/","title":"options","text":""},{"location":"reference/sdk/chassis/builder/options/#chassis.builder.options.DefaultBuildOptions","title":"DefaultBuildOptions  <code>module-attribute</code>","text":"<pre><code>DefaultBuildOptions = BuildOptions()\n</code></pre>"},{"location":"reference/sdk/chassis/builder/options/#chassis.builder.options.BuildOptions","title":"BuildOptions  <code>dataclass</code>","text":"<p>Configures how the resulting Chassis container will be built.</p> <p>By default, it will configure the build to use:     - your machine's native CPU architecture (or <code>amd64</code> if it can't be       determined by <code>platform.machine()</code>)     - use Python 3.9 as the Python version in the container     - the Open Model Interface server</p> <p>When using chassis.builder.DockerBuilder, only a single architecture is supported. When using chassis.builder.RemoteBuilder, a multi-architecture build can be specified. To do a multi-architecture build, supply a list of architectures that you want to build for. The <code>RemoteBuilder</code> will then build each variant and deploy them all under a manifest located at the image tag. Clients that then pull that image tag will receive the container image appropriate for their native architecture.</p> <p>When setting the Python version, you can use either minor version or full patch version (e.g. \"3.9\" or \"3.9.17\"). The version will be used to set the base image to the corresponding official <code>python</code> image on Docker Hub.</p> <p>To enable GPU support, you can set <code>cuda_version</code> to a string. The string should match the version on one of the available image tags for <code>nvidia/cuda</code> on Docker Hub. Note: Due to limitations with the <code>nvidia/cuda</code> image, only Python 3.8 is currently supported for GPU-enabled images.</p> <p>Chassis supports building container images that support either the Open Model Interface standard or the standard(s) supported by KServe. Use \"omi\" or \"kserve\" as the <code>server</code> value.</p> <p>Unless <code>base_dir</code> is provided, a temporary directory will be used to stage all the resources. If <code>base_dir</code> is supplied, it will use that directory, which is expected to already exist and be empty. Setting <code>base_dir</code> is mostly useful for testing, developing Chassis itself, or if you want to use something other than Docker (but that supports using Dockerfiles) to build the container.</p> <p>Attributes:</p> Name Type Description <code>arch</code> <code>Union[str, List[str]]</code> <p>List of target platforms to build and compile container versions. See above for more information.</p> <code>python_version</code> <code>str</code> <p>Python version to build into container. Python v3.8 or greater supported.</p> <code>cuda_version</code> <code>Optional[str]</code> <p>CUDA version if model supports GPU.</p> <code>server</code> <code>str</code> <p>Server specification to build. \"omi\" and \"kserve\" supported.</p> <code>base_dir</code> <code>Optional[str]</code> <p>Optional directory path to save the build context.</p>"},{"location":"reference/sdk/chassis/builder/options/#chassis.builder.options.BuildOptions.base_dir","title":"base_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>base_dir: Optional[str] = None\n</code></pre>"},{"location":"reference/sdk/chassis/builder/options/#chassis.builder.options.BuildOptions.arch","title":"arch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arch: Union[str, List[str]] = platform.machine() or 'amd64'\n</code></pre>"},{"location":"reference/sdk/chassis/builder/options/#chassis.builder.options.BuildOptions.python_version","title":"python_version  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>python_version: str = f'{sys.version_info.major}.{sys.version_info.minor}'\n</code></pre>"},{"location":"reference/sdk/chassis/builder/options/#chassis.builder.options.BuildOptions.cuda_version","title":"cuda_version  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cuda_version: Optional[str] = None\n</code></pre>"},{"location":"reference/sdk/chassis/builder/options/#chassis.builder.options.BuildOptions.server","title":"server  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>server: str = 'omi'\n</code></pre>"},{"location":"reference/sdk/chassis/builder/options/#chassis.builder.options.BuildOptions.labels","title":"labels  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels: Optional[Dict[str, str]] = None\n</code></pre>"},{"location":"reference/sdk/chassis/builder/remote/","title":"remote","text":""},{"location":"reference/sdk/chassis/builder/remote/#chassis.builder.remote.RemoteBuilder","title":"RemoteBuilder","text":"<pre><code>RemoteBuilder(url, package, options=DefaultBuildOptions, credentials=None, tls_verify=True)\n</code></pre> <p>Initializes a connection to a Chassis remote build server for a Buildable object (like ChassisModel).</p> <p>A Docker context also be prepared according to the options supplied. The Docker context is a directory (in <code>/tmp</code> unless <code>base_dir</code> is given in <code>options</code>) containing a Dockerfile and all the resources necessary to build the container. For more information on how the context is prepared given the supplied options, see chassis.builder.Buildable.prepare_context.</p> <p>Examples:</p> <p>See chassis.builder.RemoteBuilder.build_image.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to the Chassis remote build server. Example: \"https://chassis.example.com:8443\".</p> required <code>package</code> <code>Buildable</code> <p>ChassisModel object that contains the code to be built.</p> required <code>options</code> <code>BuildOptions</code> <p>Object that provides specific build configuration options. See <code>chassis.builder.BuildOptions</code> for more details.</p> <code>DefaultBuildOptions</code> <code>credentials</code> <code>Optional[str]</code> <p>A string that will be used in the \"Authorization\" header.</p> <code>None</code> <code>tls_verify</code> <code>bool</code> <p>Whether to enable TLS verification.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if - the URL is not valid - the build server is not available - the build server is too old</p>"},{"location":"reference/sdk/chassis/builder/remote/#chassis.builder.remote.RemoteBuilder.build_image","title":"build_image","text":"<pre><code>build_image(name, tag='latest', timeout=3600, webhook=None, clean_context=True, block_until_complete=True)\n</code></pre> <p>Starts a remote build of the container. When finished, the built image will be pushed to the registry that the remote builder is configured for (see the Chassis remote build server Helm chart for configuration options) with the name and tag supplied as arguments.</p> <p>By default, the build will be submitted with a timeout of one hour. You can change this value if desired. If the build takes longer than the timeout value, it will be canceled.</p> <p>An optional webhook can be supplied as well. A webhook is a URL that will be called by the remote build server with the result of the build (see chassis.builder.BuildResponse).</p> <p>Finally, at the end of this function, the Docker context that was created when the RemoteBuilder was initialized will be deleted by default. To prevent this, pass <code>clean_context=False</code> to this function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of container image repository.</p> required <code>tag</code> <p>Tag of container image.</p> <code>'latest'</code> <code>timeout</code> <code>int</code> <p>Timeout value passed to build config object.</p> <code>3600</code> <code>webhook</code> <code>Optional[str]</code> <p>A URL that will be called when the remote build finishes.</p> <code>None</code> <code>clean_context</code> <code>bool</code> <p>If False does not remove build context folder.</p> <code>True</code> <code>block_until_complete</code> <code>bool</code> <p>If <code>True</code>, will block until the job is complete. To get an immediate response and poll for build completion yourself, set to <code>False</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>BuildResponse</code> <p>A <code>BuildResponse</code> object with details from the build job.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If webhook is not valid URL</p> <p>Example: <pre><code>from chassisml import ChassisModel\nfrom chassis.builder import RemoteBuilder, BuildOptions\nmodel = ChassisModel(process_fn=predict)\nmodel.metadata.model_name = \"My Awesome Model\"\nmodel.metadata.model_version = \"1.0.1\"\nmodel.metadata.add_input(\"input.txt\", [\"text/plain\"])\nmodel.metadata.add_output(\"results.json\", \"application/json\")\noptions = BuildOptions(arch=\"arm64\")\nbuilder = RemoteBuilder(\"http://localhost:8080\", model, options)\nresponse = builder.build_image(name=\"chassis-model\", tag=\"1.0.1\")\nprint(response)\n</code></pre></p>"},{"location":"reference/sdk/chassis/builder/remote/#chassis.builder.remote.RemoteBuilder.get_build_status","title":"get_build_status","text":"<pre><code>get_build_status(remote_build_id)\n</code></pre> <p>Checks the status of a remote build.</p> <p>Parameters:</p> Name Type Description Default <code>remote_build_id</code> <code>str</code> <p>Remote build identifier generated from [chassis.builder.RemoteBuilder.build_image].</p> required <p>Returns:</p> Type Description <code>BuildResponse</code> <p>A <code>BuildResponse</code> object with details from the build job.</p> <p>Example: <pre><code>from chassisml import ChassisModel\nfrom chassis.builder import RemoteBuilder, BuildOptions\nmodel = ChassisModel(process_fn=predict)\nmodel.metadata.model_name = \"My Awesome Model\"\nmodel.metadata.model_version = \"1.0.1\"\nmodel.metadata.add_input(\"input.txt\", [\"text/plain\"])\nmodel.metadata.add_output(\"results.json\", \"application/json\")\noptions = BuildOptions(arch=\"arm64\")\nbuilder = RemoteBuilder(\"http://localhost:8080\", model, options)\nresponse = builder.build_image(name=\"chassis-model\", tag=\"1.0.1\", block_until_complete=False)\nbuild_id = response.remote_build_id\nprint(builder.get_build_status(build_id))\n</code></pre></p>"},{"location":"reference/sdk/chassis/builder/remote/#chassis.builder.remote.RemoteBuilder.get_build_logs","title":"get_build_logs","text":"<pre><code>get_build_logs(remote_build_id)\n</code></pre> <p>Retrieves the logs from the remote build container.</p> <p>Parameters:</p> Name Type Description Default <code>remote_build_id</code> <code>str</code> <p>Remote build identifier generated from chassis.builder.RemoteBuilder.build_image.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The logs from the remote build container</p> <p>Example: <pre><code>from chassisml import ChassisModel\nfrom chassis.builder import RemoteBuilder, BuildOptions\nmodel = ChassisModel(process_fn=predict)\nmodel.metadata.model_name = \"My Awesome Model\"\nmodel.metadata.model_version = \"1.0.1\"\nmodel.metadata.add_input(\"input.txt\", [\"text/plain\"])\nmodel.metadata.add_output(\"results.json\", \"application/json\")\noptions = BuildOptions(arch=\"arm64\")\nbuilder = RemoteBuilder(\"http://localhost:8080\", model, options)\nresponse = builder.build_image(name=\"chassis-model\", tag=\"1.0.1\")\nbuild_id = response.remote_build_id\nprint(builder.get_build_logs(build_id))\n</code></pre></p>"},{"location":"reference/sdk/chassis/builder/remote/#chassis.builder.remote.RemoteBuilder.block_until_complete","title":"block_until_complete","text":"<pre><code>block_until_complete(remote_build_id, timeout=None, poll_interval=5)\n</code></pre> <p>Blocks until Chassis remote build is complete or the timeout has been reached. Polls the Chassis job API until a result is marked as Completed or Failed.</p> <p>Parameters:</p> Name Type Description Default <code>remote_build_id</code> <code>str</code> <p>Remote build identifier generated from chassis.builder.RemoteBuilder.build_image.</p> required <code>timeout</code> <code>Optional[int]</code> <p>Timeout threshold in seconds.</p> <code>None</code> <code>poll_interval</code> <code>int</code> <p>Amount of time in seconds to wait in between API polls to check the build status.</p> <code>5</code> <p>Returns:</p> Type Description <code>BuildResponse</code> <p>A <code>BuildResponse</code> object with details from the build job.</p> <p>Example: <pre><code>from chassisml import ChassisModel\nfrom chassis.builder import RemoteBuilder, BuildOptions\nmodel = ChassisModel(process_fn=predict)\nmodel.metadata.model_name = \"My Awesome Model\"\nmodel.metadata.model_version = \"1.0.1\"\nmodel.metadata.add_input(\"input.txt\", [\"text/plain\"])\nmodel.metadata.add_output(\"results.json\", \"application/json\")\noptions = BuildOptions(arch=\"arm64\")\nbuilder = RemoteBuilder(\"http://localhost:8080\", model, options)\nresponse = builder.build_image(name=\"chassis-model\", tag=\"1.0.1\", block_until_complete=False)\nbuild_id = response.remote_build_id\nresponse = builder.block_until_complete(build_id)\nprint(response)\n</code></pre></p>"},{"location":"reference/sdk/chassis/builder/response/","title":"response","text":""},{"location":"reference/sdk/chassis/builder/response/#chassis.builder.response.BuildResponse","title":"BuildResponse  <code>dataclass</code>","text":"<p>An object representing the status of a Chassis build.</p> <p>This object is used for both local and remote builds. For local builds, this object will be returned exactly once at the end of the build process with all fields set except for <code>remote_build_id</code>.</p> <p>For remote builds, this object will be returned each time chassis.builder.RemoteBuilder.get_build_status is called and will only have all its fields set once the build is complete.</p> <p>Attributes:</p> Name Type Description <code>image_tag</code> <code>Optional[str]</code> <p>The URI where the built image was pushed.</p> <code>logs</code> <code>Optional[str]</code> <p>Build logs, if requested.</p> <code>success</code> <code>bool</code> <p>Whether the build was successful or not.</p> <code>completed</code> <code>bool</code> <p>Whether the build is finished or not.</p> <code>error_message</code> <code>Optional[str]</code> <p>The error message, if applicable.</p> <code>remote_build_id</code> <code>Optional[str]</code> <p>The unique ID of a remote build, if applicable.</p>"},{"location":"reference/sdk/chassis/builder/response/#chassis.builder.response.BuildResponse.image_tag","title":"image_tag  <code>instance-attribute</code>","text":"<pre><code>image_tag: Optional[str]\n</code></pre>"},{"location":"reference/sdk/chassis/builder/response/#chassis.builder.response.BuildResponse.logs","title":"logs  <code>instance-attribute</code>","text":"<pre><code>logs: Optional[str]\n</code></pre>"},{"location":"reference/sdk/chassis/builder/response/#chassis.builder.response.BuildResponse.success","title":"success  <code>instance-attribute</code>","text":"<pre><code>success: bool\n</code></pre>"},{"location":"reference/sdk/chassis/builder/response/#chassis.builder.response.BuildResponse.completed","title":"completed  <code>instance-attribute</code>","text":"<pre><code>completed: bool\n</code></pre>"},{"location":"reference/sdk/chassis/builder/response/#chassis.builder.response.BuildResponse.error_message","title":"error_message  <code>instance-attribute</code>","text":"<pre><code>error_message: Optional[str]\n</code></pre>"},{"location":"reference/sdk/chassis/builder/response/#chassis.builder.response.BuildResponse.remote_build_id","title":"remote_build_id  <code>instance-attribute</code>","text":"<pre><code>remote_build_id: Optional[str]\n</code></pre>"},{"location":"reference/sdk/chassis/builder/response/#chassis.builder.response.BuildError","title":"BuildError","text":"<pre><code>BuildError(error, logs=None)\n</code></pre>"},{"location":"reference/sdk/chassis/builder/response/#chassis.builder.response.BuildError.logs","title":"logs  <code>instance-attribute</code>","text":"<pre><code>logs = logs\n</code></pre>"},{"location":"reference/sdk/chassis/builder/utils/","title":"utils","text":""},{"location":"reference/sdk/chassis/builder/utils/#chassis.builder.utils.sanitize_image_name","title":"sanitize_image_name","text":"<pre><code>sanitize_image_name(image_name, tag='latest')\n</code></pre> <p>Sanitizes the image name according to the Docker spec.</p> <pre><code>The tag must be valid ASCII and can contain lowercase and uppercase\nletters, digits, underscores, periods, and hyphens. It cannot start\nwith a period or hyphen and must be no longer than 128 characters.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the image to sanitize, without the tag component.</p> required <code>tag</code> <code>str</code> <p>The tag of the image.</p> <code>'latest'</code> <p>Returns:</p> Type Description <code>str</code> <p>The full sanitized image tag suitable for use with <code>docker tag</code>, etc.</p>"},{"location":"reference/sdk/chassis/client/omi/","title":"omi","text":""},{"location":"reference/sdk/chassis/client/omi/#chassis.client.omi.OMIClient","title":"OMIClient","text":"<pre><code>OMIClient(host, port=45000, timeout=10)\n</code></pre> <p>Provides a convenient client for interacting with a model running the OMI (Open Model Interface) server.</p> <p>The API for this object is asynchronous like the underlying gRPC <code>grpclib</code> client.</p> <p>Attributes:</p> Name Type Description <code>client</code> <p>Access to the underlying <code>grpclib</code> async client.</p> Example <pre><code>async with OMIClient(\"localhost\", 45000) as client:\nstatus = await client.status()\nprint(f\"Status: {status}\")\nres = await client.run([{\"input\": b\"testing one two three\"}])\nresult = res.outputs[0].output[\"results.json\"]\nprint(f\"Result: {result}\")\n</code></pre>"},{"location":"reference/sdk/chassis/client/omi/#chassis.client.omi.OMIClient.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client = ModzyModelStub(self._channel)\n</code></pre>"},{"location":"reference/sdk/chassis/client/omi/#chassis.client.omi.OMIClient.status","title":"status  <code>async</code>","text":"<pre><code>status()\n</code></pre> <p>Queries the model to get its status.</p> <p>The first time this method is called it will also initialize the model, giving it the opportunity to load any assets or perform any setup required to perform inferences.</p> <p>Returns:</p> Type Description <code>StatusResponse</code> <p>The status of the model.</p>"},{"location":"reference/sdk/chassis/client/omi/#chassis.client.omi.OMIClient.run","title":"run  <code>async</code>","text":"<pre><code>run(inputs, detect_drift=False, explain=False)\n</code></pre> <p>Perform an inference.</p> <p>The <code>inputs</code> parameter represents a batch of inputs to send to the model. If the model supports batch, then it will process the inputs in groups according to its batch size. If the model does not support batch, then it will loop through each input and process it individually.</p> <p>Each input in <code>inputs</code> is a dictionary that allows multiple pieces of data to be supplied to the model for each discrete inference. The key should match the key name expected by the model (e.g. the first value supplied in the ChassisModel.metadata.add_input method) and the value should always be of type <code>bytes</code>. The bytes should be decodable using one of the model's declared media type for that key (e.g. the <code>accepted_media_types</code> argument in ChassisModel.metadata.add_input).</p> <p>To enable drift detection and/or explainability on models that support it, you can set the appropriate parameters to <code>True</code>.</p> <p>In the <code>RunResponse</code> object, you will be given a similar structure to the inputs. The <code>outputs</code> property will be an array that corresponds to the batch of inputs. The index of each item in <code>outputs</code> will be the inference result for the corresponding index in the <code>inputs</code> array. And similarly to inputs, each inference result will be a dictionary that can return multiple pieces of data per inference. The key and media type of the bytes value should match the values supplied in ChassisModel.metadata.add_input).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Sequence[Mapping[str, bytes]]</code> <p>The batch of inputs to supply to the model. See above for more information.</p> required <code>detect_drift</code> <code>bool</code> <p>Whether to enable drift detection on models that support it.</p> <code>False</code> <code>explain</code> <code>bool</code> <p>Whether to enable explainability on models that support it.</p> <code>False</code> <p>Returns:</p> Type Description <code>RunResponse</code> <p>See above for more details.</p>"},{"location":"reference/sdk/chassis/client/omi/#chassis.client.omi.OMIClient.shutdown","title":"shutdown  <code>async</code>","text":"<pre><code>shutdown()\n</code></pre> <p>Tells the model to shut itself down. The container will immediately shut down upon receiving this call.</p>"},{"location":"reference/sdk/chassis/client/omi/#chassis.client.omi.OMIClient.test_container","title":"test_container  <code>async</code> <code>classmethod</code>","text":"<pre><code>test_container(container_name, inputs, tag='latest', port=45000, timeout=10, pull=True, detect_drift=False, explain=False)\n</code></pre> <p>Tests a container. This method will use your local Docker engine to spin up the named container, perform an inference against it with the given <code>inputs</code>, and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>container_name</code> <code>str</code> <p>The full name of the container without the tag.</p> required <code>inputs</code> <code>Sequence[Mapping[str, bytes]]</code> <p>A batch of input(s) to perform inference on. See chassis.client.OMIClient.run for more information.</p> required <code>tag</code> <code>str</code> <p>The tag of the image to test.</p> <code>'latest'</code> <code>port</code> <code>int</code> <p>The port on the host that the container should map to.</p> <code>45000</code> <code>timeout</code> <code>int</code> <p>A timeout in seconds to wait for the model container to become available.</p> <code>10</code> <code>pull</code> <code>bool</code> <p>Whether to pull the image if it doesn't exist in your local image cache.</p> <code>True</code> <code>detect_drift</code> <code>bool</code> <p>Whether to enable drift detection on models that support it.</p> <code>False</code> <code>explain</code> <code>bool</code> <p>Whether to enable explainability on models that support it.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[RunResponse]</code> <p>See chassis.client.OMIClient.run for more information.</p>"},{"location":"reference/sdk/chassis/ftypes/types/","title":"types","text":""},{"location":"reference/sdk/chassis/ftypes/types/#chassis.ftypes.types.LegacyNormalPredictFunction","title":"LegacyNormalPredictFunction  <code>module-attribute</code>","text":"<pre><code>LegacyNormalPredictFunction = Callable[[bytes], Any]\n</code></pre>"},{"location":"reference/sdk/chassis/ftypes/types/#chassis.ftypes.types.LegacyBatchPredictFunction","title":"LegacyBatchPredictFunction  <code>module-attribute</code>","text":"<pre><code>LegacyBatchPredictFunction = Callable[[Sequence[bytes]], Sequence[Any]]\n</code></pre>"},{"location":"reference/sdk/chassis/ftypes/types/#chassis.ftypes.types.NormalPredictFunction","title":"NormalPredictFunction  <code>module-attribute</code>","text":"<pre><code>NormalPredictFunction = Callable[[Mapping[str, bytes]], Mapping[str, bytes]]\n</code></pre>"},{"location":"reference/sdk/chassis/ftypes/types/#chassis.ftypes.types.BatchPredictFunction","title":"BatchPredictFunction  <code>module-attribute</code>","text":"<pre><code>BatchPredictFunction = Callable[[Sequence[Mapping[str, bytes]]], Sequence[Mapping[str, bytes]]]\n</code></pre>"},{"location":"reference/sdk/chassis/ftypes/types/#chassis.ftypes.types.LegacyPredictFunction","title":"LegacyPredictFunction  <code>module-attribute</code>","text":"<pre><code>LegacyPredictFunction = Union[LegacyNormalPredictFunction, LegacyBatchPredictFunction]\n</code></pre>"},{"location":"reference/sdk/chassis/ftypes/types/#chassis.ftypes.types.PredictFunction","title":"PredictFunction  <code>module-attribute</code>","text":"<pre><code>PredictFunction = Union[LegacyPredictFunction, NormalPredictFunction, BatchPredictFunction]\n</code></pre>"},{"location":"reference/sdk/chassis/guides/quickstart/","title":"quickstart","text":""},{"location":"reference/sdk/chassis/guides/quickstart/#chassis.guides.quickstart.ROOT","title":"ROOT  <code>module-attribute</code>","text":"<pre><code>ROOT = os.path.abspath(os.path.dirname(__file__))\n</code></pre>"},{"location":"reference/sdk/chassis/guides/quickstart/#chassis.guides.quickstart.digits_clf","title":"digits_clf  <code>module-attribute</code>","text":"<pre><code>digits_clf = pickle.load(open(os.path.join(ROOT, 'data', 'logistic.pkl'), 'rb'))\n</code></pre>"},{"location":"reference/sdk/chassis/guides/quickstart/#chassis.guides.quickstart.digits_sample","title":"digits_sample  <code>module-attribute</code>","text":"<pre><code>digits_sample = os.path.join(ROOT, 'data', 'digits_sample.json')\n</code></pre>"},{"location":"reference/sdk/chassis/guides/quickstart/#chassis.guides.quickstart.chassis_model","title":"chassis_model  <code>module-attribute</code>","text":"<pre><code>chassis_model = ChassisModel(predict)\n</code></pre>"},{"location":"reference/sdk/chassis/guides/quickstart/#chassis.guides.quickstart.QuickstartDigitsClassifier","title":"QuickstartDigitsClassifier  <code>module-attribute</code>","text":"<pre><code>QuickstartDigitsClassifier = chassis_model\n</code></pre>"},{"location":"reference/sdk/chassis/guides/quickstart/#chassis.guides.quickstart.DigitsClassifier","title":"DigitsClassifier  <code>module-attribute</code>","text":"<pre><code>DigitsClassifier = open(os.path.join(ROOT, 'data', 'logistic.pkl'), 'rb')\n</code></pre>"},{"location":"reference/sdk/chassis/guides/quickstart/#chassis.guides.quickstart.DigitsSampleData","title":"DigitsSampleData  <code>module-attribute</code>","text":"<pre><code>DigitsSampleData = [{'input': open(digits_sample, 'rb').read()}]\n</code></pre>"},{"location":"reference/sdk/chassis/guides/quickstart/#chassis.guides.quickstart.predict","title":"predict","text":"<pre><code>predict(input_bytes)\n</code></pre>"},{"location":"reference/sdk/chassis/metadata/model_metadata/","title":"model_metadata","text":""},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata","title":"ModelMetadata","text":"<pre><code>ModelMetadata(info=None, description=None, inputs=None, outputs=None, resources=None, timeout=None, features=None)\n</code></pre> <p>This class provides an interface for customizing metadata embedded into the model container.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.model_name","title":"model_name  <code>property</code> <code>writable</code>","text":"<pre><code>model_name: str\n</code></pre> <p>The human-readable name of the model.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.model_version","title":"model_version  <code>property</code> <code>writable</code>","text":"<pre><code>model_version: str\n</code></pre> <p>The semantic-versioning compatible version of the model.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.model_author","title":"model_author  <code>property</code> <code>writable</code>","text":"<pre><code>model_author: str\n</code></pre> <p>The name and optional email of the author.</p> Example <p>John Smith john.smith@example.com</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.summary","title":"summary  <code>property</code> <code>writable</code>","text":"<pre><code>summary: str\n</code></pre> <p>A short summary of what the model does and how to use it.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.details","title":"details  <code>property</code> <code>writable</code>","text":"<pre><code>details: str\n</code></pre> <p>A longer description of the model that contains useful information that was unsuitable to put in the Summary.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.technical","title":"technical  <code>property</code> <code>writable</code>","text":"<pre><code>technical: str\n</code></pre> <p>Technical information about the model such as how it was trained, any known biases, the dataset that was used, etc.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.performance","title":"performance  <code>property</code> <code>writable</code>","text":"<pre><code>performance: str\n</code></pre> <p>Performance information about the model.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.required_ram","title":"required_ram  <code>property</code> <code>writable</code>","text":"<pre><code>required_ram: str\n</code></pre> <p>The amount of RAM required to run the model. This string can be any value accepted by Docker or Kubernetes.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.num_cpus","title":"num_cpus  <code>property</code> <code>writable</code>","text":"<pre><code>num_cpus: float\n</code></pre> <p>The number of fractional CPU cores required to run the model.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.num_gpus","title":"num_gpus  <code>property</code> <code>writable</code>","text":"<pre><code>num_gpus: int\n</code></pre> <p>The number of GPUs required to run the model.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.status_timeout","title":"status_timeout  <code>property</code> <code>writable</code>","text":"<pre><code>status_timeout: str\n</code></pre> <p>The amount of time after which a model should be considered to have failed initializing itself.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.run_timeout","title":"run_timeout  <code>property</code> <code>writable</code>","text":"<pre><code>run_timeout: str\n</code></pre> <p>The amount of time after which an inference should be considered to have failed.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.batch_size","title":"batch_size  <code>property</code> <code>writable</code>","text":"<pre><code>batch_size: int\n</code></pre> <p>The batch size supported by this model. For models that don't support batch, set this to 1.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.has_inputs","title":"has_inputs","text":"<pre><code>has_inputs()\n</code></pre> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if at least one input has been defined.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.add_input","title":"add_input","text":"<pre><code>add_input(key, accepted_media_types=None, max_size='1M', description='')\n</code></pre> <p>Defines an input to the model. Inputs are identified by a string <code>key</code> that will be used to retrieve them from the dictionary of inputs during inference.</p> <p>Since all input values are sent as <code>bytes</code>, each input should define one or more MIME types that are suitable for decoding the bytes into a usable object.</p> <p>Additionally, each input can be set to have a maximum size to easily reject requests with inputs that are too large.</p> <p>Finally, you can give each input a description which can be used in documentation to explain any further details about the input requirements, such as indicating whether color channels need to be stripped from the image, etc.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key name to represent the input. E.g., \"input\", \"image\", etc.</p> required <code>accepted_media_types</code> <code>Optional[List[str]]</code> <p>Acceptable mime type(s) for the respective input. For more information on common mime types, visit https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Common_types</p> <code>None</code> <code>max_size</code> <code>str</code> <p>Maximum acceptable size of input. This value should include an integer followed by a letter indicating the unit of measure (e.g., \"3M\" = 3 MB, \"1.5G\" = 1.5 GB, etc.)</p> <code>'1M'</code> <code>description</code> <code>str</code> <p>Short description of the input</p> <code>''</code> <p>Example: <pre><code>from chassisml import ChassisModel\nmodel = ChassisModel(process_fn=predict)\nmodel.metadata.add_input(\n\"image\",\n[\"image/png\", \"image/jpeg\"],\n\"10M\",\n\"Image to be classified by computer vision model\"\n)\n</code></pre></p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.has_outputs","title":"has_outputs","text":"<pre><code>has_outputs()\n</code></pre> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if at least one output has been defined.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.add_output","title":"add_output","text":"<pre><code>add_output(key, media_type='application/octet-stream', max_size='1M', description='')\n</code></pre> <p>Defines an output from the model. Outputs are identified by a string <code>key</code> that will be used to retrieve them from the dictionary of outputs received after inference.</p> <p>Since all output values are sent as <code>bytes</code>, each output should define the MIME type that is suitable for decoding the bytes into a usable object.</p> <p>Additionally, each output should be set to have a maximum size to prevent results that are too large for practical use.</p> <p>Finally, you can give each output a description which can be used in documentation to explain any further details about the output.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key name to represent the output. E.g., \"results.json\", \"results\", \"output\", etc.</p> required <code>media_type</code> <code>str</code> <p>Acceptable mime type for the respective output. For more information on common mime types, visit https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Common_types</p> <code>'application/octet-stream'</code> <code>max_size</code> <code>str</code> <p>Maximum acceptable size of output. This value should include an integer followed by a letter indicating the unit of measure (e.g., \"3M\" = 3 MB, \"1.5G\" = 1.5 GB, etc.)</p> <code>'1M'</code> <code>description</code> <code>str</code> <p>Short description of the output</p> <code>''</code> <p>Example: <pre><code>from chassisml import ChassisModel\nmodel = ChassisModel(process_fn=predict)\nmodel.metadata.add_input(\n\"results.json\",\n[\"application/json\"],\n\"1M,\n\"Classification results of computer vision model with class name and confidence score in JSON format\"\n)\n</code></pre></p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.serialize","title":"serialize","text":"<pre><code>serialize()\n</code></pre> <p>For internal use only.</p> <p>This method will take the values of this object and serialize them in the protobuf message that the final container expects to receive.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>The serialized protobuf object.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.default","title":"default  <code>classmethod</code>","text":"<pre><code>default()\n</code></pre> <p>A ModelMetadata object that corresponds to the defaults used by Chassis v1.5+.</p> <p>The defaults are blank values for all properties. You are responsible for setting any appropriate values for your model.</p> Note <p>It is always required to set the <code>model_name</code>, <code>model_version</code> fields and to add at least one input and one output.</p> <p>Returns:</p> Type Description <code>ModelMetadata</code> <p>An empty <code>ModelMetadata</code> object.</p>"},{"location":"reference/sdk/chassis/metadata/model_metadata/#chassis.metadata.model_metadata.ModelMetadata.legacy","title":"legacy  <code>classmethod</code>","text":"<pre><code>legacy()\n</code></pre> <p>A ModelMetadata object that corresponds to the values used before Chassis v1.5.</p> <p>Returns:</p> Type Description <code>ModelMetadata</code> <p>A partially filled <code>ModelMetadata</code> object.</p>"},{"location":"reference/sdk/chassis/runtime/constants/","title":"constants","text":""},{"location":"reference/sdk/chassis/runtime/constants/#chassis.runtime.constants.PACKAGE_DATA_PATH","title":"PACKAGE_DATA_PATH  <code>module-attribute</code>","text":"<pre><code>PACKAGE_DATA_PATH = 'data'\n</code></pre>"},{"location":"reference/sdk/chassis/runtime/constants/#chassis.runtime.constants.PYTHON_MODEL_KEY","title":"PYTHON_MODEL_KEY  <code>module-attribute</code>","text":"<pre><code>PYTHON_MODEL_KEY = '__chassis_model'\n</code></pre>"},{"location":"reference/sdk/chassis/runtime/constants/#chassis.runtime.constants.python_pickle_filename_for_key","title":"python_pickle_filename_for_key","text":"<pre><code>python_pickle_filename_for_key(key)\n</code></pre> <p>Helper function for serializing and deserializing Python functions.</p> <p>When used in the SDK, it returns the name of the file that should be written out in the build context based on what kind of function it is.</p> <p>When used inside the built container, it does exactly the opposite, it returns the name of the file that it should load to hydrate the function.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the keys defined in chassis.runtime.constants.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The appropriate filename for the given <code>key</code>.</p>"},{"location":"reference/sdk/chassis/runtime/model_runner/","title":"model_runner","text":""},{"location":"reference/sdk/chassis/runtime/model_runner/#chassis.runtime.model_runner.ModelRunner","title":"ModelRunner","text":"<pre><code>ModelRunner(predict_fn, batch_size=1, is_legacy_fn=False)\n</code></pre> <p>This class abstracts all the potentially different <code>predict</code> method signatures into a single API that can be used by the model servers.</p> <p>When initializing the model, pass in a Python function that adheres to any of the defined signatures indicated by chassis.ftypes.PredictFunction type alias. If your model supports batch predictions, set the <code>batch_size</code> to the number of inputs that your model can process at once.</p> <p>Parameters:</p> Name Type Description Default <code>predict_fn</code> <code>PredictFunction</code> <p>Single predict function of type <code>PredictFunction</code> that represents a model inference function.</p> required <code>batch_size</code> <code>int</code> <p>Integer representing the batch size your model supports. If your model does not support batching, the default value is 1</p> <code>1</code> <code>is_legacy_fn</code> <code>bool</code> <p>If <code>True</code>, predict_fn follows legacy format (not typed, only single input and output supported, returns dictionary)</p> <code>False</code>"},{"location":"reference/sdk/chassis/runtime/model_runner/#chassis.runtime.model_runner.ModelRunner.predict_fn","title":"predict_fn  <code>instance-attribute</code>","text":"<pre><code>predict_fn = predict_fn\n</code></pre>"},{"location":"reference/sdk/chassis/runtime/model_runner/#chassis.runtime.model_runner.ModelRunner.supports_batch","title":"supports_batch  <code>instance-attribute</code>","text":"<pre><code>supports_batch = batch_size &gt; 1\n</code></pre>"},{"location":"reference/sdk/chassis/runtime/model_runner/#chassis.runtime.model_runner.ModelRunner.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"reference/sdk/chassis/runtime/model_runner/#chassis.runtime.model_runner.ModelRunner.legacy","title":"legacy  <code>instance-attribute</code>","text":"<pre><code>legacy = is_legacy_fn\n</code></pre>"},{"location":"reference/sdk/chassis/runtime/model_runner/#chassis.runtime.model_runner.ModelRunner.load","title":"load  <code>classmethod</code>","text":"<pre><code>load()\n</code></pre> <p>Convenience function used by model servers to load a cloudpickle'd model in the model container.</p>"},{"location":"reference/sdk/chassis/runtime/model_runner/#chassis.runtime.model_runner.ModelRunner.predict","title":"predict","text":"<pre><code>predict(inputs)\n</code></pre> <p>Performs an inference against the model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Sequence[Mapping[str, bytes]]</code> <p>Mapping of input name (str) to input data (bytes) which the predict function is expected to process for inference.</p> required <p>Returns:</p> Type Description <code>Sequence[Mapping[str, bytes]]</code> <p>List of outputs the <code>predict_fn</code> returns</p>"},{"location":"reference/sdk/chassis/runtime/model_runner/#chassis.runtime.model_runner.batch","title":"batch","text":"<pre><code>batch(items, size)\n</code></pre> <p>Yields lists of size <code>size</code> until all items have been exhausted.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Sequence</code> <p>The batch of inputs to perform inference on.</p> required <code>size</code> <code>int</code> <p>The batch size that the model supports.</p> required"},{"location":"reference/sdk/chassis/runtime/numpy_encoder/","title":"numpy_encoder","text":""},{"location":"reference/sdk/chassis/runtime/numpy_encoder/#chassis.runtime.numpy_encoder.NumpyEncoder","title":"NumpyEncoder","text":""},{"location":"reference/sdk/chassis/runtime/numpy_encoder/#chassis.runtime.numpy_encoder.NumpyEncoder.default","title":"default","text":"<pre><code>default(obj)\n</code></pre>"},{"location":"reference/sdk/chassis/scripts/normalize_requirements/","title":"normalize_requirements","text":""},{"location":"reference/sdk/chassis/scripts/normalize_requirements/#chassis.scripts.normalize_requirements.REQUIREMENTS_SUBSTITUTIONS","title":"REQUIREMENTS_SUBSTITUTIONS  <code>module-attribute</code>","text":"<pre><code>REQUIREMENTS_SUBSTITUTIONS = {'opencv-python=': 'opencv-python-headless='}\n</code></pre>"},{"location":"reference/sdk/chassis/scripts/normalize_requirements/#chassis.scripts.normalize_requirements.requirements_txt","title":"requirements_txt  <code>module-attribute</code>","text":"<pre><code>requirements_txt = 'requirements.txt'\n</code></pre>"},{"location":"reference/sdk/chassis/scripts/normalize_requirements/#chassis.scripts.normalize_requirements.reqs","title":"reqs  <code>module-attribute</code>","text":"<pre><code>reqs = reqs.replace(old, new)\n</code></pre>"},{"location":"reference/sdk/chassis/server/kserve/server/","title":"server","text":""},{"location":"reference/sdk/chassis/server/kserve/server/#chassis.server.kserve.server.KServe","title":"KServe","text":"<pre><code>KServe(name, protocol)\n</code></pre>"},{"location":"reference/sdk/chassis/server/kserve/server/#chassis.server.kserve.server.KServe.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"reference/sdk/chassis/server/kserve/server/#chassis.server.kserve.server.KServe.protocol","title":"protocol  <code>instance-attribute</code>","text":"<pre><code>protocol = protocol\n</code></pre>"},{"location":"reference/sdk/chassis/server/kserve/server/#chassis.server.kserve.server.KServe.ready","title":"ready  <code>instance-attribute</code>","text":"<pre><code>ready = False\n</code></pre>"},{"location":"reference/sdk/chassis/server/kserve/server/#chassis.server.kserve.server.KServe.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Optional[ModelRunner] = None\n</code></pre>"},{"location":"reference/sdk/chassis/server/kserve/server/#chassis.server.kserve.server.KServe.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: StatusResponse = sr\n</code></pre>"},{"location":"reference/sdk/chassis/server/kserve/server/#chassis.server.kserve.server.KServe.load","title":"load","text":"<pre><code>load()\n</code></pre>"},{"location":"reference/sdk/chassis/server/kserve/server/#chassis.server.kserve.server.KServe.predict","title":"predict","text":"<pre><code>predict(payload, headers=None)\n</code></pre>"},{"location":"reference/sdk/chassis/server/kserve/server/#chassis.server.kserve.server.serve","title":"serve","text":"<pre><code>serve()\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/","title":"server","text":""},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.GRPC_SERVER_PORT","title":"GRPC_SERVER_PORT  <code>module-attribute</code>","text":"<pre><code>GRPC_SERVER_PORT = 45000\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.LOGGER","title":"LOGGER  <code>module-attribute</code>","text":"<pre><code>LOGGER = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.ModzyModel","title":"ModzyModel","text":"<pre><code>ModzyModel()\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.ModzyModel.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Optional[ModelRunner] = None\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.ModzyModel.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata = sr\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.ModzyModel.Status","title":"Status  <code>async</code>","text":"<pre><code>Status(stream)\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.ModzyModel.Run","title":"Run  <code>async</code>","text":"<pre><code>Run(stream)\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.ModzyModel.Shutdown","title":"Shutdown  <code>async</code>","text":"<pre><code>Shutdown(stream)\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.log_stack_trace","title":"log_stack_trace","text":"<pre><code>log_stack_trace()\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.create_output_item","title":"create_output_item","text":"<pre><code>create_output_item(message, data=None)\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.get_server_port","title":"get_server_port","text":"<pre><code>get_server_port()\n</code></pre>"},{"location":"reference/sdk/chassis/server/omi/server/#chassis.server.omi.server.serve","title":"serve  <code>async</code>","text":"<pre><code>serve()\n</code></pre>"},{"location":"reference/sdk/chassisml/v1/chassis_client/","title":"chassis_client","text":""},{"location":"reference/sdk/chassisml/v1/chassis_client/#chassisml.v1.chassis_client.ChassisClient","title":"ChassisClient","text":"<pre><code>ChassisClient(base_url='http://localhost:5000', auth_header=None, ssl_verification=True)\n</code></pre> <p>DEPRECATED</p> <p>Please use chassis.builder.RemoteBuilder moving forward.</p> <p>The Chassis Client object.</p> <p>This class is used to interact with the Chassis remote build service.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>The base url for the API.</p> <code>'http://localhost:5000'</code> <code>auth_header</code> <code>Optional[str]</code> <p>Optional authorization header to be included with all requests.</p> <code>None</code> <code>ssl_verification</code> <code>bool</code> <p>Verify TLS connections.</p> <code>True</code>"},{"location":"reference/sdk/chassisml/v1/chassis_client/#chassisml.v1.chassis_client.ChassisClient.get_job_status","title":"get_job_status","text":"<pre><code>get_job_status(job_id)\n</code></pre> <p>DEPRECATED</p> <p>Please use chassis.builder.RemoteBuilder.get_build_status moving forward.</p> <p>Checks the status of a chassis job</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Chassis job identifier generated from ChassisModel.publish method.</p> required <p>Returns:</p> Type Description <code>BuildResponse</code> <p>Chassis job status.</p> <p>Example: <pre><code># Create Chassisml model\nchassis_model = chassis_client.create_model(process_fn=process)\n# Define Dockerhub credentials\ndockerhub_user = \"user\"\ndockerhub_pass = \"password\"\n# Publish model to Docker registry\nresponse = chassis_model.publish(\nmodel_name=\"Chassisml Regression Model\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)\njob_id = response.get('job_id')\njob_status = chassis_client.get_job_status(job_id)\n</code></pre></p>"},{"location":"reference/sdk/chassisml/v1/chassis_client/#chassisml.v1.chassis_client.ChassisClient.get_job_logs","title":"get_job_logs","text":"<pre><code>get_job_logs(job_id)\n</code></pre> <p>DEPRECATED</p> <p>Please use chassis.builder.RemoteBuilder.get_build_logs moving forward.</p> <p>Checks the status of a chassis job</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Chassis job identifier generated from ChassisModel.publish method.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The job logs.</p> <p>Example: <pre><code># Create Chassisml model\nchassis_model = chassis_client.create_model(process_fn=process)\n# Define Dockerhub credentials\ndockerhub_user = \"user\"\ndockerhub_pass = \"password\"\n# Publish model to Docker registry\nresponse = chassis_model.publish(\nmodel_name=\"Chassisml Regression Model\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)\njob_id = response.get('job_id')\njob_status = chassis_client.get_job_logs(job_id)\n</code></pre></p>"},{"location":"reference/sdk/chassisml/v1/chassis_client/#chassisml.v1.chassis_client.ChassisClient.block_until_complete","title":"block_until_complete","text":"<pre><code>block_until_complete(job_id, timeout=None, poll_interval=5)\n</code></pre> <p>DEPRECATED</p> <p>Please use chassis.builder.RemoteBuilder.block_until_complete moving forward.</p> <p>Blocks until Chassis job is complete or timeout is reached. Polls Chassis job API until a result is marked finished.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Chassis job identifier generated from ChassisModel.publish method.</p> required <code>timeout</code> <code>Optional[int]</code> <p>Timeout threshold in seconds.</p> <code>None</code> <code>poll_interval</code> <code>int</code> <p>Amount of time to wait in between API polls to check status of job.</p> <code>5</code> <p>Returns:</p> Type Description <code>BuildResponse</code> <p>Final job status.</p> <p>Example: <pre><code># Create Chassisml model\nchassis_model = chassis_client.create_model(process_fn=process)\n# Define Dockerhub credentials\ndockerhub_user = \"user\"\ndockerhub_pass = \"password\"\n# Publish model to Docker registry\nresponse = chassis_model.publish(\nmodel_name=\"Chassisml Regression Model\",\nmodel_version=\"0.0.1\",\nregistry_user=dockerhub_user,\nregistry_pass=dockerhub_pass,\n)\njob_id = response.get('job_id')\nfinal_status = chassis_client.block_until_complete(job_id)\n</code></pre></p>"},{"location":"reference/sdk/chassisml/v1/chassis_client/#chassisml.v1.chassis_client.ChassisClient.download_tar","title":"download_tar","text":"<pre><code>download_tar(job_id, output_filename)\n</code></pre> <p>This method is no longer available.</p>"},{"location":"reference/sdk/chassisml/v1/chassis_client/#chassisml.v1.chassis_client.ChassisClient.create_model","title":"create_model","text":"<pre><code>create_model(process_fn=None, batch_process_fn=None, batch_size=None)\n</code></pre> <p>DEPRECATED</p> <p>Please use chassisml.ChassisModel moving forward.</p> <p>Builds chassis model locally</p> <p>Parameters:</p> Name Type Description Default <code>process_fn</code> <code>Optional[LegacyNormalPredictFunction]</code> <p>Python function that must accept a single piece of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the <code>process</code> method</p> <code>None</code> <code>batch_process_fn</code> <code>Optional[LegacyBatchPredictFunction]</code> <p>Python function that must accept a batch of input data in raw bytes form. This method is responsible for handling all data preprocessing, executing inference, and returning the processed predictions. Defining additional functions is acceptable as long as they are called within the <code>process</code> method</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>Maximum batch size if <code>batch_process_fn</code> is defined</p> <code>None</code> <p>Returns:</p> Type Description <code>ChassisModel</code> <p>Chassis Model object that can be tested locally and published to a</p> <code>ChassisModel</code> <p>Docker Registry.</p> <p>Examples:</p> <p>The following snippet was taken from this example. <pre><code># Import and normalize data\nX_digits, y_digits = datasets.load_digits(return_X_y=True)\nX_digits = X_digits / X_digits.max()\nn_samples = len(X_digits)\n# Split data into training and test sets\nX_train = X_digits[: int(0.9 * n_samples)]\ny_train = y_digits[: int(0.9 * n_samples)]\nX_test = X_digits[int(0.9 * n_samples) :]\ny_test = y_digits[int(0.9 * n_samples) :]\n# Train Model\nlogistic = LogisticRegression(max_iter=1000)\nprint(\n\"LogisticRegression mean accuracy score: %f\"\n% logistic.fit(X_train, y_train).score(X_test, y_test)\n)\n# Save small sample input to use for testing later\nsample = X_test[:5].tolist()\nwith open(\"digits_sample.json\", 'w') as out:\njson.dump(sample, out)\n# Define Process function\ndef process(input_bytes):\ninputs = np.array(json.loads(input_bytes))\ninference_results = logistic.predict(inputs)\nstructured_results = []\nfor inference_result in inference_results:\nstructured_output = {\n\"data\": {\n\"result\": {\n\"classPredictions\": [\n{\n\"class\": str(inference_result),\n\"score\": str(1)\n}\n]\n}\n}\n}\nstructured_results.append(structured_output)\nreturn structured_results\n# create Chassis model\nchassis_model = chassis_client.create_model(process_fn=process)\n</code></pre></p>"},{"location":"reference/sdk/chassisml/v1/chassis_client/#chassisml.v1.chassis_client.ChassisClient.run_inference","title":"run_inference","text":"<pre><code>run_inference(input_data, container_url='localhost', host_port=45000)\n</code></pre> <p>No Longer Available</p> <p>Please use chassis.client.OMIClient.run moving forward.</p> <p>This is the method you use to submit data to a container chassis has built for inference. It assumes the container has been downloaded from dockerhub and is running somewhere you have access to.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Dict[str, bytes]</code> <p>dictionary of the form {\"input\": b\"binary representation of your data\"}.</p> required <code>container_url</code> <code>str</code> <p>URL where container is running.</p> <code>'localhost'</code> <code>host_port</code> <code>int</code> <p>host port that forwards to container's grpc server port</p> <code>45000</code> <p>Returns:</p> Type Description <code>Iterable[OutputItem]</code> <p>Success -&gt; results from model processing as specified in the        process function.</p> <code>Iterable[OutputItem]</code> <p>Failure -&gt; Error codes from processing errors. All errors should        container the word \"Error.\"</p> <p>Example: <pre><code># assume that the container is running locally, and that it was started with this docker command\n#  docker run -it -p 5001:45000 &lt;docker_uname&gt;/&lt;container_name&gt;:&lt;tag_id&gt;\nfrom chassisml_sdk.chassisml import chassisml\nclient = chassisml.ChassisClient(\"https://chassis.app.modzy.com/\")\ninput_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"}\ninput_list = [input_data for _ in range(30)]\nprint(\"single input\")\nprint(client.run_inference(input_data, container_url=\"localhost\", host_port=5001))\nprint(\"multi inputs\")\nresults = client.run_inference(input_list, container_url=\"localhost\", host_port=5001)\nfor x in results:\nprint(x)\n</code></pre></p>"},{"location":"reference/sdk/chassisml/v1/chassis_client/#chassisml.v1.chassis_client.ChassisClient.docker_infer","title":"docker_infer","text":"<pre><code>docker_infer(image_id, input_data, container_url='localhost', host_port=5001, container_port=None, timeout=20, clean_up=True, pull_container=False)\n</code></pre> <p>No Longer Available</p> <p>Please use chassis.client.OMIClient.test_container moving forward.</p> <p>Runs inference on an OMI compliant container. This method checks to see if a container is running and if not starts it. The method then runs inference against the input_data with the model in the container, and optionally shuts down the container.</p> <p>Parameters:</p> Name Type Description Default <code>image_id</code> <code>str</code> <p>the name of an OMI container image usually of the form /: required <code>input_data</code> <code>Mapping[str, bytes]</code> <p>dictionary of the form</p> required <code>container_url</code> <code>str</code> <p>No longer used.</p> <code>'localhost'</code> <code>host_port</code> <code>int</code> <p>host port that forwards to container's grpc server port</p> <code>5001</code> <code>container_port</code> <code>Optional[str]</code> <p>No longer used.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>number of seconds to wait for gRPC server to spin up</p> <code>20</code> <code>clean_up</code> <code>bool</code> <p>No longer used.</p> <code>True</code> <code>pull_container</code> <code>bool</code> <p>if True pulls missing container from repo</p> <code>False</code> <p>Returns:</p> Type Description <code>Iterable[OutputItem]</code> <p>Success -&gt; model output as defined in the process function</p> <code>Iterable[OutputItem]</code> <p>Failure -&gt; Error message if any success criteria is missing.</p> <p>Example: <pre><code>host_port = 5002\nclient = chassisml.ChassisClient()\ninput_data = {\"input\": b\"[[0.0, 0.0, 0.0, 1.0, 12.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 6.0, 1.0, 2.0, 0.0, 0.0, 4.0, 16.0, 9.0, 1.0, 15.0, 9.0, 0.0, 0.0, 13.0, 15.0, 6.0, 10.0, 16.0, 6.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 16.0, 1.0, 0.0, 0.0, 1.0, 7.0, 4.0, 14.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 9.0, 0.0, 0.0], [0.0, 0.0, 8.0, 16.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 14.0, 5.0, 14.0, 12.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 3.0, 16.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 16.0, 11.0, 16.0, 4.0, 0.0, 0.0, 0.0, 3.0, 16.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 10.0, 1.0, 0.0, 0.0], [0.0, 0.0, 5.0, 12.0, 8.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 5.0, 13.0, 6.0, 0.0, 0.0, 0.0, 2.0, 15.0, 16.0, 12.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 1.0, 15.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 8.0, 16.0, 16.0, 11.0, 0.0, 0.0, 0.0, 0.0, 11.0, 16.0, 16.0, 9.0, 0.0, 0.0, 0.0, 0.0, 6.0, 12.0, 12.0, 3.0, 0.0, 0.0], [0.0, 0.0, 0.0, 3.0, 15.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 16.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 3.0, 4.0, 3.0, 0.0, 0.0, 7.0, 16.0, 5.0, 3.0, 15.0, 8.0, 0.0, 0.0, 13.0, 16.0, 13.0, 15.0, 16.0, 2.0, 0.0, 0.0, 12.0, 16.0, 16.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 4.0, 5.0, 16.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 16.0, 4.0, 0.0, 0.0], [0.0, 0.0, 10.0, 14.0, 8.0, 1.0, 0.0, 0.0, 0.0, 2.0, 16.0, 14.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 8.0, 15.0, 0.0, 0.0, 0.0, 0.0, 5.0, 16.0, 16.0, 10.0, 0.0, 0.0, 0.0, 0.0, 12.0, 15.0, 15.0, 12.0, 0.0, 0.0, 0.0, 4.0, 16.0, 6.0, 4.0, 16.0, 6.0, 0.0, 0.0, 8.0, 16.0, 10.0, 8.0, 16.0, 8.0, 0.0, 0.0, 1.0, 8.0, 12.0, 14.0, 12.0, 1.0, 0.0]]\"}\ninput_list = [input_data for _ in range(30)]\nprint(\"single input\")\nprint(client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_data, container_url=\"localhost\", host_port=host_port, clean_up=False, pull_container=True))\nprint(\"multi inputs\")\nresults = client.run_inference(input_list, container_url=\"localhost\", host_port=host_port)\nresults = client.docker_infer(image_id=\"claytondavisms/sklearn-digits-docker-test:0.0.7\", input_data=input_list, container_url=\"localhost\", host_port=host_port)\nfor x in results:\nprint(x)\n</code></pre></p>"},{"location":"reference/sdk/chassisml/v1/chassis_model/","title":"chassis_model","text":""},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.DEFAULT_CUDA_VERSION","title":"DEFAULT_CUDA_VERSION  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CUDA_VERSION = '11.0.3'\n</code></pre>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel","title":"ChassisModel","text":"<pre><code>ChassisModel(process_fn, batch_size=1, legacy_predict_fn=False, chassis_client=None)\n</code></pre> <p>The Chassis Model Object.</p> <p>This class inherits from chassis.builder.Buildable and is the main object that gets fed into a Chassis builder object (i.e., chassis.builder.DockerBuilder or chassis.builder.RemoteBuilder)</p> <p>Parameters:</p> Name Type Description Default <code>process_fn</code> <code>PredictFunction</code> <p>Single predict function of type <code>PredictFunction</code> that represents a model inference function.</p> required <code>batch_size</code> <code>int</code> <p>Integer representing the batch size your model supports. If your model does not support batching, the default value is 1.</p> <code>1</code> <code>legacy_predict_fn</code> <code>bool</code> <p>For internal backwards-compatibility use only.</p> <code>False</code> <code>chassis_client</code> <p>For internal backwards-compatibility use only.</p> <code>None</code>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.runner","title":"runner  <code>instance-attribute</code>","text":"<pre><code>runner = ModelRunner(process_fn, batch_size=batch_size, is_legacy_fn=legacy_predict_fn)\n</code></pre>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata = ModelMetadata.legacy()\n</code></pre>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.chassis_client","title":"chassis_client  <code>instance-attribute</code>","text":"<pre><code>chassis_client = chassis_client\n</code></pre>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.packaged","title":"packaged  <code>instance-attribute</code>","text":"<pre><code>packaged = False\n</code></pre>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.requirements","title":"requirements  <code>instance-attribute</code>","text":"<pre><code>requirements: set[str] = set()\n</code></pre>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.apt_packages","title":"apt_packages  <code>instance-attribute</code>","text":"<pre><code>apt_packages: set[str] = set()\n</code></pre>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.additional_files","title":"additional_files  <code>instance-attribute</code>","text":"<pre><code>additional_files: set[str] = set()\n</code></pre>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.python_modules","title":"python_modules  <code>instance-attribute</code>","text":"<pre><code>python_modules: dict = {}\n</code></pre>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.test","title":"test","text":"<pre><code>test(test_input)\n</code></pre> <p>Runs a test inference against the model before it is packaged.</p> <p>This method supports multiple input types</p> <ul> <li>Single input: A map-like object with a string for the key and     bytes as the value.</li> <li>Batch input: A list of map-like objects with strings for keys and     bytes for values.</li> </ul> <p>The following input types are also supported but considered deprecated     and may be removed in a future release</p> <ul> <li>File: A BufferedReader object. Use of this type assumes that your     predict function expects the input key to be \"input\".</li> <li>bytes: Any arbitrary bytes. Use of this type assumes that your     predict function expects the input key to be \"input\".</li> <li>str: A string. If the string maps to a filesystem location, then     the file at that location will be read and used as the value.     If not the string itself is used as the value. Use of this type     assumes that your predict function expects the input key to     be \"input\".</li> </ul> <p>Parameters:</p> Name Type Description Default <code>test_input</code> <code>Union[str, bytes, _io.BufferedReader, Mapping[str, bytes], Sequence[Mapping[str, bytes]]]</code> <p>Sample input data used to test the model. See above for more information.</p> required <p>Returns:</p> Type Description <code>Sequence[Mapping[str, bytes]]</code> <p>Results returned by your model's predict function based on the <code>test_input</code> sample data fed to this function.</p> <p>Example: <pre><code>from chassisml import ChassisModel\nchassis_model = ChassisModel(process_fn=predict)\nresults = chassis_model.test(sample_data)\n</code></pre></p>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.merge_package","title":"merge_package","text":"<pre><code>merge_package(package)\n</code></pre> <p>Allows for merging two Buildable objects. This will ensure that any pip requirements, apt packages, files, or modules are merged.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Buildable</code> <p>Another <code>Buildable</code> object to merge into this one.</p> required"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.add_requirements","title":"add_requirements","text":"<pre><code>add_requirements(reqs)\n</code></pre> <p>Declare a pip requirement for your model.</p> <p>The value of each requirement can be anything supported by a line in a <code>requirements.txt</code> file, including version constraints, etc.</p> <p>All pip requirements declared via this method will be automatically installed when the container is built.</p> <p>Parameters:</p> Name Type Description Default <code>reqs</code> <code>Union[str, list[str]]</code> <p>Single python package (str) or list of python packages that   are required dependencies to run the <code>ChassisModel.process_fn</code>   attribute. These values are the same values that would follow   <code>pip install</code> or that would be added to a Python dependencies   txt file (e.g., <code>requirements.txt</code>)</p> required"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.test_batch","title":"test_batch","text":"<pre><code>test_batch(test_input)\n</code></pre> <p>DEPRECATED</p> <p>The chassisml.ChassisModel.test method now supports supplying batches of inputs.</p>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.add_apt_packages","title":"add_apt_packages","text":"<pre><code>add_apt_packages(packages)\n</code></pre> <p>Add an OS package that will be installed via <code>apt-get</code>.</p> <p>If your model requires additional OS packages that are not part of the standard Python container, you can declare them here. Each package declared here will be <code>apt-get install</code>'d when the container is built.</p> <p>Parameters:</p> Name Type Description Default <code>packages</code> <code>Union[str, list]</code> <p>Single OS-level package (str) or list of OS-level packages       that are required dependencies to run the       <code>ChassisModel.process_fn</code> attribute. These values are the       same values that can be installed via <code>apt-get install</code>.</p> required"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.get_packaged_path","title":"get_packaged_path","text":"<pre><code>get_packaged_path(path)\n</code></pre> <p>Convenience method for developers wanting to implement their own subclasses of Buildable. This method will return the final path in the built container of any additional files, etc.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local path of a file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path the file will have in the final built container.</p>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.test_env","title":"test_env","text":"<pre><code>test_env(test_input_path, conda_env=None, fix_env=True)\n</code></pre> <p>No Longer Available</p> <p>Please use chassis.client.OMIClient.test_container moving forward.</p>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.save","title":"save","text":"<pre><code>save(path=None, requirements=None, overwrite=False, fix_env=False, gpu=False, arm64=False, conda_env=None)\n</code></pre> <p>DEPRECATED</p> <p>Please use chassisml.ChassisModel.prepare_context moving forward.</p> <p>Saves a copy of ChassisModel to local filepath</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Filepath to save chassis model.</p> <code>None</code> <code>requirements</code> <code>Optional[Union[str, List[str]]]</code> <p>Additional pip requirements needed by the model.</p> <code>None</code> <code>conda_env</code> <code>Optional[dict]</code> <p>A dictionary with environment requirements.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>No longer used.</p> <code>False</code> <code>fix_env</code> <code>bool</code> <p>No longer used.</p> <code>False</code> <code>gpu</code> <code>bool</code> <p>If True and <code>arm64</code> is True, modifies dependencies as needed by chassis for ARM64+GPU support</p> <code>False</code> <code>arm64</code> <code>bool</code> <p>If True and <code>gpu</code> is True, modifies dependencies as needed by chassis for ARM64+GPU support</p> <code>False</code> <p>Returns:</p> Type Description <code>BuildContext</code> <p>The <code>BuildContext</code> object that allows for further actions to be taken.</p> <p>Example: <pre><code>chassis_model = ChassisModel(process_fn=process)\ncontext = chassis_model.save(\"local_model_directory\")\n</code></pre></p>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.verify_prerequisites","title":"verify_prerequisites","text":"<pre><code>verify_prerequisites(options)\n</code></pre> <p>Raises an exception if the object is not yet ready for building.</p> <p>Models require having a name, version, and at least one input and one output.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>BuildOptions</code> <p>The <code>BuildOptions</code> used for the build.</p> required"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.prepare_context","title":"prepare_context","text":"<pre><code>prepare_context(options=DefaultBuildOptions)\n</code></pre> <p>Constructs the build context that will be used to build the container.</p> <p>A build context is a directory containing a <code>Dockerfile</code> and any other resources the <code>Dockerfile</code> needs to build the container.</p> <p>This method is called just before the build is initiated and compiles all the resources necessary to build the container. This includes the <code>Dockerfile</code>, required Chassis library code, the server implementation indicated by the <code>BuildOptions</code>, the cloudpickle'd model, the serialized model metadata, copies of any additional files, and a <code>requirements.txt</code>.</p> <p>Typically, you won't call this method directly, it will be called automatically by a Builder. The one instance where you might want to use this method directly is if you want to inspect the contents of the build context before sending it to a Builder.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>BuildOptions</code> <p>The <code>BuildOptions</code> to be used for this build.</p> <code>DefaultBuildOptions</code> <p>Returns:</p> Type Description <code>BuildContext</code> <p>A <code>BuildContext</code> object.</p>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.publish","title":"publish","text":"<pre><code>publish(model_name, model_version, registry_user=None, registry_pass=None, requirements=None, fix_env=True, gpu=False, arm64=False, sample_input_path=None, webhook=None, conda_env=None)\n</code></pre> <p>DEPRECATED</p> <p>Please use chassis.builder.RemoteBuilder moving forward.</p> <p>Builds the model locally using Docker.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model name that serves as model's name and docker registry repository name.</p> required <code>model_version</code> <code>str</code> <p>Version of model</p> required <code>registry_user</code> <code>Optional[str]</code> <p>Docker registry username</p> <code>None</code> <code>registry_pass</code> <code>Optional[str]</code> <p>Docker registry password</p> <code>None</code> <code>requirements</code> <code>Optional[Union[str, list[str]]]</code> <p>Additional pip requirements needed by the model.</p> <code>None</code> <code>conda_env</code> <code>Optional[dict]</code> <p>A dictionary with environment requirements.</p> <code>None</code> <code>fix_env</code> <code>bool</code> <p>No longer used.</p> <code>True</code> <code>gpu</code> <code>bool</code> <p>If True, builds container image that runs on GPU hardware</p> <code>False</code> <code>arm64</code> <code>bool</code> <p>If True, builds container image that runs on ARM64 architecture</p> <code>False</code> <code>sample_input_path</code> <code>Optional[str]</code> <p>No longer used.</p> <code>None</code> <code>webhook</code> <code>Optional[str]</code> <p>No longer used.</p> <code>None</code> <p>Returns:</p> Type Description <p>Details about the result of the build.</p> <p>Example: <pre><code># Create Chassisml model\nchassis_model = ChassisModel(process_fn=process)\n# Build the model locally using Docker.\nresponse = chassis_model.publish(\nmodel_name=\"Chassisml Regression Model\",\nmodel_version=\"0.0.1\",\n)\n</code></pre></p>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.render_dockerfile","title":"render_dockerfile","text":"<pre><code>render_dockerfile(options)\n</code></pre> <p>Renders an appropriate <code>Dockerfile</code> for this object with the supplied <code>BuildOptions</code>.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>BuildOptions</code> <p>The <code>BuildOptions</code> that will be used for this build.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string containing the contents for a <code>Dockerfile</code>.</p>"},{"location":"reference/sdk/chassisml/v1/chassis_model/#chassisml.v1.chassis_model.ChassisModel.parse_conda_env","title":"parse_conda_env","text":"<pre><code>parse_conda_env(conda_env)\n</code></pre> <p>Supports legacy Chassis <code>conda_env</code> functionality by parsing pip dependencies and inserting into the <code>Buildable</code> object via the <code>add_requirements</code> function.</p> <p>If supplied, the input parameter will look like this:</p> <pre><code>env = {\n\"name\": \"sklearn-chassis\",\n\"channels\": ['conda-forge'],\n\"dependencies\": [\n\"python=3.8.5\",\n{\n\"pip\": [\n\"scikit-learn\",\n\"numpy\",\n\"chassisml\"\n]\n}\n]\n}\nchassis_model = ChassisModel(process_fn=predict)\nchassis_model.parse_conda_env(env)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>conda_env</code> <code>Optional[dict]</code> <p>A conda environment structure. See above for more details.</p> required"},{"location":"reference/sdk/chassisml/v1/helpers/","title":"helpers","text":""},{"location":"reference/sdk/chassisml/v1/helpers/#chassisml.v1.helpers.deprecated","title":"deprecated","text":"<pre><code>deprecated(message=None)\n</code></pre>"},{"location":"reference/sdk/chassisml/v1/helpers/#chassisml.v1.helpers.caller_name","title":"caller_name","text":"<pre><code>caller_name(skip=2)\n</code></pre> <p>Get a name of a caller in the format module.class.method</p> <p><code>skip</code> specifies how many levels of stack to skip while getting caller name. skip=1 means \"who calls me\", skip=2 \"who calls my caller\" etc.</p> <p>An empty string is returned if skipped levels exceed stack height</p>"}]}