{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chassis Example Notebooks\n",
    "Welcome to the examples section for [Chassis](https://chassis.ml), which contains notebooks that auto-containerize models built using the most common machine learning (ML) frameworks. \n",
    "\n",
    "#### What is Chassis?\n",
    "Chassis allows you to automatically create a Docker container from your model code and push that container image to a Docker registry. All you need is your model loaded into memory and a few lines of Chassis code! Our example bank is here to provide reference examples for many common ML frameworks.  \n",
    "\n",
    "Can't find the framework you are looking for or need help? Fork this repository and open a PR, or list the desired framework in a new issue. We're always interested in growing this example bank! \n",
    "\n",
    "The primary maintainers of Chassis also actively monitor our [Discord Server](https://discord.gg/cHpzY9yCcM), so feel free to join and ask any questions you might have. We'll be there to respond and help out promptly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import tempfile\n",
    "import chassisml\n",
    "import numpy as np\n",
    "import getpass\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from shutil import rmtree\n",
    "import json\n",
    "import onnx\n",
    "from onnx import backend\n",
    "from onnx import numpy_helper\n",
    "import onnxruntime as ort\n",
    "from transformers import GPT2Model, GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter credentials\n",
    "Dockerhub creds and Modzy API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerhub_user = getpass.getpass('docker hub username')\n",
    "dockerhub_pass = getpass.getpass('docker hub password')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ONNX Model and Test Locally\n",
    "This model was downloaded from the [ONNX Model Zoo](https://github.com/onnx/models/tree/master/text/machine_comprehension/gpt-2), which contains several pre-trained models saved in the ONNX open standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gpt-2 and gpt-2 head models are valid onnx models files\n",
    "head_model = onnx.load(\"models/head_model.onnx\")\n",
    "\n",
    "# check onnx file is valid model\n",
    "onnx.checker.check_model(head_model)\n",
    "\n",
    "# load input tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING FUNTIONS NEEDED\n",
    "def flatten(inputs):\n",
    "    return [[flatten(i) for i in inputs] if isinstance(inputs, (list, tuple)) else inputs]\n",
    "def to_numpy(x):\n",
    "    if type(x) is not np.ndarray:\n",
    "        x = x.detach().cpu().numpy().astype(np.int64) if x.requires_grad else x.cpu().numpy().astype(np.int64)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "length = 10\n",
    "\n",
    "text = \"Today is a great day to learn about transformers. And Chassisml makes it really easy to package models\"\n",
    "tokens = np.array(tokenizer.encode(text, add_special_tokens=True))\n",
    "tensors = torch.tensor([[tokens]])\n",
    "prev = tensors\n",
    "output = tensors.squeeze(0)\n",
    "\n",
    "for i in range(length):\n",
    "    if len(prev.shape) == 2:\n",
    "        prev = prev.unsqueeze(0)     \n",
    "    session = ort.InferenceSession(\"models/head_model.onnx\")\n",
    "    ort_inputs = dict((session.get_inputs()[i].name, to_numpy(input)) for i, input in enumerate(flatten(prev)))\n",
    "    outputs = session.run(None, ort_inputs)\n",
    "    logits = torch.tensor(outputs[0]).squeeze(0)\n",
    "    logits = logits[:, -1, :]\n",
    "    log_probs = F.softmax(logits, dim=-1)\n",
    "    _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "    output = torch.cat((output, prev), dim=1)\n",
    "\n",
    "output = output[:, len(tokens):].tolist()\n",
    "generated = 0\n",
    "output_text = []\n",
    "for i in range(batch_size):\n",
    "    generated += 1\n",
    "    text = tokenizer.decode(output[i])\n",
    "    output_text.append(text)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write process function\n",
    "\n",
    "* Must take bytes as input\n",
    "* Preprocess bytes, run inference, postprocess model output, return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(input_bytes):\n",
    "    # initialize fixed variables for post processing\n",
    "    batch_size = 1\n",
    "    length = 10\n",
    "    \n",
    "    # save model to filepath for inference\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    import onnx\n",
    "    onnx.save(head_model, \"{}/head_model.onnx\".format(tmp_dir))\n",
    "    \n",
    "    # preprocess text\n",
    "    text_input = input_bytes.decode()\n",
    "    tokens = np.array(tokenizer.encode(text_input, add_special_tokens=True))\n",
    "    tensors = torch.tensor([[tokens]])\n",
    "\n",
    "    # run inference \n",
    "    prev = tensors\n",
    "    output = tensors.squeeze(0)\n",
    "    for i in range(length):\n",
    "        if len(prev.shape) == 2:\n",
    "            prev = prev.unsqueeze(0)     \n",
    "        session = ort.InferenceSession(\"{}/head_model.onnx\".format(tmp_dir))\n",
    "        ort_inputs = dict((session.get_inputs()[i].name, to_numpy(input)) for i, input in enumerate(flatten(prev)))\n",
    "        outputs = session.run(None, ort_inputs)\n",
    "        logits = torch.tensor(outputs[0]).squeeze(0)\n",
    "        logits = logits[:, -1, :]\n",
    "        log_probs = F.softmax(logits, dim=-1)\n",
    "        _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "        output = torch.cat((output, prev), dim=1)\n",
    "\n",
    "    # process output\n",
    "    output = output[:, len(tokens):].tolist()\n",
    "    generated = 0\n",
    "    for i in range(batch_size):\n",
    "        generated += 1\n",
    "        text_full = tokenizer.decode(output[i])\n",
    "        \n",
    "    # format text\n",
    "    output_text = text_full.split(\" \")[1:]\n",
    "    \n",
    "    # format results\n",
    "    structured_result = {\n",
    "        \"data\": {\n",
    "            \"result\": {\"nextWordPredictions\": [{\"word_{}\".format(i): text_pred} for i, text_pred in enumerate(output_text)]},\n",
    "            \"combined\": text_input + text_full\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # remove temp directory\n",
    "    rmtree(tmp_dir)\n",
    "    return structured_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Chassis Client\n",
    "We'll use this to interact with the Chassis service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and test Chassis model\n",
    "* Requires `process_fn` defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"data\":{\"result\":{\"nextWordPredictions\":[]},\"combined\":\"Today is a great day to learn about transformers. And Chassisml makes it really easy to package models.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\"}}'\n"
     ]
    }
   ],
   "source": [
    "# create Chassis model\n",
    "chassis_model = chassis_client.create_model(process_fn=process)\n",
    "\n",
    "# test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here):\n",
    "sample_filepath = 'data/sample_text.txt'\n",
    "results = chassis_model.test(sample_filepath)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test environment and model within Chassis service, must pass filepath here:\n",
    "\n",
    "# dry run before build\n",
    "test_env_result = chassis_model.test_env(sample_filepath)\n",
    "print(test_env_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish model to Docker\n",
    "Need to provide model name, model version, and Dockerhub credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting build job... Ok!\n"
     ]
    }
   ],
   "source": [
    "response = chassis_model.publish(\n",
    "    model_name=\"ONNX GPT2 Word Prediction\",\n",
    "    model_version=\"0.0.1\",\n",
    "    registry_user=dockerhub_user,\n",
    "    registry_pass=dockerhub_pass\n",
    ")\n",
    "\n",
    "job_id = response.get('job_id')\n",
    "final_status = chassis_client.block_until_complete(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model URL: https://integration.modzy.engineering/models/txje9pp5co/0.0.1\n"
     ]
    }
   ],
   "source": [
    "if chassis_client.get_job_status(job_id)[\"result\"] is not None:\n",
    "    print(\"New model URL: {}\".format(chassis_client.get_job_status(job_id)[\"result\"][\"container_url\"]))\n",
    "else:\n",
    "    print(\"Chassis job failed \\n\\n {}\".format(chassis_client.get_job_status(job_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95ab8d3bdd708a5b71676285742277315a30b0df1ed91c8905076616709c59d5"
  },
  "kernelspec": {
   "display_name": "chassis-demo-1",
   "language": "python",
   "name": "chassis-demo-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
